{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation using downloaded document corpus for 'debiaser' data product\n",
    "#### Sagar Setru, September 21th, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief description using CoNVO framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context\n",
    "\n",
    "Some people are eager to get news from outside of their echo chamber. However, they do not know where to go outside of their echo chambers, and may also have some activation energy when it comes to seeking information from other sources. In the meantime, most newsfeeds only push you content that you agree with. You end up in an echo chamber, but may not have ever wanted to be in one in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need\n",
    "\n",
    "A way to find news articles from different yet reliable media sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision\n",
    "\n",
    "Debiaser, a data product (maybe Chrome plug-in?) that will recommend news articles similar in topic to the one currently being read, but from several pre-curated and reliable news media organizations across the political spectrum, for example, following the \"media bias chart\" here https://www.adfontesmedia.com/ or the \"media bias ratings\" here: https://www.allsides.com/media-bias/media-bias-ratings. The app will determine the main topics of the text of a news article, and then show links to similar articles from other news organizations.\n",
    "\n",
    "The product will generate topics for a given document via latent Dirichlet allocation (LDA) and then search news websites for the topic words generated.\n",
    "\n",
    "Caveats: Many of these articles may be behind paywalls. News aggregators already basically do this. How different is this than just searching Google using the title of an article?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outcome\n",
    "\n",
    "People who are motivated to engage in content outside of their echo chambers have a tool that enables them to quickly find news similar to what they are currently reading, but from a variety of news organizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing single document lda on these articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conda environment:\n",
      "debiaser\n"
     ]
    }
   ],
   "source": [
    "# make sure I'm in the right environment (should be 'debiaser')\n",
    "import os\n",
    "print('Conda environment:')\n",
    "print(os.environ['CONDA_DEFAULT_ENV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "import json\n",
    "\n",
    "# NLP Packages\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# to break articles up into sentences\n",
    "from nltk import tokenize\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "from text_processing_functions import process_all_articles\n",
    "from text_processing_functions import remove_stopwords\n",
    "from text_processing_functions import get_simple_corpus_dictionary_bow\n",
    "from text_processing_functions import entity_recognizer\n",
    "from text_processing_functions import get_topic_words_mean_std_prob_frequency\n",
    "from text_processing_functions import sort_topics_mean_frequency\n",
    "\n",
    "import pickle\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sagarsetru/anaconda3/envs/debiaser/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (12,13) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>index.1</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>publication</th>\n",
       "      <th>category</th>\n",
       "      <th>digital</th>\n",
       "      <th>section</th>\n",
       "      <th>url</th>\n",
       "      <th>article_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Agent Cooper in Twin Peaks is the audience: on...</td>\n",
       "      <td>\\nTasha Robinson\\n</td>\n",
       "      <td>2017-05-31</td>\n",
       "      <td>And never more so than in Showtime’s new...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>AI, the humanity!</td>\n",
       "      <td>\\nSam Byford\\n</td>\n",
       "      <td>2017-05-30</td>\n",
       "      <td>AlphaGo’s victory isn’t a defeat for hum...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>The Viral Machine</td>\n",
       "      <td>\\nKaitlyn Tiffany\\n</td>\n",
       "      <td>2017-05-25</td>\n",
       "      <td>Super Deluxe built a weird internet empi...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>How Anker is beating Apple and Samsung at thei...</td>\n",
       "      <td>\\nNick Statt\\n</td>\n",
       "      <td>2017-05-22</td>\n",
       "      <td>Steven Yang quit his job at Google in th...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Tour Black Panther’s reimagined homeland with ...</td>\n",
       "      <td>\\nKwame Opam\\n</td>\n",
       "      <td>2017-05-15</td>\n",
       "      <td>Ahead of Black Panther’s 2018 theatrical...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  index.1  id                                              title  \\\n",
       "0      0        0   1  Agent Cooper in Twin Peaks is the audience: on...   \n",
       "1      1        1   2                                  AI, the humanity!   \n",
       "2      2        2   3                                  The Viral Machine   \n",
       "3      3        3   4  How Anker is beating Apple and Samsung at thei...   \n",
       "4      4        4   5  Tour Black Panther’s reimagined homeland with ...   \n",
       "\n",
       "                author        date  \\\n",
       "0   \\nTasha Robinson\\n  2017-05-31   \n",
       "1       \\nSam Byford\\n  2017-05-30   \n",
       "2  \\nKaitlyn Tiffany\\n  2017-05-25   \n",
       "3       \\nNick Statt\\n  2017-05-22   \n",
       "4       \\nKwame Opam\\n  2017-05-15   \n",
       "\n",
       "                                             content    year  month  \\\n",
       "0        And never more so than in Showtime’s new...  2017.0    5.0   \n",
       "1        AlphaGo’s victory isn’t a defeat for hum...  2017.0    5.0   \n",
       "2        Super Deluxe built a weird internet empi...  2017.0    5.0   \n",
       "3        Steven Yang quit his job at Google in th...  2017.0    5.0   \n",
       "4        Ahead of Black Panther’s 2018 theatrical...  2017.0    5.0   \n",
       "\n",
       "  publication  category  digital section  url  article_length  \n",
       "0       Verge  Longform      1.0     NaN  NaN            2121  \n",
       "1       Verge  Longform      1.0     NaN  NaN            1948  \n",
       "2       Verge  Longform      1.0     NaN  NaN            3011  \n",
       "3       Verge  Longform      1.0     NaN  NaN            3281  \n",
       "4       Verge  Longform      1.0     NaN  NaN             239  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_news_df = pd.read_csv('./all_the_news/all_news_df_processed.csv')\n",
    "all_news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stop_words_csv_to_list(full_file_name):\n",
    "    \"\"\"fxn that loads stop words list downloaded from git repo called 'news-stopwords'\"\"\"\n",
    "    \n",
    "    stop_words = pd.read_csv(full_file_name)\n",
    "\n",
    "    stop_words = stop_words['term']\n",
    "\n",
    "    stop_words = [word for word in stop_words]\n",
    "    \n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lda_top_topic_words(lda_topics,num_topics,do_unique_search_words,n_search_words):\n",
    "    \"\"\"\n",
    "    fxn for algorithm to return the top topic words\n",
    "    algo varies based on:\n",
    "    1) whether only unique words are wanted, and\n",
    "    2) whether there is 1 or more topics\n",
    "    \n",
    "                \n",
    "    if one topic, just take top word in each generated topic\n",
    "    else, if do_unique_search_words, get top word in each topic that is unique,\n",
    "          else, just get top word in each topic even if it isn't unique\n",
    "    \n",
    "    parameters\n",
    "    ----------\n",
    "    lda_topics - topic output from lda model\n",
    "    num_topic - how many lda topics were generated\n",
    "    do_unique_search_words - whether to all repeating words as search terms\n",
    "    n_search_words - how many topic words to use as search terms\n",
    "    \n",
    "    outputs\n",
    "    -------\n",
    "    list and string of search/topic words\n",
    "    \"\"\"\n",
    "    \n",
    "    # string is for final search string\n",
    "    lda_top_topic_words_string = ''\n",
    "\n",
    "    # list is for checking previous words\n",
    "    lda_top_topic_words_list = []\n",
    "    \n",
    "    # if lda model has only one topic\n",
    "    if num_topics == 1:\n",
    "\n",
    "        for topic in lda_topics:\n",
    "\n",
    "            # get the list of topic words\n",
    "            topic_words = topic[1]\n",
    "\n",
    "            # loop through these words and get the top n number\n",
    "            counter = -1\n",
    "            for topic_word in topic_words:\n",
    "\n",
    "                counter += 1\n",
    "\n",
    "                if counter < n_search_words:\n",
    "\n",
    "                    lda_top_topic_words_string += ' '+topic_word[0]\n",
    "                    lda_top_topic_words_list.append(topic_word[0])\n",
    "\n",
    "    # if lda model has more than one topic\n",
    "    elif num_topics > 1:\n",
    "            \n",
    "        # this ind is to always get list of tuples of (word, prob)\n",
    "        fixed_ind1 = 1\n",
    "\n",
    "        # this ind is to always access the word in the tuple (word, prob)\n",
    "        fixed_ind2 = 0\n",
    "\n",
    "        # if you're okay with topic words repeating (often happens..)\n",
    "        if not do_unique_search_words:\n",
    "\n",
    "            # loop counter\n",
    "            counter = 0\n",
    "            \n",
    "            # index of word within topic\n",
    "            ind_use = 0\n",
    "            \n",
    "            # index of topic\n",
    "            topic_use = -1\n",
    "            \n",
    "            for i in range(n_search_words):\n",
    "                counter += 1\n",
    "\n",
    "                if counter > num_topics:\n",
    "                    ind_use += 1\n",
    "                    counter = 1\n",
    "\n",
    "                if topic_use < num_topics-1:\n",
    "                    topic_use += 1\n",
    "                else:\n",
    "                    topic_use = 0\n",
    "\n",
    "                # access the appropriate topic word\n",
    "                word = lda_topics[topic_use][fixed_ind1][ind_use][fixed_ind2]\n",
    "\n",
    "                lda_top_topic_words_string += ' '+word\n",
    "\n",
    "                lda_top_topic_words_list.append(word)\n",
    "\n",
    "        # don't reuse a word if it has already been used\n",
    "        else:\n",
    "\n",
    "            counter = 0\n",
    "            ind_use = 0\n",
    "            topic_use = -1\n",
    "            \n",
    "            # do loop over total words across all topics\n",
    "            total_topic_words = len(lda_topics)*len(lda_topics[0][fixed_ind1])\n",
    "            for i in range(total_topic_words):\n",
    "                counter += 1\n",
    "\n",
    "                if counter > num_topics:\n",
    "                    ind_use += 1\n",
    "                    counter = 1\n",
    "\n",
    "                if topic_use < num_topics-1:\n",
    "                    topic_use += 1\n",
    "                else:\n",
    "                    topic_use = 0\n",
    "\n",
    "                # access the appropriate topic word\n",
    "                word = lda_topics[topic_use][fixed_ind1][ind_use][fixed_ind2]\n",
    "\n",
    "                # only add if it is not currently in the top topic words\n",
    "                if word not in lda_top_topic_words_list:\n",
    "\n",
    "                    lda_top_topic_words_string += ' '+word\n",
    "\n",
    "                    lda_top_topic_words_list.append(word)\n",
    "                \n",
    "                # if the length of the topic words list is at the number of descired topics\n",
    "                if len(lda_top_topic_words_list) == n_search_words:\n",
    "                    break\n",
    "\n",
    "    return lda_top_topic_words_string, lda_top_topic_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_topic_word_probs(lda_topics,n_search_words_single_topic_analysis):\n",
    "    \"\"\"\n",
    "    fxn for algorithm to return the top topic words\n",
    "    algo varies based on:\n",
    "    1) whether only unique words are wanted, and\n",
    "    2) whether there is 1 or more topics\n",
    "    \n",
    "                \n",
    "    if one topic, just take top word in each generated topic\n",
    "    else, if do_unique_search_words, get top word in each topic that is unique,\n",
    "          else, just get top word in each topic even if it isn't unique\n",
    "    \n",
    "    parameters\n",
    "    ----------\n",
    "    lda_topics - topic output from lda model\n",
    "    num_topics - how many lda topics were generated\n",
    "    do_unique_search_words - whether to all repeating words as search terms\n",
    "    n_search_words - how many topic words to use as search terms\n",
    "    \n",
    "    outputs\n",
    "    -------\n",
    "    list and string of search/topic words\n",
    "    \"\"\"\n",
    "\n",
    "    # generate empty vector for probs associated with words in topic\n",
    "    lda_topic_word_probs = np.zeros((n_search_words_single_topic_analysis,1))\n",
    "\n",
    "    # set default to nan in case any probs eval to 0..\n",
    "    lda_topic_word_probs[:] = np.nan\n",
    "\n",
    "    for topic in lda_topics:\n",
    "\n",
    "        # get the list of topic words and probs\n",
    "        topic_words = topic[1]\n",
    "\n",
    "        # loop through these words and get the associated probabilities\n",
    "        for ind, topic_word in enumerate(topic_words):\n",
    "\n",
    "            # add probability to prob vector\n",
    "            lda_topic_word_probs[ind] = topic_word[1]\n",
    "            \n",
    "    return lda_topic_word_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_frequencies(article_processed_whole,n_search_words):\n",
    "    \"\"\"\n",
    "    fxn that does simple counting of word frequency.\n",
    "    Goal is to have some baseline for how single doc LDA approach \n",
    "    compares to just counting most common words.\n",
    "    \"\"\"\n",
    "    \n",
    "    # dictionary of word counts\n",
    "    word_dict_count = {}\n",
    "\n",
    "    for word in article_processed_whole[0]:\n",
    "\n",
    "        if word in word_dict_count.keys():\n",
    "\n",
    "            word_dict_count[word] += 1\n",
    "\n",
    "        else:\n",
    "\n",
    "            word_dict_count[word] = 1\n",
    "\n",
    "    # make list for word counts\n",
    "    word_counts = []\n",
    "\n",
    "    # loop through dictionary\n",
    "    for key, value in word_dict_count.items():\n",
    "        word_counts.append(value)\n",
    "    \n",
    "    # get unique values of word counts\n",
    "#     word_counts = list(set(word_counts))\n",
    "\n",
    "    # sort counts from high to low \n",
    "    word_counts = sorted(word_counts, reverse=True)\n",
    "\n",
    "    # keep appropriate number of word counts\n",
    "    word_counts_top = word_counts[0:n_search_words]\n",
    "\n",
    "    # list for most common words\n",
    "    most_common_words_list = []\n",
    "    most_common_words_string = ''\n",
    "\n",
    "    # loop through dictionary\n",
    "    for key, value in word_dict_count.items():\n",
    "\n",
    "        # if value of this word is one of the top ones, add this word for list of common words\n",
    "        if value in word_counts_top:\n",
    "            most_common_words_list.append(key)\n",
    "            most_common_words_string += ' '+key\n",
    "            \n",
    "    return word_dict_count, word_counts_top, most_common_words_list, most_common_words_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jaccard_sim(list1, list2): \n",
    "    \"\"\"\n",
    "    fxn calculates jaccard sim between two lists of words\n",
    "    from https://towardsdatascience.com/overview-of-text-similarity-metrics-3397c4601f50\n",
    "    \"\"\"\n",
    "    a = set(list1) \n",
    "    b = set(list2)\n",
    "    \n",
    "    c = a.intersection(b)\n",
    "    \n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(bow_vec1,bow_vec2):\n",
    "    \"\"\"\n",
    "    fxn calculates the bag of words similarity between two word vectors\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the words in each vector and their lengths in a dictionary\n",
    "    vec1_words_dict = {}\n",
    "    vec2_words_dict = {}\n",
    "    \n",
    "    # get just the words\n",
    "    vec1_words = []\n",
    "    vec2_words = []\n",
    "    \n",
    "    # get just the values\n",
    "    vec1_vals = np.zeros((len(bow_vec1)))\n",
    "    vec2_vals = np.zeros((len(bow_vec2)))\n",
    "    \n",
    "    # populate dictionary and lists\n",
    "    for ind, val in enumerate(bow_vec1):\n",
    "\n",
    "        vec1_words_dict[val[0]] = val[1]\n",
    "        vec1_words.append(val[0])\n",
    "        vec1_vals[ind] = val[1]\n",
    "    \n",
    "    # populate dictionary and lists\n",
    "    for ind, val in enumerate(bow_vec2):\n",
    "        \n",
    "        vec2_words_dict[val[0]] = val[1]\n",
    "        vec2_words.append(val[0])\n",
    "        vec2_vals[ind] = val[1]\n",
    "        \n",
    "    # get norms of each vector\n",
    "    norm_vec1 = np.linalg.norm(vec1_vals)\n",
    "    norm_vec2 = np.linalg.norm(vec2_vals)\n",
    "    \n",
    "    # get the list of all the words\n",
    "    all_words = list(set().union(vec1_words,vec2_words))\n",
    "    \n",
    "    # loop through words, update dictionaries if word is not in original vector\n",
    "    for word in all_words:\n",
    "        \n",
    "        if word not in vec1_words:\n",
    "            \n",
    "            vec1_words_dict[word] = 0\n",
    "            \n",
    "        if word not in vec2_words:\n",
    "            \n",
    "            vec2_words_dict[word] = 0\n",
    "       \n",
    "    # initialize float for final dot product\n",
    "    dot_product = 0.0\n",
    "    \n",
    "    # loop through words\n",
    "    for word in vec1_words_dict.keys():\n",
    "        \n",
    "        vec1_val = vec1_words_dict[word]\n",
    "        vec2_val = vec2_words_dict[word]\n",
    "        \n",
    "        dot_product += (vec1_val * vec2_val)\n",
    "    \n",
    "    cosine_sim = dot_product/(norm_vec1*norm_vec2)\n",
    "    \n",
    "    return cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose list of stop words\n",
    "\n",
    "# choose whether 1k, 10k, 100k, or nltk\n",
    "which_stop_words = '1k'\n",
    "# which_stop_words = '10k'\n",
    "# which_stop_words = '100k'\n",
    "# which_stop_words = 'nltk'\n",
    "\n",
    "stop_words_path = '/Users/sagarsetru/Documents/post PhD positions search/insightDataScience/project/debiaser/stop_words_db/news-stopwords-master/'\n",
    "\n",
    "\n",
    "if which_stop_words == '1k':\n",
    "    \n",
    "    # doing 1k words list\n",
    "    stop_words_file_name = 'sw1k.csv'\n",
    "    \n",
    "    # make full file name\n",
    "    stop_words_full_file_name = stop_words_path+stop_words_file_name\n",
    "    \n",
    "    # get list of stop words\n",
    "    stop_words = load_stop_words_csv_to_list(stop_words_full_file_name)\n",
    "    \n",
    "elif which_stop_words == '10k':\n",
    "    \n",
    "    # doing 10k words list\n",
    "    stop_words_file_name = 'sw10k.csv'\n",
    "\n",
    "    # make full file name\n",
    "    stop_words_full_file_name = stop_words_path+stop_words_file_name\n",
    "    \n",
    "    # get list of stop words\n",
    "    stop_words = load_stop_words_csv_to_list(stop_words_full_file_name)\n",
    "\n",
    "elif which_stop_words == '100k':\n",
    "    \n",
    "    # doing 100k\n",
    "    stop_words_file_name = 'sw100k.csv'  \n",
    "    \n",
    "    # get full file name\n",
    "    stop_words_full_file_name = stop_words_path+stop_words_file_name\n",
    "    \n",
    "    # get list of stop words\n",
    "    stop_words = load_stop_words_csv_to_list(stop_words_full_file_name)\n",
    "\n",
    "\n",
    "elif which_stop_words == 'nltk':\n",
    "    # import from nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "else:\n",
    "    print('Select proper variable name for \"which_stop_words\"')\n",
    "    \n",
    "# adding custom words\n",
    "stop_words.append('said')\n",
    "stop_words.append('youre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a random sampling of these articles for testing single document lda\n",
    "\n",
    "# number of times to draw a bootstrap sampling\n",
    "n_bootstrap_samples = 1000\n",
    "\n",
    "# number of articles to draw per sampling\n",
    "n_articles_per_sample = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap sample 1 out of 3\n",
      "Article 1 out of 4\n",
      "LDA topic number 1 out of 2\n",
      "N topics: 1\n",
      " \n",
      "Bootstrap sample 1 out of 3\n",
      "Article 1 out of 4\n",
      "LDA topic number 2 out of 2\n",
      "N topics: 2\n",
      " \n",
      "Bootstrap sample 1 out of 3\n",
      "Article 2 out of 4\n",
      "LDA topic number 1 out of 2\n",
      "N topics: 1\n",
      " \n",
      "Bootstrap sample 1 out of 3\n",
      "Article 2 out of 4\n",
      "LDA topic number 2 out of 2\n",
      "N topics: 2\n",
      " \n",
      "Bootstrap sample 1 out of 3\n",
      "Article 3 out of 4\n",
      "LDA topic number 1 out of 2\n",
      "N topics: 1\n",
      " \n",
      "Bootstrap sample 1 out of 3\n",
      "Article 3 out of 4\n",
      "LDA topic number 2 out of 2\n",
      "N topics: 2\n",
      " \n",
      "Bootstrap sample 1 out of 3\n",
      "Article 4 out of 4\n",
      "LDA topic number 1 out of 2\n",
      "N topics: 1\n",
      " \n",
      "Bootstrap sample 1 out of 3\n",
      "Article 4 out of 4\n",
      "LDA topic number 2 out of 2\n",
      "N topics: 2\n",
      " \n",
      "Bootstrap sample 2 out of 3\n",
      "Article 1 out of 4\n",
      "LDA topic number 1 out of 2\n",
      "N topics: 1\n",
      " \n",
      "Bootstrap sample 2 out of 3\n",
      "Article 1 out of 4\n",
      "LDA topic number 2 out of 2\n",
      "N topics: 2\n",
      " \n",
      "Bootstrap sample 2 out of 3\n",
      "Article 2 out of 4\n",
      "LDA topic number 1 out of 2\n",
      "N topics: 1\n",
      " \n",
      "Bootstrap sample 2 out of 3\n",
      "Article 2 out of 4\n",
      "LDA topic number 2 out of 2\n",
      "N topics: 2\n",
      " \n",
      "Bootstrap sample 2 out of 3\n",
      "Article 3 out of 4\n",
      "LDA topic number 1 out of 2\n",
      "N topics: 1\n",
      " \n",
      "Bootstrap sample 2 out of 3\n",
      "Article 3 out of 4\n",
      "LDA topic number 2 out of 2\n",
      "N topics: 2\n",
      " \n",
      "Bootstrap sample 2 out of 3\n",
      "Article 4 out of 4\n",
      "LDA topic number 1 out of 2\n",
      "N topics: 1\n",
      " \n",
      "Bootstrap sample 2 out of 3\n",
      "Article 4 out of 4\n",
      "LDA topic number 2 out of 2\n",
      "N topics: 2\n",
      " \n",
      "Bootstrap sample 3 out of 3\n",
      "Article 1 out of 4\n",
      "LDA topic number 1 out of 2\n",
      "N topics: 1\n",
      " \n",
      "Bootstrap sample 3 out of 3\n",
      "Article 1 out of 4\n",
      "LDA topic number 2 out of 2\n",
      "N topics: 2\n",
      " \n",
      "Bootstrap sample 3 out of 3\n",
      "Article 2 out of 4\n",
      "LDA topic number 1 out of 2\n",
      "N topics: 1\n",
      " \n",
      "Bootstrap sample 3 out of 3\n",
      "Article 2 out of 4\n",
      "LDA topic number 2 out of 2\n",
      "N topics: 2\n",
      " \n",
      "Bootstrap sample 3 out of 3\n",
      "Article 3 out of 4\n",
      "LDA topic number 1 out of 2\n",
      "N topics: 1\n",
      " \n",
      "Bootstrap sample 3 out of 3\n",
      "Article 3 out of 4\n",
      "LDA topic number 2 out of 2\n",
      "N topics: 2\n",
      " \n",
      "Bootstrap sample 3 out of 3\n",
      "Article 4 out of 4\n",
      "LDA topic number 1 out of 2\n",
      "N topics: 1\n",
      " \n",
      "Bootstrap sample 3 out of 3\n",
      "Article 4 out of 4\n",
      "LDA topic number 2 out of 2\n",
      "N topics: 2\n",
      " \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './number_most_common_words/number_most_common_words.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9f16c14f8bbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mjaccard_sim_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'./validation_data/jaccard_sim_all.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mcosine_sim_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'./validation_data/cosine_sim_all.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mcosine_sim_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'./number_most_common_words/number_most_common_words.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './number_most_common_words/number_most_common_words.pkl'"
     ]
    }
   ],
   "source": [
    "# choose the number of LDA topics\n",
    "# num_lda_topics = [1,2,3,4,5,6,7,8,9,10]\n",
    "num_lda_topics = [1,2]\n",
    "\n",
    "# do by sentences\n",
    "do_sentences = 1\n",
    "\n",
    "# to show plots per  run\n",
    "do_plot = 0\n",
    "\n",
    "# to print output\n",
    "do_print = 0\n",
    "\n",
    "# to print progress of testing\n",
    "do_progress_print = 1\n",
    "\n",
    "# number of passes LDA does through corpus (hyperparameter)\n",
    "n_passes = 6\n",
    "\n",
    "# whether to use unique topic words or allow repeated words\n",
    "do_unique_search_words = 1\n",
    "\n",
    "# number of words to use in search\n",
    "# (or number of top most often frequencies of words, e.g., use 5 highest word frequencies)\n",
    "# (This could be more than 5 words if some words show up equally as often and among the msot often of all words)\n",
    "n_search_words = 5\n",
    "\n",
    "# dummy nlp variable, for now not in use because lemmatization not in use\n",
    "nlp = []\n",
    "\n",
    "# empty matrix for perplexity scores\n",
    "perplexity_scores = np.zeros(( n_bootstrap_samples, n_articles_per_sample, len(num_lda_topics) ))\n",
    "\n",
    "# empty matrix for coherence scores\n",
    "coherence_scores = np.zeros(( n_bootstrap_samples, n_articles_per_sample, len(num_lda_topics) ))\n",
    "\n",
    "# empty matrix for jaccard sim, set to nan in case any have 0 similarity\n",
    "jaccard_sim_all = np.zeros(( n_bootstrap_samples, n_articles_per_sample, len(num_lda_topics) ))\n",
    "jaccard_sim_all[:] = np.nan\n",
    "\n",
    "# empty matrix for cosine sim, set to nan in case any have 0 similarity\n",
    "cosine_sim_all = np.zeros(( n_bootstrap_samples, n_articles_per_sample, len(num_lda_topics) ))\n",
    "cosine_sim_all[:] = np.nan\n",
    "\n",
    "# empty matrix for length of most common words vec\n",
    "number_most_common_words = np.zeros(( n_bootstrap_samples, n_articles_per_sample, len(num_lda_topics) ))\n",
    "\n",
    "# empty matrix for probability vs word in topic (doing for n = 1 topic only)\n",
    "n_search_words_single_topic_analysis = 10\n",
    "topic_word_probs = np.zeros(( n_bootstrap_samples, n_articles_per_sample, n_search_words_single_topic_analysis ))\n",
    "\n",
    "\n",
    "# counter for bootstrap sampling\n",
    "counter_nboot = -1\n",
    "\n",
    "for i in range(n_bootstrap_samples):\n",
    "    \n",
    "    # draw random articles\n",
    "    articles_df_random_subset = all_news_df.sample(n=n_articles_per_sample)\n",
    "    \n",
    "    # get their content\n",
    "    articles_content = articles_df_random_subset['content']\n",
    "    \n",
    "    counter_nboot += 1\n",
    "\n",
    "    # counter for articles\n",
    "    counter_article = -1\n",
    "    \n",
    "    for i in range(len(articles_content)):\n",
    "        \n",
    "        counter_article += 1\n",
    "        \n",
    "        for ind_num_topicss, num_topics in enumerate(num_lda_topics):\n",
    "\n",
    "            # get the article text\n",
    "            article_text = articles_content.iloc[i]\n",
    "            \n",
    "            if do_print:\n",
    "                print(article_text)\n",
    "                \n",
    "            if do_progress_print:\n",
    "                print(f'Bootstrap sample {counter_nboot+1} out of {n_bootstrap_samples}')\n",
    "                print(f'Article {counter_article+1} out of {n_articles_per_sample}')\n",
    "                print(f'LDA topic number {ind_num_topicss+1} out of {len(num_lda_topics)}')\n",
    "                print(f'N topics: {num_topics}')\n",
    "                print(' ')\n",
    "\n",
    "            # for counting word frequencies\n",
    "            article_processed_whole = process_all_articles([article_text],nlp)\n",
    "\n",
    "            article_processed_whole = remove_stopwords(article_processed_whole,stop_words)\n",
    "\n",
    "\n",
    "            if do_sentences:\n",
    "\n",
    "                # break article into sentences\n",
    "                article_text = tokenize.sent_tokenize(article_text)\n",
    "\n",
    "                # process article\n",
    "                article_processed = process_all_articles(article_text,nlp)\n",
    "\n",
    "                # remove stopwords\n",
    "                article_processed = remove_stopwords(article_processed,stop_words)\n",
    "\n",
    "            else:\n",
    "\n",
    "                # process article\n",
    "                article_processed = process_all_articles([article_text],nlp)\n",
    "\n",
    "                # remove stopwords\n",
    "                article_processed = remove_stopwords(article_processed,stop_words)\n",
    "\n",
    "\n",
    "            # get corpus, dictionary, bag of words\n",
    "            processed_corpus, processed_dictionary, bow_corpus = get_simple_corpus_dictionary_bow(article_processed)\n",
    "\n",
    "            # generate the LDA model\n",
    "            lda = LdaModel(corpus = bow_corpus,\n",
    "                             num_topics = num_topics,\n",
    "                             id2word = processed_dictionary,\n",
    "                             passes = n_passes)\n",
    "            \n",
    "            # calculate and store perplexity\n",
    "            perplexity = lda.log_perplexity(bow_corpus)\n",
    "            perplexity_scores[counter_nboot,counter_article,ind_num_topicss] = perplexity\n",
    "\n",
    "            # calculate and store coherence\n",
    "            coherence_model_lda = CoherenceModel(model=lda, texts=article_processed, dictionary=processed_dictionary, coherence='c_v')\n",
    "            coherence_lda = coherence_model_lda.get_coherence()\n",
    "            coherence_scores[counter_nboot,counter_article,ind_num_topicss] = coherence_lda\n",
    "            \n",
    "            # get the topics from the lda model\n",
    "            lda_topics = lda.show_topics(formatted=False)\n",
    "\n",
    "            # get the top topic words\n",
    "            lda_top_topic_words_string, lda_top_topic_words_list = get_lda_top_topic_words(lda_topics,num_topics,do_unique_search_words,n_search_words)\n",
    "            \n",
    "            # for case of only one topic, store matrix of word probs\n",
    "            if num_topics == 1:\n",
    "\n",
    "                lda_topic_word_probs = get_single_topic_word_probs(lda_topics,n_search_words_single_topic_analysis)\n",
    "                        \n",
    "                # add to matrix of word probs\n",
    "                topic_word_probs[counter_nboot,counter_article,:] = lda_topic_word_probs[:,0]\n",
    "\n",
    "            # count word frequencies\n",
    "            word_dict_count, word_counts_top, most_common_words_list, most_common_words_string = count_word_frequencies(article_processed_whole,n_search_words)\n",
    "            \n",
    "            # get jaccard similarity\n",
    "            jaccard_sim = get_jaccard_sim(lda_top_topic_words_list, most_common_words_list)\n",
    "            jaccard_sim_all[counter_nboot,counter_article,ind_num_topicss] = jaccard_sim\n",
    "            \n",
    "            # get word vectors for common words and lda top words\n",
    "            vec_most_common_words = processed_dictionary.doc2bow(most_common_words_list,return_missing=False)\n",
    "            vec_lda_top_topic_words = processed_dictionary.doc2bow(lda_top_topic_words_list,return_missing=False)\n",
    "            \n",
    "            # calculate cosine similarity\n",
    "            cosine_sim = calculate_cosine_similarity(vec_most_common_words,vec_lda_top_topic_words)\n",
    "            cosine_sim_all[counter_nboot,counter_article,ind_num_topicss] = cosine_sim\n",
    "            \n",
    "            # add number of words from counting top most frequent\n",
    "            number_most_common_words[counter_nboot,counter_article,ind_num_topicss] = len(most_common_words_list)\n",
    "            \n",
    "            if do_plot:\n",
    "                plt.figure(figsize=(15,5));\n",
    "                plt.bar(topics_means,means_sorted,yerr=std_sorted);\n",
    "                plt.ylabel('Mean probability');\n",
    "                sns.set_context('talk', font_scale=1.5);\n",
    "                plt.xticks(rotation=90);\n",
    "                plt.show();\n",
    "                plt.clf();\n",
    "    #             plt.savefig('./eda_figs/mean_prob_vs_topic_big_ten_resumes.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "                plt.figure(figsize=(15,5));\n",
    "                plt.bar(topics_freq,freq_sorted);\n",
    "                plt.ylabel('N');\n",
    "                # plt.xlabel('Topics')\n",
    "                sns.set_context('talk', font_scale=1.5);\n",
    "                plt.xticks(rotation=90);\n",
    "                plt.show();\n",
    "                plt.clf();\n",
    "    #             plt.savefig('./edafigs/frequency_vs_topic_big_ten_resumes.pdf')\n",
    "\n",
    "# save all output to pickle files\n",
    "pickle.dump( coherence_scores, open( './validation_data/coherence_scores.pkl', 'wb'))\n",
    "pickle.dump( perplexity_scores, open( './validation_data/perplexity_scores.pkl', 'wb'))\n",
    "pickle.dump( topic_word_probs, open( './validation_data/topic_word_probs.pkl', 'wb'))\n",
    "pickle.dump( jaccard_sim_all, open( './validation_data/jaccard_sim_all.pkl', 'wb'))\n",
    "pickle.dump( cosine_sim_all, open( './validation_data/cosine_sim_all.pkl', 'wb'))\n",
    "pickle.dump( cosine_sim_all, open( './validation_data/number_most_common_words.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sagarsetru/Documents/post PhD positions search/insightDataScience/project/debiaser'"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4, 2)\n",
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "coherence_scores_mean = np.mean(coherence_scores,axis=1)\n",
    "print(coherence_scores.shape)\n",
    "print(coherence_scores_mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting for perplexity and coherence scores\n",
    "\n",
    "# get means per bootstrap sampling\n",
    "coherence_scores_mean_per_sampling = np.mean(coherence_scores,axis=1)\n",
    "perplexity_scores_mean_per_sampling = np.mean(perplexity_scores,axis=1)\n",
    "\n",
    "# get mean across bootstrap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.51186944, 0.49164086],\n",
       "        [0.57954709, 0.52093418],\n",
       "        [0.47166151, 0.490692  ],\n",
       "        [0.41322083, 0.34657116]],\n",
       "\n",
       "       [[0.38860847, 0.39986555],\n",
       "        [0.63278217, 0.59506605],\n",
       "        [0.59818701, 0.60738289],\n",
       "        [0.51623939, 0.39482988]],\n",
       "\n",
       "       [[0.61204301, 0.57760808],\n",
       "        [0.35694274, 0.40510517],\n",
       "        [0.44635011, 0.41103484],\n",
       "        [0.45970607, 0.40096481]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-4.02900825, -4.23792879],\n",
       "        [-5.07480161, -5.29961568],\n",
       "        [-4.60041331, -4.85357503],\n",
       "        [-4.96198402, -5.15695703]],\n",
       "\n",
       "       [[-5.26447149, -5.45155033],\n",
       "        [-6.24191224, -6.44985396],\n",
       "        [-6.55337379, -6.74278562],\n",
       "        [-5.24764633, -5.44106885]],\n",
       "\n",
       "       [[-5.5982057 , -5.79121929],\n",
       "        [-5.29544008, -5.41833252],\n",
       "        [-4.17055247, -4.39684645],\n",
       "        [-5.40098499, -5.49089704]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.08196741, 0.04918041, 0.04098367, 0.03278691, 0.03278691,\n",
       "         0.02459017, 0.02459017, 0.02459017, 0.02459017, 0.02459017],\n",
       "        [0.04088081, 0.01886801, 0.01886801, 0.01572333, 0.01572333,\n",
       "         0.01572333, 0.01572333, 0.01257865, 0.01257865, 0.00943397],\n",
       "        [0.0314137 , 0.0314137 , 0.0314137 , 0.02617807, 0.02617806,\n",
       "         0.02094244, 0.02094243, 0.01570682, 0.01570681, 0.01570681],\n",
       "        [0.04901993, 0.02614394, 0.02287594, 0.02287593, 0.01633993,\n",
       "         0.01307193, 0.01307193, 0.01307193, 0.01307192, 0.00980393]],\n",
       "\n",
       "       [[0.02392364, 0.02392363, 0.01674652, 0.01674652, 0.01435415,\n",
       "         0.01435415, 0.01435414, 0.01196178, 0.01196177, 0.01196177],\n",
       "        [0.01595784, 0.01595781, 0.00957465, 0.00744693, 0.00638308,\n",
       "         0.00638308, 0.00531922, 0.00531921, 0.00425536, 0.00425536],\n",
       "        [0.00767482, 0.00767482, 0.00690732, 0.00613984, 0.00613983,\n",
       "         0.00613983, 0.00613982, 0.00537234, 0.00460486, 0.00460485],\n",
       "        [0.02619071, 0.02380972, 0.01428579, 0.01190482, 0.01190481,\n",
       "         0.01190481, 0.01190481, 0.01190481, 0.01190481, 0.01190481]],\n",
       "\n",
       "       [[0.01268509, 0.01268508, 0.01268508, 0.01268508, 0.01057089,\n",
       "         0.01057089, 0.01057089, 0.0084567 , 0.0084567 , 0.0084567 ],\n",
       "        [0.02189798, 0.02189798, 0.01459863, 0.01459863, 0.01459863,\n",
       "         0.01216551, 0.01216551, 0.01216551, 0.0121655 , 0.0121655 ],\n",
       "        [0.04464292, 0.03571432, 0.03571432, 0.02678572, 0.02678572,\n",
       "         0.02678572, 0.02678572, 0.02678572, 0.02678572, 0.02678572],\n",
       "        [0.03370821, 0.02696657, 0.01348323, 0.01123602, 0.01123602,\n",
       "         0.01123602, 0.01123602, 0.011236  , 0.0089888 , 0.0089888 ]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plotting for probability vs words, when single topics used\n",
    "topic_word_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.        , 0.25      ],\n",
       "        [0.71428571, 0.71428571],\n",
       "        [1.        , 0.42857143],\n",
       "        [1.        , 0.42857143]],\n",
       "\n",
       "       [[0.71428571, 0.5       ],\n",
       "        [0.83333333, 0.83333333],\n",
       "        [0.71428571, 0.71428571],\n",
       "        [0.5       , 0.5       ]],\n",
       "\n",
       "       [[0.71428571, 0.2       ],\n",
       "        [1.        , 0.66666667],\n",
       "        [0.5       , 0.5       ],\n",
       "        [0.625     , 0.625     ]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comparing word counting with lda\n",
    "jaccard_sim_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.        , 0.4       ],\n",
       "        [0.84515425, 0.84515425],\n",
       "        [1.        , 0.6       ],\n",
       "        [1.        , 0.6       ]],\n",
       "\n",
       "       [[0.84515425, 0.6761234 ],\n",
       "        [0.91287093, 0.91287093],\n",
       "        [0.84515425, 0.84515425],\n",
       "        [0.70710678, 0.70710678]],\n",
       "\n",
       "       [[0.84515425, 0.3380617 ],\n",
       "        [1.        , 0.8       ],\n",
       "        [0.70710678, 0.70710678],\n",
       "        [0.79056942, 0.79056942]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 5.,  5.],\n",
       "        [ 7.,  7.],\n",
       "        [ 5.,  5.],\n",
       "        [ 5.,  5.]],\n",
       "\n",
       "       [[ 7.,  7.],\n",
       "        [ 6.,  6.],\n",
       "        [ 7.,  7.],\n",
       "        [10., 10.]],\n",
       "\n",
       "       [[ 7.,  7.],\n",
       "        [ 5.,  5.],\n",
       "        [10., 10.],\n",
       "        [ 8.,  8.]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_most_common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         abcnews.go.com\n",
       "1          aljazeera.com\n",
       "2             apnews.com\n",
       "3                bbc.com\n",
       "4          bloomberg.com\n",
       "5          breitbart.com\n",
       "6       buzzfeednews.com\n",
       "7                cbn.com\n",
       "8            cbsnews.com\n",
       "9          csmonitor.com\n",
       "10               cnn.com\n",
       "11     thedailybeast.com\n",
       "12      democracynow.org\n",
       "13         factcheck.org\n",
       "14            forbes.com\n",
       "15           foxnews.com\n",
       "16          huffpost.com\n",
       "17       motherjones.com\n",
       "18             msnbc.com\n",
       "19    nationalreview.com\n",
       "20           nbcnews.com\n",
       "21            nypost.com\n",
       "22           nytimes.com\n",
       "23           newsmax.com\n",
       "24               npr.org\n",
       "25          politico.com\n",
       "26            reason.com\n",
       "27           reuters.com\n",
       "28             salon.com\n",
       "29         spectator.org\n",
       "30       theatlantic.com\n",
       "31       theguardian.com\n",
       "32           thehill.com\n",
       "33               wsj.com\n",
       "Name: domain, dtype: object"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lda_top_topic_words\n",
    "all_sides_domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load domain names from all sides media csv, write string for google search\n",
    "all_sides_with_domains = pd.read_csv('./all_sides_media_data/allsides_final_plus_others_with_domains.csv')\n",
    "\n",
    "all_sides_names = all_sides_with_domains['name']\n",
    "all_sides_domains = all_sides_with_domains['domain']\n",
    "\n",
    "all_sides_names_domains = pd.concat([all_sides_names,all_sides_domains],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>domain</th>\n",
       "      <th>google_query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABC News (Online)</td>\n",
       "      <td>abcnews.go.com</td>\n",
       "      <td>site:nytimes.com dow trading rates vaccine wsj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Al Jazeera</td>\n",
       "      <td>aljazeera.com</td>\n",
       "      <td>site:bloomberg.com dow trading rates vaccine wsj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Associated Press</td>\n",
       "      <td>apnews.com</td>\n",
       "      <td>site:reuters.com dow trading rates vaccine wsj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BBC News</td>\n",
       "      <td>bbc.com</td>\n",
       "      <td>site:wsj.com dow trading rates vaccine wsj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bloomberg</td>\n",
       "      <td>bloomberg.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Breitbart News</td>\n",
       "      <td>breitbart.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BuzzFeed News</td>\n",
       "      <td>buzzfeednews.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CBN</td>\n",
       "      <td>cbn.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CBS News</td>\n",
       "      <td>cbsnews.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Christian Science Monitor</td>\n",
       "      <td>csmonitor.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CNN (Web News)</td>\n",
       "      <td>cnn.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Daily Beast</td>\n",
       "      <td>thedailybeast.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Democracy Now</td>\n",
       "      <td>democracynow.org</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>FactCheck.org</td>\n",
       "      <td>factcheck.org</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Forbes</td>\n",
       "      <td>forbes.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Fox News (Online)</td>\n",
       "      <td>foxnews.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>HuffPost</td>\n",
       "      <td>huffpost.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Mother Jones</td>\n",
       "      <td>motherjones.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MSNBC</td>\n",
       "      <td>msnbc.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>National Review</td>\n",
       "      <td>nationalreview.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NBC News (Online)</td>\n",
       "      <td>nbcnews.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>New York Post</td>\n",
       "      <td>nypost.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>New York Times (Online News)</td>\n",
       "      <td>nytimes.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Newsmax - News</td>\n",
       "      <td>newsmax.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NPR Online News</td>\n",
       "      <td>npr.org</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Politico</td>\n",
       "      <td>politico.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Reason</td>\n",
       "      <td>reason.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Reuters</td>\n",
       "      <td>reuters.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Salon</td>\n",
       "      <td>salon.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>The American Spectator</td>\n",
       "      <td>spectator.org</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>The Atlantic</td>\n",
       "      <td>theatlantic.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>The Guardian</td>\n",
       "      <td>theguardian.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>The Hill</td>\n",
       "      <td>thehill.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>The Wall Street Journal</td>\n",
       "      <td>wsj.com</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            name              domain  \\\n",
       "0              ABC News (Online)      abcnews.go.com   \n",
       "1                     Al Jazeera       aljazeera.com   \n",
       "2               Associated Press          apnews.com   \n",
       "3                       BBC News             bbc.com   \n",
       "4                      Bloomberg       bloomberg.com   \n",
       "5                 Breitbart News       breitbart.com   \n",
       "6                  BuzzFeed News    buzzfeednews.com   \n",
       "7                            CBN             cbn.com   \n",
       "8                       CBS News         cbsnews.com   \n",
       "9      Christian Science Monitor       csmonitor.com   \n",
       "10                CNN (Web News)             cnn.com   \n",
       "11                   Daily Beast   thedailybeast.com   \n",
       "12                 Democracy Now    democracynow.org   \n",
       "13                 FactCheck.org       factcheck.org   \n",
       "14                        Forbes          forbes.com   \n",
       "15             Fox News (Online)         foxnews.com   \n",
       "16                      HuffPost        huffpost.com   \n",
       "17                  Mother Jones     motherjones.com   \n",
       "18                         MSNBC           msnbc.com   \n",
       "19               National Review  nationalreview.com   \n",
       "20             NBC News (Online)         nbcnews.com   \n",
       "21                 New York Post          nypost.com   \n",
       "22  New York Times (Online News)         nytimes.com   \n",
       "23                Newsmax - News         newsmax.com   \n",
       "24               NPR Online News             npr.org   \n",
       "25                      Politico        politico.com   \n",
       "26                        Reason          reason.com   \n",
       "27                       Reuters         reuters.com   \n",
       "28                         Salon           salon.com   \n",
       "29        The American Spectator       spectator.org   \n",
       "30                  The Atlantic     theatlantic.com   \n",
       "31                  The Guardian     theguardian.com   \n",
       "32                      The Hill         thehill.com   \n",
       "33       The Wall Street Journal             wsj.com   \n",
       "\n",
       "                                        google_query  \n",
       "0     site:nytimes.com dow trading rates vaccine wsj  \n",
       "1   site:bloomberg.com dow trading rates vaccine wsj  \n",
       "2     site:reuters.com dow trading rates vaccine wsj  \n",
       "3         site:wsj.com dow trading rates vaccine wsj  \n",
       "4                                                NaN  \n",
       "5                                                NaN  \n",
       "6                                                NaN  \n",
       "7                                                NaN  \n",
       "8                                                NaN  \n",
       "9                                                NaN  \n",
       "10                                               NaN  \n",
       "11                                               NaN  \n",
       "12                                               NaN  \n",
       "13                                               NaN  \n",
       "14                                               NaN  \n",
       "15                                               NaN  \n",
       "16                                               NaN  \n",
       "17                                               NaN  \n",
       "18                                               NaN  \n",
       "19                                               NaN  \n",
       "20                                               NaN  \n",
       "21                                               NaN  \n",
       "22                                               NaN  \n",
       "23                                               NaN  \n",
       "24                                               NaN  \n",
       "25                                               NaN  \n",
       "26                                               NaN  \n",
       "27                                               NaN  \n",
       "28                                               NaN  \n",
       "29                                               NaN  \n",
       "30                                               NaN  \n",
       "31                                               NaN  \n",
       "32                                               NaN  \n",
       "33                                               NaN  "
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sides_names_domains"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
