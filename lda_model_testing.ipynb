{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA model testing for downloaded article database for 'debiaser' data product\n",
    "#### Sagar Setru, September 21th, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief description using CoNVO framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context\n",
    "\n",
    "Some people are eager to get news from outside of their echo chamber. However, they do not know where to go outside of their echo chambers, and may also have some activation energy when it comes to seeking information from other sources. In the meantime, most newsfeeds only push you content that you agree with. You end up in an echo chamber, but may not have ever wanted to be in one in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need\n",
    "\n",
    "A way to find news articles from different yet reliable media sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision\n",
    "\n",
    "Debiaser, a data product (maybe Chrome plug-in?) that will recommend news articles similar in topic to the one currently being read, but from several pre-curated and reliable news media organizations across the political spectrum, for example, following the \"media bias chart\" here https://www.adfontesmedia.com/ or the \"media bias ratings\" here: https://www.allsides.com/media-bias/media-bias-ratings. The app will determine the main topics of the text of a news article, and then show links to similar articles from other news organizations.\n",
    "\n",
    "The product will generate topics for a given document via latent Dirichlet allocation (LDA) and then search news websites for the topic words generated.\n",
    "\n",
    "Caveats: Many of these articles may be behind paywalls. News aggregators already basically do this. How different is this than just searching Google using the title of an article?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outcome\n",
    "\n",
    "People who are motivated to engage in content outside of their echo chambers have a tool that enables them to quickly find news similar to what they are currently reading, but from a variety of news organizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing LDA on larger document corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import user defined functions for text processing\n",
    "from text_processing_functions import process_all_articles, remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del process_all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conda environment:\n",
      "insight\n"
     ]
    }
   ],
   "source": [
    "print('Conda environment:')\n",
    "print(os.environ['CONDA_DEFAULT_ENV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import text processing and NLP specific packages\n",
    "\n",
    "# for generating LDA models\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# for preprocessing documents\n",
    "from gensim.parsing.preprocessing import preprocess_documents\n",
    "\n",
    "# for counting frequency of words\n",
    "from collections import defaultdict\n",
    "\n",
    "import string\n",
    "\n",
    "# gensim\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "import gensim.corpora as corpora\n",
    "# from gensim.utils import simple_preprocess\n",
    "\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import string\n",
    "\n",
    "import spacy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stop_words_csv_to_list(full_file_name):\n",
    "    \"\"\"fxn that loads stop words list downloaded from git repo called 'news-stopwords'\"\"\"\n",
    "    \n",
    "    stop_words = pd.read_csv(full_file_name)\n",
    "\n",
    "    stop_words = stop_words['term']\n",
    "\n",
    "    stop_words = [word for word in stop_words]\n",
    "    \n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_corpus_dictionary_bow(texts,word_frequency_threshold):\n",
    "    \"\"\"fxn returns corpus, proc. dict, bag of words\"\"\"\n",
    "    \n",
    "    # Count word frequencies\n",
    "    frequency = defaultdict(int)\n",
    "    for text in texts:\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "\n",
    "    # Only keep words that appear more than set frequency, to produce the corpus\n",
    "    processed_corpus = [[token for token in text if frequency[token] > word_frequency_threshold] for text in texts]\n",
    "    \n",
    "    # generate a dictionary via gensim\n",
    "    processed_dictionary = Dictionary(processed_corpus)\n",
    "    \n",
    "    # generate bag of words of the corpus\n",
    "    bow_corpus = [processed_dictionary.doc2bow(text) for text in processed_corpus]\n",
    "    \n",
    "    return processed_corpus, processed_dictionary, bow_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose list of stop words\n",
    "\n",
    "# choose whether 1k, 10k, 100k, or nltk\n",
    "which_stop_words = '1k'\n",
    "# which_stop_words = '10k'\n",
    "# which_stop_words = '100k'\n",
    "# which_stop_words = 'nltk'\n",
    "\n",
    "stop_words_path = '/Users/sagarsetru/Documents/post PhD positions search/insightDataScience/project/debiaser/stop_words_db/news-stopwords-master/'\n",
    "\n",
    "\n",
    "if which_stop_words == '1k':\n",
    "    \n",
    "    # doing 1k words list\n",
    "    stop_words_file_name = 'sw1k.csv'\n",
    "    \n",
    "    # make full file name\n",
    "    stop_words_full_file_name = stop_words_path+stop_words_file_name\n",
    "    \n",
    "    # get list of stop words\n",
    "    stop_words = load_stop_words_csv_to_list(stop_words_full_file_name)\n",
    "    \n",
    "elif which_stop_words == '10k':\n",
    "    \n",
    "    # doing 10k words list\n",
    "    stop_words_file_name = 'sw10k.csv'\n",
    "\n",
    "    # make full file name\n",
    "    stop_words_full_file_name = stop_words_path+stop_words_file_name\n",
    "    \n",
    "    # get list of stop words\n",
    "    stop_words = load_stop_words_csv_to_list(stop_words_full_file_name)\n",
    "\n",
    "elif which_stop_words == '100k':\n",
    "    \n",
    "    # doing 100k\n",
    "    stop_words_file_name = 'sw100k.csv'  \n",
    "    \n",
    "    # get full file name\n",
    "    stop_words_full_file_name = stop_words_path+stop_words_file_name\n",
    "    \n",
    "    # get list of stop words\n",
    "    stop_words = load_stop_words_csv_to_list(stop_words_full_file_name)\n",
    "\n",
    "\n",
    "elif which_stop_words == 'nltk':\n",
    "    # import from nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "else:\n",
    "    print('Select proper variable name for \"which_stop_words\"')\n",
    "    \n",
    "# adding custom words\n",
    "stop_words.append('said')\n",
    "stop_words.append('youre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sagarsetru/anaconda3/envs/insight/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3145: DtypeWarning: Columns (12,13) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# load csv of processed data to pandas dataframe\n",
    "articles_df = pd.read_csv('./all_the_news/all_news_df_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>index.1</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>publication</th>\n",
       "      <th>category</th>\n",
       "      <th>digital</th>\n",
       "      <th>section</th>\n",
       "      <th>url</th>\n",
       "      <th>article_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Agent Cooper in Twin Peaks is the audience: on...</td>\n",
       "      <td>\\nTasha Robinson\\n</td>\n",
       "      <td>2017-05-31</td>\n",
       "      <td>And never more so than in Showtime’s new...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>AI, the humanity!</td>\n",
       "      <td>\\nSam Byford\\n</td>\n",
       "      <td>2017-05-30</td>\n",
       "      <td>AlphaGo’s victory isn’t a defeat for hum...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>The Viral Machine</td>\n",
       "      <td>\\nKaitlyn Tiffany\\n</td>\n",
       "      <td>2017-05-25</td>\n",
       "      <td>Super Deluxe built a weird internet empi...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>How Anker is beating Apple and Samsung at thei...</td>\n",
       "      <td>\\nNick Statt\\n</td>\n",
       "      <td>2017-05-22</td>\n",
       "      <td>Steven Yang quit his job at Google in th...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Tour Black Panther’s reimagined homeland with ...</td>\n",
       "      <td>\\nKwame Opam\\n</td>\n",
       "      <td>2017-05-15</td>\n",
       "      <td>Ahead of Black Panther’s 2018 theatrical...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  index.1  id                                              title  \\\n",
       "0      0        0   1  Agent Cooper in Twin Peaks is the audience: on...   \n",
       "1      1        1   2                                  AI, the humanity!   \n",
       "2      2        2   3                                  The Viral Machine   \n",
       "3      3        3   4  How Anker is beating Apple and Samsung at thei...   \n",
       "4      4        4   5  Tour Black Panther’s reimagined homeland with ...   \n",
       "\n",
       "                author        date  \\\n",
       "0   \\nTasha Robinson\\n  2017-05-31   \n",
       "1       \\nSam Byford\\n  2017-05-30   \n",
       "2  \\nKaitlyn Tiffany\\n  2017-05-25   \n",
       "3       \\nNick Statt\\n  2017-05-22   \n",
       "4       \\nKwame Opam\\n  2017-05-15   \n",
       "\n",
       "                                             content    year  month  \\\n",
       "0        And never more so than in Showtime’s new...  2017.0    5.0   \n",
       "1        AlphaGo’s victory isn’t a defeat for hum...  2017.0    5.0   \n",
       "2        Super Deluxe built a weird internet empi...  2017.0    5.0   \n",
       "3        Steven Yang quit his job at Google in th...  2017.0    5.0   \n",
       "4        Ahead of Black Panther’s 2018 theatrical...  2017.0    5.0   \n",
       "\n",
       "  publication  category  digital section  url  article_length  \n",
       "0       Verge  Longform      1.0     NaN  NaN            2121  \n",
       "1       Verge  Longform      1.0     NaN  NaN            1948  \n",
       "2       Verge  Longform      1.0     NaN  NaN            3011  \n",
       "3       Verge  Longform      1.0     NaN  NaN            3281  \n",
       "4       Verge  Longform      1.0     NaN  NaN             239  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview the data\n",
    "articles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    182636.000000\n",
       "mean        862.016393\n",
       "std         864.620185\n",
       "min          51.000000\n",
       "25%         397.000000\n",
       "50%         693.000000\n",
       "75%        1069.000000\n",
       "max       50517.000000\n",
       "Name: article_length, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df['article_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get random n_sample number articles for testing\n",
    "n_sample = 1000\n",
    "\n",
    "articles_df_test = articles_df.sample(n=n_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# get just the articles content and titles\n",
    "articles_content = articles_df_test['content'].astype('str')\n",
    "articles_titles = articles_df_test['title'].astype('str')\n",
    "\n",
    "# check for nans; if there are any, make sure to not add nan\n",
    "print(articles_df['title'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: following nice tutorial provided here: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#10removestopwordsmakebigramsandlemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge titles and content\n",
    "articles_full = []\n",
    "\n",
    "for content,title in zip(articles_content, articles_titles):\n",
    "    \n",
    "    # don't add word 'nan'\n",
    "    if title == 'nan':\n",
    "        \n",
    "        print(title)\n",
    "        \n",
    "        articles_full.append(content)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        articles_full.append(title+content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1000 in this corpus.\n"
     ]
    }
   ],
   "source": [
    "# show number of documents\n",
    "n_documents = len(articles_full)\n",
    "print(f'There are {n_documents} in this corpus.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process documents\n",
    "articles_processed = process_all_articles(articles_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "# articles_processed = remove_stopwords(articles_processed,stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(articles_processed)\n",
    "# articles_processed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build ngram models (bi, tri, quad)\n",
    "\n",
    "ngram_min_count = 2;\n",
    "\n",
    "bigram_threshold = 25;\n",
    "trigram_threshold = 15;\n",
    "quadgram_theshold = 100;\n",
    "\n",
    "bigram = gensim.models.Phrases(articles_processed, min_count=ngram_min_count, threshold=bigram_threshold) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[articles_processed], threshold=trigram_threshold)\n",
    "quadgram = gensim.models.Phrases(trigram[articles_processed], threshold=quadgram_theshold)\n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "quadgram_mod = gensim.models.phrases.Phraser(quadgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fxns for bi, tri, quadgrams, and lemmatization\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def make_quadgrams(texts):\n",
    "    return [quadgram_mod[trigram_mod[bigram_mod[doc]]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for text in texts:\n",
    "        doc = nlp(\" \".join(text)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-75b231b38218>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_lg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# # Do lemmatization keeping only noun, adj, vb, adv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/insight/lib/python3.8/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/insight/lib/python3.8/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "# remove stop words\n",
    "articles_processed = remove_stopwords(articles_processed,stop_words)\n",
    "\n",
    "# for up to quad grams\n",
    "articles_processed_ngrams = make_quadgrams(articles_processed)\n",
    "\n",
    "# # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# # Do lemmatization keeping only noun, adj, vb, adv\n",
    "articles_processed_ngrams_lemmaed = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "# print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(articles_processed)\n",
    "\n",
    "# Create Corpus\n",
    "texts = articles_processed\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_full[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brock',\n",
       " 'turner',\n",
       " 'judge',\n",
       " 'backs',\n",
       " 'out',\n",
       " 'of',\n",
       " 'child',\n",
       " 'porn',\n",
       " 'case',\n",
       " 'cnn',\n",
       " 'the',\n",
       " 'judge',\n",
       " 'at',\n",
       " 'the',\n",
       " 'center',\n",
       " 'of',\n",
       " 'controversial',\n",
       " 'sentence',\n",
       " 'involving',\n",
       " 'brock',\n",
       " 'turner',\n",
       " 'former',\n",
       " 'stanford',\n",
       " 'student',\n",
       " 'convicted',\n",
       " 'of',\n",
       " 'sexual',\n",
       " 'assault',\n",
       " 'has',\n",
       " 'recused',\n",
       " 'himself',\n",
       " 'from',\n",
       " 'making',\n",
       " 'decision',\n",
       " 'in',\n",
       " 'another',\n",
       " 'sex',\n",
       " 'case',\n",
       " 'santa',\n",
       " 'clara',\n",
       " 'county',\n",
       " 'judge',\n",
       " 'aaron',\n",
       " 'persky',\n",
       " 'was',\n",
       " 'expected',\n",
       " 'to',\n",
       " 'decide',\n",
       " 'on',\n",
       " 'thursday',\n",
       " 'whether',\n",
       " 'to',\n",
       " 'reduce',\n",
       " 'plumber',\n",
       " 'felony',\n",
       " 'conviction',\n",
       " 'for',\n",
       " 'possession',\n",
       " 'of',\n",
       " 'child',\n",
       " 'pornography',\n",
       " 'to',\n",
       " 'misdemeanor',\n",
       " 'but',\n",
       " 'has',\n",
       " 'now',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'take',\n",
       " 'himself',\n",
       " 'off',\n",
       " 'the',\n",
       " 'case',\n",
       " 'apparently',\n",
       " 'in',\n",
       " 'light',\n",
       " 'of',\n",
       " 'media',\n",
       " 'coverage',\n",
       " 'while',\n",
       " 'on',\n",
       " 'vacation',\n",
       " 'earlier',\n",
       " 'this',\n",
       " 'month',\n",
       " 'my',\n",
       " 'family',\n",
       " 'and',\n",
       " 'were',\n",
       " 'exposed',\n",
       " 'to',\n",
       " 'publicity',\n",
       " 'surrounding',\n",
       " 'this',\n",
       " 'case',\n",
       " 'persky',\n",
       " 'said',\n",
       " 'in',\n",
       " 'written',\n",
       " 'ruling',\n",
       " 'obtained',\n",
       " 'by',\n",
       " 'cnn',\n",
       " 'this',\n",
       " 'publicity',\n",
       " 'has',\n",
       " 'resulted',\n",
       " 'in',\n",
       " 'personal',\n",
       " 'family',\n",
       " 'situation',\n",
       " 'such',\n",
       " 'that',\n",
       " 'person',\n",
       " 'aware',\n",
       " 'of',\n",
       " 'the',\n",
       " 'facts',\n",
       " 'might',\n",
       " 'reasonably',\n",
       " 'entertain',\n",
       " 'doubt',\n",
       " 'that',\n",
       " 'the',\n",
       " 'judge',\n",
       " 'would',\n",
       " 'be',\n",
       " 'able',\n",
       " 'to',\n",
       " 'be',\n",
       " 'impartial',\n",
       " 'it',\n",
       " 'not',\n",
       " 'clear',\n",
       " 'what',\n",
       " 'family',\n",
       " 'situation',\n",
       " 'led',\n",
       " 'to',\n",
       " 'persky',\n",
       " 'decision',\n",
       " 'the',\n",
       " 'judge',\n",
       " 'has',\n",
       " 'faced',\n",
       " 'harsh',\n",
       " 'criticism',\n",
       " 'since',\n",
       " 'he',\n",
       " 'sentenced',\n",
       " 'former',\n",
       " 'stanford',\n",
       " 'swimmer',\n",
       " 'turner',\n",
       " 'to',\n",
       " 'six',\n",
       " 'months',\n",
       " 'in',\n",
       " 'county',\n",
       " 'jail',\n",
       " 'for',\n",
       " 'sexually',\n",
       " 'assaulting',\n",
       " 'an',\n",
       " 'unconscious',\n",
       " 'woman',\n",
       " 'prosecutors',\n",
       " 'had',\n",
       " 'asked',\n",
       " 'that',\n",
       " 'turner',\n",
       " 'be',\n",
       " 'sentenced',\n",
       " 'to',\n",
       " 'six',\n",
       " 'years',\n",
       " 'in',\n",
       " 'prison',\n",
       " 'for',\n",
       " 'the',\n",
       " 'january',\n",
       " 'assault',\n",
       " 'outraged',\n",
       " 'by',\n",
       " 'what',\n",
       " 'they',\n",
       " 'believed',\n",
       " 'was',\n",
       " 'an',\n",
       " 'unusually',\n",
       " 'light',\n",
       " 'sentence',\n",
       " 'critics',\n",
       " 'led',\n",
       " 'by',\n",
       " 'stanford',\n",
       " 'law',\n",
       " 'professor',\n",
       " 'michele',\n",
       " 'dauber',\n",
       " 'launched',\n",
       " 'campaign',\n",
       " 'to',\n",
       " 'recall',\n",
       " 'persky',\n",
       " 'while',\n",
       " 'examining',\n",
       " 'perksy',\n",
       " 'record',\n",
       " 'dauber',\n",
       " 'found',\n",
       " 'case',\n",
       " 'involving',\n",
       " 'the',\n",
       " 'plumber',\n",
       " 'robert',\n",
       " 'chain',\n",
       " 'the',\n",
       " 'year',\n",
       " 'old',\n",
       " 'was',\n",
       " 'arrested',\n",
       " 'in',\n",
       " 'may',\n",
       " 'and',\n",
       " 'accused',\n",
       " 'of',\n",
       " 'downloading',\n",
       " 'pornographic',\n",
       " 'images',\n",
       " 'of',\n",
       " 'children',\n",
       " 'persky',\n",
       " 'sentenced',\n",
       " 'him',\n",
       " 'to',\n",
       " 'four',\n",
       " 'days',\n",
       " 'in',\n",
       " 'jail',\n",
       " 'according',\n",
       " 'to',\n",
       " 'dauber',\n",
       " 'research',\n",
       " 'others',\n",
       " 'convicted',\n",
       " 'of',\n",
       " 'similar',\n",
       " 'crimes',\n",
       " 'in',\n",
       " 'santa',\n",
       " 'clara',\n",
       " 'county',\n",
       " 'got',\n",
       " 'at',\n",
       " 'least',\n",
       " 'six',\n",
       " 'months',\n",
       " 'in',\n",
       " 'jail',\n",
       " 'persky',\n",
       " 'also',\n",
       " 'indicated',\n",
       " 'he',\n",
       " 'would',\n",
       " 'be',\n",
       " 'willing',\n",
       " 'to',\n",
       " 'reduce',\n",
       " 'chain',\n",
       " 'felony',\n",
       " 'to',\n",
       " 'misdemeanor',\n",
       " 'if',\n",
       " 'he',\n",
       " 'stayed',\n",
       " 'out',\n",
       " 'of',\n",
       " 'trouble',\n",
       " 'defense',\n",
       " 'attorneys',\n",
       " 'often',\n",
       " 'press',\n",
       " 'to',\n",
       " 'have',\n",
       " 'felonies',\n",
       " 'reduced',\n",
       " 'to',\n",
       " 'misdemeanors',\n",
       " 'because',\n",
       " 'it',\n",
       " 'makes',\n",
       " 'finding',\n",
       " 'employment',\n",
       " 'easier',\n",
       " 'among',\n",
       " 'potential',\n",
       " 'employers',\n",
       " 'who',\n",
       " 'do',\n",
       " 'background',\n",
       " 'checks',\n",
       " 'the',\n",
       " 'issue',\n",
       " 'will',\n",
       " 'now',\n",
       " 'be',\n",
       " 'decided',\n",
       " 'by',\n",
       " 'different',\n",
       " 'judge',\n",
       " 'in',\n",
       " 'october',\n",
       " 'chain',\n",
       " 'attorney',\n",
       " 'brian',\n",
       " 'madden',\n",
       " 'declined',\n",
       " 'to',\n",
       " 'comment',\n",
       " 'while',\n",
       " 'pleased',\n",
       " 'with',\n",
       " 'persky',\n",
       " 'decision',\n",
       " 'to',\n",
       " 'recuse',\n",
       " 'himself',\n",
       " 'dauber',\n",
       " 'said',\n",
       " 'the',\n",
       " 'recall',\n",
       " 'campaign',\n",
       " 'will',\n",
       " 'go',\n",
       " 'forward',\n",
       " 'we',\n",
       " 're',\n",
       " 'building',\n",
       " 'support',\n",
       " 'and',\n",
       " 'endorsements',\n",
       " 'from',\n",
       " 'elected',\n",
       " 'officials',\n",
       " 'she',\n",
       " 'said',\n",
       " 'she',\n",
       " 'said',\n",
       " 'there',\n",
       " 'will',\n",
       " 'also',\n",
       " 'be',\n",
       " 'rally',\n",
       " 'september',\n",
       " 'at',\n",
       " 'the',\n",
       " 'hall',\n",
       " 'of',\n",
       " 'justice',\n",
       " 'in',\n",
       " 'downtown',\n",
       " 'san',\n",
       " 'jose',\n",
       " 'the',\n",
       " 'day',\n",
       " 'turner',\n",
       " 'is',\n",
       " 'expected',\n",
       " 'to',\n",
       " 'be',\n",
       " 'released',\n",
       " 'from',\n",
       " 'jail',\n",
       " 'persky',\n",
       " 'has',\n",
       " 'not',\n",
       " 'responded',\n",
       " 'to',\n",
       " 'requests',\n",
       " 'for',\n",
       " 'comment']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim.utils.simple_preprocess(articles_full[0], deacc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
