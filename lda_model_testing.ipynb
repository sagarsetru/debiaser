{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA model testing for downloaded article database for 'debiaser' data product\n",
    "#### Sagar Setru, September 21th, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief description using CoNVO framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context\n",
    "\n",
    "Some people are eager to get news from outside of their echo chamber. However, they do not know where to go outside of their echo chambers, and may also have some activation energy when it comes to seeking information from other sources. In the meantime, most newsfeeds only push you content that you agree with. You end up in an echo chamber, but may not have ever wanted to be in one in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need\n",
    "\n",
    "A way to find news articles from different yet reliable media sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision\n",
    "\n",
    "Debiaser, a data product (maybe Chrome plug-in?) that will recommend news articles similar in topic to the one currently being read, but from several pre-curated and reliable news media organizations across the political spectrum, for example, following the \"media bias chart\" here https://www.adfontesmedia.com/ or the \"media bias ratings\" here: https://www.allsides.com/media-bias/media-bias-ratings. The app will determine the main topics of the text of a news article, and then show links to similar articles from other news organizations.\n",
    "\n",
    "The product will generate topics for a given document via latent Dirichlet allocation (LDA) and then search news websites for the topic words generated.\n",
    "\n",
    "Caveats: Many of these articles may be behind paywalls. News aggregators already basically do this. How different is this than just searching Google using the title of an article?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outcome\n",
    "\n",
    "People who are motivated to engage in content outside of their echo chambers have a tool that enables them to quickly find news similar to what they are currently reading, but from a variety of news organizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing LDA on larger document corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import user defined functions for text processing\n",
    "from text_processing_functions import process_all_articles, remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del process_all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conda environment:\n",
      "base\n"
     ]
    }
   ],
   "source": [
    "print('Conda environment:')\n",
    "print(os.environ['CONDA_DEFAULT_ENV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# import text processing and NLP specific packages\n",
    "\n",
    "# for generating LDA models\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# for preprocessing documents\n",
    "from gensim.parsing.preprocessing import preprocess_documents\n",
    "\n",
    "# for counting frequency of words\n",
    "from collections import defaultdict\n",
    "\n",
    "import string\n",
    "\n",
    "# gensim\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "import gensim.corpora as corpora\n",
    "# from gensim.utils import simple_preprocess\n",
    "\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import spacy\n",
    "\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing on HTML data\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request,sys,time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stop_words_csv_to_list(full_file_name):\n",
    "    \"\"\"fxn that loads stop words list downloaded from git repo called 'news-stopwords'\"\"\"\n",
    "    \n",
    "    stop_words = pd.read_csv(full_file_name)\n",
    "\n",
    "    stop_words = stop_words['term']\n",
    "\n",
    "    stop_words = [word for word in stop_words]\n",
    "    \n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_corpus_dictionary_bow(texts,word_frequency_threshold):\n",
    "    \"\"\"fxn returns corpus, proc. dict, bag of words\"\"\"\n",
    "    \n",
    "    # Count word frequencies\n",
    "    frequency = defaultdict(int)\n",
    "    for text in texts:\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "\n",
    "    # Only keep words that appear more than set frequency, to produce the corpus\n",
    "    processed_corpus = [[token for token in text if frequency[token] > word_frequency_threshold] for text in texts]\n",
    "    \n",
    "    # generate a dictionary via gensim\n",
    "    processed_dictionary = Dictionary(processed_corpus)\n",
    "    \n",
    "    # generate bag of words of the corpus\n",
    "    bow_corpus = [processed_dictionary.doc2bow(text) for text in processed_corpus]\n",
    "    \n",
    "    return processed_corpus, processed_dictionary, bow_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose list of stop words\n",
    "\n",
    "# choose whether 1k, 10k, 100k, or nltk\n",
    "which_stop_words = '1k'\n",
    "# which_stop_words = '10k'\n",
    "# which_stop_words = '100k'\n",
    "# which_stop_words = 'nltk'\n",
    "\n",
    "stop_words_path = './stop_words_db/news-stopwords-master/'\n",
    "\n",
    "\n",
    "if which_stop_words == '1k':\n",
    "    \n",
    "    # doing 1k words list\n",
    "    stop_words_file_name = 'sw1k.csv'\n",
    "    \n",
    "    # make full file name\n",
    "    stop_words_full_file_name = stop_words_path+stop_words_file_name\n",
    "    \n",
    "    # get list of stop words\n",
    "    stop_words = load_stop_words_csv_to_list(stop_words_full_file_name)\n",
    "    \n",
    "elif which_stop_words == '10k':\n",
    "    \n",
    "    # doing 10k words list\n",
    "    stop_words_file_name = 'sw10k.csv'\n",
    "\n",
    "    # make full file name\n",
    "    stop_words_full_file_name = stop_words_path+stop_words_file_name\n",
    "    \n",
    "    # get list of stop words\n",
    "    stop_words = load_stop_words_csv_to_list(stop_words_full_file_name)\n",
    "\n",
    "elif which_stop_words == '100k':\n",
    "    \n",
    "    # doing 100k\n",
    "    stop_words_file_name = 'sw100k.csv'  \n",
    "    \n",
    "    # get full file name\n",
    "    stop_words_full_file_name = stop_words_path+stop_words_file_name\n",
    "    \n",
    "    # get list of stop words\n",
    "    stop_words = load_stop_words_csv_to_list(stop_words_full_file_name)\n",
    "\n",
    "\n",
    "elif which_stop_words == 'nltk':\n",
    "    # import from nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "else:\n",
    "    print('Select proper variable name for \"which_stop_words\"')\n",
    "    \n",
    "# adding custom words\n",
    "stop_words.append('said')\n",
    "stop_words.append('youre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3071: DtypeWarning: Columns (12,13) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# load csv of processed data to pandas dataframe\n",
    "articles_df = pd.read_csv('./all_the_news/all_news_df_processed.csv')\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>index.1</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>publication</th>\n",
       "      <th>category</th>\n",
       "      <th>digital</th>\n",
       "      <th>section</th>\n",
       "      <th>url</th>\n",
       "      <th>article_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Agent Cooper in Twin Peaks is the audience: on...</td>\n",
       "      <td>\\nTasha Robinson\\n</td>\n",
       "      <td>2017-05-31</td>\n",
       "      <td>And never more so than in Showtime’s new...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>AI, the humanity!</td>\n",
       "      <td>\\nSam Byford\\n</td>\n",
       "      <td>2017-05-30</td>\n",
       "      <td>AlphaGo’s victory isn’t a defeat for hum...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>The Viral Machine</td>\n",
       "      <td>\\nKaitlyn Tiffany\\n</td>\n",
       "      <td>2017-05-25</td>\n",
       "      <td>Super Deluxe built a weird internet empi...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>How Anker is beating Apple and Samsung at thei...</td>\n",
       "      <td>\\nNick Statt\\n</td>\n",
       "      <td>2017-05-22</td>\n",
       "      <td>Steven Yang quit his job at Google in th...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Tour Black Panther’s reimagined homeland with ...</td>\n",
       "      <td>\\nKwame Opam\\n</td>\n",
       "      <td>2017-05-15</td>\n",
       "      <td>Ahead of Black Panther’s 2018 theatrical...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  index.1  id                                              title  \\\n",
       "0      0        0   1  Agent Cooper in Twin Peaks is the audience: on...   \n",
       "1      1        1   2                                  AI, the humanity!   \n",
       "2      2        2   3                                  The Viral Machine   \n",
       "3      3        3   4  How Anker is beating Apple and Samsung at thei...   \n",
       "4      4        4   5  Tour Black Panther’s reimagined homeland with ...   \n",
       "\n",
       "                author        date  \\\n",
       "0   \\nTasha Robinson\\n  2017-05-31   \n",
       "1       \\nSam Byford\\n  2017-05-30   \n",
       "2  \\nKaitlyn Tiffany\\n  2017-05-25   \n",
       "3       \\nNick Statt\\n  2017-05-22   \n",
       "4       \\nKwame Opam\\n  2017-05-15   \n",
       "\n",
       "                                             content    year  month  \\\n",
       "0        And never more so than in Showtime’s new...  2017.0    5.0   \n",
       "1        AlphaGo’s victory isn’t a defeat for hum...  2017.0    5.0   \n",
       "2        Super Deluxe built a weird internet empi...  2017.0    5.0   \n",
       "3        Steven Yang quit his job at Google in th...  2017.0    5.0   \n",
       "4        Ahead of Black Panther’s 2018 theatrical...  2017.0    5.0   \n",
       "\n",
       "  publication  category  digital section  url  article_length  \n",
       "0       Verge  Longform      1.0     NaN  NaN            2121  \n",
       "1       Verge  Longform      1.0     NaN  NaN            1948  \n",
       "2       Verge  Longform      1.0     NaN  NaN            3011  \n",
       "3       Verge  Longform      1.0     NaN  NaN            3281  \n",
       "4       Verge  Longform      1.0     NaN  NaN             239  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview the data\n",
    "articles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    182636.000000\n",
       "mean        862.016393\n",
       "std         864.620185\n",
       "min          51.000000\n",
       "25%         397.000000\n",
       "50%         693.000000\n",
       "75%        1069.000000\n",
       "max       50517.000000\n",
       "Name: article_length, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df['article_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get random n_sample number articles for testing\n",
    "n_sample = 182636\n",
    "\n",
    "# articles_df_test = articles_df.sample(n=n_sample)\n",
    "articles_df_test = articles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# get just the articles content and titles\n",
    "articles_content = articles_df_test['content'].astype('str')\n",
    "articles_titles = articles_df_test['title'].astype('str')\n",
    "\n",
    "# check for nans; if there are any, make sure to not add nan\n",
    "print(articles_df['title'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: following nice tutorial provided here: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#10removestopwordsmakebigramsandlemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "# merge titles and content\n",
    "articles_full = []\n",
    "\n",
    "for content,title in zip(articles_content, articles_titles):\n",
    "    \n",
    "    # don't add word 'nan'\n",
    "    if title == 'nan':\n",
    "        \n",
    "        print(title)\n",
    "        \n",
    "        articles_full.append(content)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        articles_full.append(title+content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 182636 in this corpus.\n"
     ]
    }
   ],
   "source": [
    "# show number of documents\n",
    "n_documents = len(articles_full)\n",
    "print(f'There are {n_documents} in this corpus.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# process documents\n",
    "articles_processed = process_all_articles(articles_full,nlp=[])\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-6fc1e9f22459>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0marticles_processed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'./articles_processed.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'done'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# articles_processed[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import pickle\n",
    "pickle.dump( articles_processed, open( './articles_processed.pkl', 'wb'))\n",
    "print('done')\n",
    "# articles_processed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182636"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running...\n",
      "making bigram\n",
      "making trigram\n",
      "making quadgram\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# build ngram models (bi, tri, quad)\n",
    "# NOTE: CONSIDER TRYING THIS FOR BETTER NGRAMS:\n",
    "# https://medium.com/@manjunathhiremath.mh/identifying-bigrams-trigrams-and-four-grams-using-word2vec-dea346130eb\n",
    "\n",
    "ngram_min_count = 2;\n",
    "\n",
    "bigram_threshold = 25;\n",
    "trigram_threshold = 15;\n",
    "quadgram_theshold = 100;\n",
    "\n",
    "print('running...')\n",
    "bigram = gensim.models.Phrases(articles_processed, min_count=ngram_min_count, threshold=bigram_threshold) # higher threshold fewer phrases.\n",
    "print('making bigram')\n",
    "trigram = gensim.models.Phrases(bigram[articles_processed], threshold=trigram_threshold)\n",
    "print('making trigram')\n",
    "quadgram = gensim.models.Phrases(trigram[articles_processed], threshold=quadgram_theshold)\n",
    "print('making quadgram')\n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "quadgram_mod = gensim.models.phrases.Phraser(quadgram)\n",
    "\n",
    "pickle.dump( bigram_mod, open( './bigram_mod.pkl', 'wb'))\n",
    "pickle.dump( trigram_mod, open( './trigram_mod.pkl', 'wb'))\n",
    "pickle.dump( quadgram_mod, open( './quadgram_mod.pkl', 'wb'))\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "with open('./bigram_mod.pkl', 'rb') as pickle_file:\n",
    "    bigram_mod = pickle.load(pickle_file)\n",
    "\n",
    "with open('./trigram_mod.pkl', 'rb') as pickle_file:\n",
    "    trigram_mod = pickle.load(pickle_file)\n",
    "    \n",
    "with open('./quadgram_mod.pkl', 'rb') as pickle_file:\n",
    "    quadgram_mod = pickle.load(pickle_file)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fxns for bi, tri, quadgrams, and lemmatization\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def make_quadgrams(texts):\n",
    "    return [quadgram_mod[trigram_mod[bigram_mod[doc]]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for text in texts:\n",
    "        doc = nlp(\" \".join(text)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running stop words\n",
      "done stop words\n",
      "running ngrams\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# remove stop words\n",
    "print('running stop words')\n",
    "articles_processed = remove_stopwords(articles_processed,stop_words)\n",
    "print('done stop words')\n",
    "\n",
    "# for up to quad grams\n",
    "print('running ngrams')\n",
    "articles_processed_ngrams = make_quadgrams(articles_processed)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize spacy 'en_core_web_sm' model\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for up to quad grams\n",
    "# articles_processed_ngrams = make_quadgrams(articles_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatizing\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# # Do lemmatization keeping only noun, adj, vb, adv\n",
    "print('lemmatizing')\n",
    "articles_processed_ngrams_lemmaed = lemmatization(articles_processed_ngrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "pickle.dump( articles_processed_ngrams_lemmaed, open( './articles_processed_ngrams_lemmaed.pkl', 'wb'))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# articles_processed_ngrams_lemmaed[322]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary\n",
    "print('making id2word')\n",
    "id2word = corpora.Dictionary(articles_processed)\n",
    "print('id2word done')\n",
    "\n",
    "print('making freq of words')\n",
    "# make frequency of words\n",
    "corpus = [id2word.doc2bow(article) for article in articles_processed_ngrams_lemmaed]\n",
    "print('freq of words done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "pickle.dump( id2word, open( './id2word.pkl', 'wb'))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "pickle.dump( corpus, open( './corpus.pkl', 'wb'))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100 200]\n"
     ]
    }
   ],
   "source": [
    "# set lda model n topics\n",
    "\n",
    "# lda_n_topics = np.arange(10,110,10)\n",
    "# lda_n_topics = np.concatenate((np.arange(1,2),lda_n_topics))\n",
    "\n",
    "# lda_n_topics = np.arange(60,110,10)\n",
    "lda_n_topics = np.concatenate(([100],[200]))\n",
    "print(lda_n_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set LDA hyperparameters\n",
    "n_docs_chunksize = 60000\n",
    "\n",
    "n_training_passes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train lda model\n",
      "n_topics: 100\n",
      " \n",
      "model training done\n",
      "calculating perplexity\n",
      "Perplexity Score:  -9.95008409124062\n",
      "saving: ./lda_models/lda_model_n_topics_100_n_passes_100_n_docs_chunksize_60000.pkl\n",
      " \n",
      "train lda model\n",
      "n_topics: 200\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# loop through number of topics\n",
    "\n",
    "n_topics_coherence_dict = {}\n",
    "n_topics_perplexity_dict = {}\n",
    "\n",
    "for n_topic in lda_n_topics:\n",
    "\n",
    "    print('train lda model')\n",
    "    print(f'n_topics: {n_topic}')\n",
    "    print(' ')\n",
    "    lda_model = gensim.models.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=n_topic, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=n_docs_chunksize,\n",
    "                                           passes=n_training_passes,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "    \n",
    "    print('model training done')\n",
    "    \n",
    "#     print('determining coherence')\n",
    "#     # get model coherence\n",
    "#     coherence_model_lda = CoherenceModel(model=lda_model, texts=articles_processed_ngrams_lemmaed, dictionary=id2word, coherence='c_v')\n",
    "#     coherence_lda = coherence_model_lda.get_coherence()\n",
    "#     n_topics_coherence_dict[n_topic] = coherence_lda\n",
    "    \n",
    "    print('calculating perplexity')\n",
    "    perplexity = lda_model.log_perplexity(corpus)\n",
    "    n_topics_perplexity_dict[n_topic] = perplexity\n",
    "    \n",
    "\n",
    "#     print('Coherence Score: ', coherence_lda)\n",
    "    print('Perplexity Score: ',perplexity)\n",
    "\n",
    "    pkl_file_name = f'./lda_models/lda_model_n_topics_{n_topic}_n_passes_{n_training_passes}_n_docs_chunksize_{n_docs_chunksize}.pkl'\n",
    "    print(f'saving: {pkl_file_name}')\n",
    "    pickle.dump( lda_model, open( pkl_file_name, 'wb'))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.922071655468258\n",
      "{1: -8.922071655468258}\n"
     ]
    }
   ],
   "source": [
    "# get model coherence\n",
    "# coherence_model_lda = CoherenceModel(model=lda_model, texts=articles_processed_ngrams_lemmaed, dictionary=id2word, coherence='c_v')\n",
    "# coherence_lda = coherence_model_lda.get_coherence()\n",
    "# n_topics_coherence_dict[n_topic] = coherence_lda\n",
    "\n",
    "# get model perplexity\n",
    "# print('calculating perplexity')\n",
    "# perplexity = lda_model.log_perplexity(corpus)\n",
    "# n_topics_perplexity_dict[n_topic] = perplexity\n",
    "print(perplexity)\n",
    "print(n_topics_perplexity_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sagarsetru/anaconda3/envs/insight/lib/python3.8/site-packages/gensim/topic_coherence/direct_confirmation_measure.py:204: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  m_lr_i = np.log(numerator / denominator)\n",
      "/Users/sagarsetru/anaconda3/envs/insight/lib/python3.8/site-packages/gensim/topic_coherence/indirect_confirmation_measure.py:323: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cv1.T.dot(cv2)[0, 0] / (_magnitude(cv1) * _magnitude(cv2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  nan\n"
     ]
    }
   ],
   "source": [
    "# compute coherence \n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=articles_processed_ngrams_lemmaed, dictionary=id2word, coherence='c_v')\n",
    "\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lda_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-21a54c2dba32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlda_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lda_model' is not defined"
     ]
    }
   ],
   "source": [
    "lda_model.show_topics(num_topics=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of manually made text files of articles\n",
    "\n",
    "full_path = '/Users/sagarsetru/Documents/post PhD positions search/insightDataScience/project/debiaser/article_text_files/'\n",
    "\n",
    "full_file_names = [\n",
    "full_path+'ap_hurricane_sally_unleashes_20200916.txt',\n",
    "full_path+'cnn_big_ten_backtracks_20200916.txt',\n",
    "full_path+'nyt_on_the_fire_line_20200915.txt',\n",
    "full_path+'foxnews_snake_face_mask_20200916_v2.txt',\n",
    "'/Users/sagarsetru/Documents/post PhD positions search/insightDataScience/project/debiaser/article_html_files/U.S. Stocks Lower as Fed Outlook Rattles Investors - WSJ.html'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ap_hurricane_sally_unleashes_20200916.txt\n",
      ".txt\n",
      "cnn_big_ten_backtracks_20200916.txt\n",
      ".txt\n",
      "nyt_on_the_fire_line_20200915.txt\n",
      ".txt\n",
      "foxnews_snake_face_mask_20200916_v2.txt\n",
      ".txt\n",
      "U.S. Stocks Lower as Fed Outlook Rattles Investors - WSJ.html\n",
      ".html\n",
      "The Wall Street Journal\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Create a floor of the frequency of words to remove\n",
    "word_frequency_threshold = 1\n",
    "\n",
    "# choose the number of LDA topics\n",
    "num_lda_topics = 5\n",
    "\n",
    "# loop through files\n",
    "for ind, full_file_name in enumerate(full_file_names):\n",
    "    \n",
    "    # get extension (for testing copy and pasted html)\n",
    "    fname, ext = os.path.splitext(full_file_name)\n",
    "\n",
    "    # get path and file name\n",
    "    pathstr, fname = os.path.split(full_file_name)\n",
    "        \n",
    "    # print file name\n",
    "    print(fname)\n",
    "    print(ext)\n",
    "    \n",
    "    if ext == '.txt':\n",
    "        # get the article text as one string, remove new lines\n",
    "        with open(full_file_name, 'r') as file:\n",
    "            article_text = file.read().replace('\\n', ' ')\n",
    "            \n",
    "    elif ext == '.html':\n",
    "        \n",
    "        # get the html as one string\n",
    "        with open(full_file_name, 'r') as file:\n",
    "            coverpage = file.read()\n",
    "\n",
    "        # create soup object\n",
    "        soup = BeautifulSoup(coverpage, 'html.parser')\n",
    "\n",
    "        # get title\n",
    "        headline = soup.find('h1').get_text()\n",
    "        print(headline)\n",
    "        print(' ')\n",
    "\n",
    "        # get text from all <p> tags\n",
    "        p_tags = soup.find_all('p')\n",
    "\n",
    "        # get text from each p tag and strip whitespace\n",
    "        p_tags_text = [tag.get_text().strip() for tag in p_tags]\n",
    "        \n",
    "        # filter out sentences without periods\n",
    "        p_tags_text = [sentence for sentence in p_tags_text if '.' in sentence]\n",
    "\n",
    "        # convert all p_tags_text to single article text string\n",
    "\n",
    "        p_tags_text_1string = ''\n",
    "\n",
    "        for p_tag_text in p_tags_text:\n",
    "            p_tags_text_1string += p_tag_text\n",
    "\n",
    "        article_text = p_tags_text_1string\n",
    "    \n",
    "    \n",
    "    article_text_processed = process_all_articles([article_text])\n",
    "    \n",
    "    id2word_test = corpora.Dictionary(article_text_processed)\n",
    "    \n",
    "\n",
    "    \n",
    "#     lda_model.get_document_topics()\n",
    "    \n",
    "#     if ind == 0:\n",
    "#         break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id2word_test = corpora.Dictionary(articles_text_processed)\n",
    "\n",
    "# make frequency of words\n",
    "corpus_test = [id2word.doc2bow(article) for article in article_text_processed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = lda_model[corpus_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = lda_model.get_document_topics(corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.interfaces.TransformedCorpus at 0x14bde42b0>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.interfaces.TransformedCorpus object at 0x14bd993d0>\n"
     ]
    }
   ],
   "source": [
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.26132786),\n",
       " (2, 0.14556913),\n",
       " (5, 0.20745209),\n",
       " (6, 0.077264264),\n",
       " (11, 0.01413767),\n",
       " (15, 0.097798266),\n",
       " (16, 0.012090704),\n",
       " (18, 0.09593318),\n",
       " (19, 0.0761351)]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.086565785),\n",
       " (2, 0.20013271),\n",
       " (5, 0.11250849),\n",
       " (6, 0.06964701),\n",
       " (11, 0.038633205),\n",
       " (15, 0.076380216),\n",
       " (16, 0.010171399),\n",
       " (18, 0.12004942),\n",
       " (19, 0.27560484)]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.017*\"drop\" + 0.015*\"oil\" + 0.012*\"fund\" + 0.011*\"product\" + 0.010*\"elect\" + 0.010*\"russian\" + 0.010*\"gain\" + 0.010*\"exchange\" + 0.009*\"investor\" + 0.009*\"contract\"'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.015*\"candidate\" + 0.014*\"voter\" + 0.014*\"race\" + 0.012*\"conservative\" + 0.009*\"debate\" + 0.009*\"republican\" + 0.008*\"democratic\" + 0.008*\"poll\" + 0.008*\"speech\" + 0.008*\"promise\"'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topic(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.015*\"candidate\" + 0.014*\"voter\" + 0.014*\"race\" + 0.012*\"conservative\" + 0.009*\"debate\" + 0.009*\"republican\" + 0.008*\"democratic\" + 0.008*\"poll\" + 0.008*\"speech\" + 0.008*\"promise\"'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topic(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hurricane',\n",
       "  'sally',\n",
       "  'unleashes',\n",
       "  'flooding',\n",
       "  'along',\n",
       "  'the',\n",
       "  'gulf',\n",
       "  'coast',\n",
       "  'by',\n",
       "  'jay',\n",
       "  'reeves',\n",
       "  'angie',\n",
       "  'wang',\n",
       "  'and',\n",
       "  'jeff',\n",
       "  'martin',\n",
       "  'minutes',\n",
       "  'ago',\n",
       "  'pensacola',\n",
       "  'fla',\n",
       "  'ap',\n",
       "  'hurricane',\n",
       "  'sally',\n",
       "  'lumbered',\n",
       "  'ashore',\n",
       "  'near',\n",
       "  'the',\n",
       "  'florida',\n",
       "  'alabama',\n",
       "  'line',\n",
       "  'wednesday',\n",
       "  'with',\n",
       "  'mph',\n",
       "  'winds',\n",
       "  'and',\n",
       "  'rain',\n",
       "  'measured',\n",
       "  'in',\n",
       "  'feet',\n",
       "  'not',\n",
       "  'inches',\n",
       "  'swamping',\n",
       "  'homes',\n",
       "  'and',\n",
       "  'trapping',\n",
       "  'people',\n",
       "  'in',\n",
       "  'high',\n",
       "  'water',\n",
       "  'as',\n",
       "  'it',\n",
       "  'crept',\n",
       "  'inland',\n",
       "  'for',\n",
       "  'what',\n",
       "  'could',\n",
       "  'be',\n",
       "  'long',\n",
       "  'slow',\n",
       "  'and',\n",
       "  'disastrous',\n",
       "  'drenching',\n",
       "  'across',\n",
       "  'the',\n",
       "  'deep',\n",
       "  'south',\n",
       "  'moving',\n",
       "  'at',\n",
       "  'an',\n",
       "  'agonizing',\n",
       "  'mph',\n",
       "  'or',\n",
       "  'about',\n",
       "  'as',\n",
       "  'fast',\n",
       "  'as',\n",
       "  'person',\n",
       "  'can',\n",
       "  'walk',\n",
       "  'the',\n",
       "  'storm',\n",
       "  'made',\n",
       "  'landfall',\n",
       "  'at',\n",
       "  'close',\n",
       "  'to',\n",
       "  'gulf',\n",
       "  'shores',\n",
       "  'alabama',\n",
       "  'battering',\n",
       "  'the',\n",
       "  'metropolitan',\n",
       "  'areas',\n",
       "  'of',\n",
       "  'mobile',\n",
       "  'alabama',\n",
       "  'and',\n",
       "  'pensacola',\n",
       "  'florida',\n",
       "  'which',\n",
       "  'have',\n",
       "  'combined',\n",
       "  'population',\n",
       "  'of',\n",
       "  'almost',\n",
       "  'million',\n",
       "  'emergency',\n",
       "  'crews',\n",
       "  'plucked',\n",
       "  'people',\n",
       "  'from',\n",
       "  'flooded',\n",
       "  'homes',\n",
       "  'in',\n",
       "  'escambia',\n",
       "  'county',\n",
       "  'which',\n",
       "  'includes',\n",
       "  'pensacola',\n",
       "  'more',\n",
       "  'than',\n",
       "  'were',\n",
       "  'rescued',\n",
       "  'including',\n",
       "  'family',\n",
       "  'of',\n",
       "  'four',\n",
       "  'found',\n",
       "  'in',\n",
       "  'tree',\n",
       "  'sheriff',\n",
       "  'david',\n",
       "  'morgan',\n",
       "  'said',\n",
       "  'he',\n",
       "  'estimated',\n",
       "  'thousands',\n",
       "  'more',\n",
       "  'will',\n",
       "  'need',\n",
       "  'to',\n",
       "  'flee',\n",
       "  'rising',\n",
       "  'waters',\n",
       "  'in',\n",
       "  'the',\n",
       "  'coming',\n",
       "  'days',\n",
       "  'county',\n",
       "  'officials',\n",
       "  'urged',\n",
       "  'residents',\n",
       "  'to',\n",
       "  'stick',\n",
       "  'to',\n",
       "  'text',\n",
       "  'messages',\n",
       "  'for',\n",
       "  'contacting',\n",
       "  'family',\n",
       "  'and',\n",
       "  'friends',\n",
       "  'to',\n",
       "  'keep',\n",
       "  'cellphone',\n",
       "  'service',\n",
       "  'open',\n",
       "  'for',\n",
       "  'calls',\n",
       "  'there',\n",
       "  'are',\n",
       "  'entire',\n",
       "  'communities',\n",
       "  'that',\n",
       "  'we',\n",
       "  're',\n",
       "  'going',\n",
       "  'to',\n",
       "  'have',\n",
       "  'to',\n",
       "  'evacuate',\n",
       "  'morgan',\n",
       "  'said',\n",
       "  'it',\n",
       "  'going',\n",
       "  'to',\n",
       "  'be',\n",
       "  'tremendous',\n",
       "  'operation',\n",
       "  'over',\n",
       "  'the',\n",
       "  'next',\n",
       "  'several',\n",
       "  'days',\n",
       "  'the',\n",
       "  'storm',\n",
       "  'collapsed',\n",
       "  'section',\n",
       "  'of',\n",
       "  'the',\n",
       "  'three',\n",
       "  'mile',\n",
       "  'bridge',\n",
       "  'across',\n",
       "  'pensacola',\n",
       "  'bay',\n",
       "  'the',\n",
       "  'sheriff',\n",
       "  'said',\n",
       "  'and',\n",
       "  'crews',\n",
       "  'struggled',\n",
       "  'to',\n",
       "  'stop',\n",
       "  'barge',\n",
       "  'that',\n",
       "  'had',\n",
       "  'broken',\n",
       "  'loose',\n",
       "  'from',\n",
       "  'drifting',\n",
       "  'into',\n",
       "  'nearby',\n",
       "  'bridge',\n",
       "  'that',\n",
       "  'is',\n",
       "  'part',\n",
       "  'of',\n",
       "  'interstate',\n",
       "  'officials',\n",
       "  'closed',\n",
       "  'which',\n",
       "  'runs',\n",
       "  'parallel',\n",
       "  'to',\n",
       "  'the',\n",
       "  'gulf',\n",
       "  'coast',\n",
       "  'in',\n",
       "  'hard',\n",
       "  'hit',\n",
       "  'areas',\n",
       "  'of',\n",
       "  'both',\n",
       "  'florida',\n",
       "  'and',\n",
       "  'alabama',\n",
       "  'more',\n",
       "  'than',\n",
       "  'feet',\n",
       "  'of',\n",
       "  'rain',\n",
       "  'centimeters',\n",
       "  'was',\n",
       "  'recorded',\n",
       "  'near',\n",
       "  'naval',\n",
       "  'air',\n",
       "  'station',\n",
       "  'pensacola',\n",
       "  'and',\n",
       "  'nearly',\n",
       "  'feet',\n",
       "  'meter',\n",
       "  'of',\n",
       "  'water',\n",
       "  'covered',\n",
       "  'streets',\n",
       "  'in',\n",
       "  'downtown',\n",
       "  'pensacola',\n",
       "  'the',\n",
       "  'national',\n",
       "  'weather',\n",
       "  'service',\n",
       "  'reported',\n",
       "  'more',\n",
       "  'stories',\n",
       "  'the',\n",
       "  'latest',\n",
       "  'sally',\n",
       "  'knocks',\n",
       "  'out',\n",
       "  'part',\n",
       "  'of',\n",
       "  'bridge',\n",
       "  'in',\n",
       "  'pensacola',\n",
       "  'huge',\n",
       "  'rainmaker',\n",
       "  'hurricane',\n",
       "  'sally',\n",
       "  'threatens',\n",
       "  'historic',\n",
       "  'floods',\n",
       "  'damage',\n",
       "  'reported',\n",
       "  'as',\n",
       "  'paulette',\n",
       "  'makes',\n",
       "  'rare',\n",
       "  'landfall',\n",
       "  'in',\n",
       "  'bermuda',\n",
       "  'it',\n",
       "  'not',\n",
       "  'common',\n",
       "  'that',\n",
       "  'you',\n",
       "  'start',\n",
       "  'measuring',\n",
       "  'rainfall',\n",
       "  'in',\n",
       "  'feet',\n",
       "  'said',\n",
       "  'forecaster',\n",
       "  'david',\n",
       "  'eversole',\n",
       "  'in',\n",
       "  'mobile',\n",
       "  'sally',\n",
       "  'moving',\n",
       "  'so',\n",
       "  'slowly',\n",
       "  'so',\n",
       "  'it',\n",
       "  'just',\n",
       "  'keeps',\n",
       "  'pounding',\n",
       "  'and',\n",
       "  'pounding',\n",
       "  'and',\n",
       "  'pounding',\n",
       "  'the',\n",
       "  'area',\n",
       "  'with',\n",
       "  'tropical',\n",
       "  'rain',\n",
       "  'and',\n",
       "  'just',\n",
       "  'powerful',\n",
       "  'winds',\n",
       "  'it',\n",
       "  'just',\n",
       "  'nightmare',\n",
       "  'the',\n",
       "  'storm',\n",
       "  'knocked',\n",
       "  'out',\n",
       "  'power',\n",
       "  'to',\n",
       "  'about',\n",
       "  'half',\n",
       "  'million',\n",
       "  'homes',\n",
       "  'and',\n",
       "  'businesses',\n",
       "  'it',\n",
       "  'was',\n",
       "  'the',\n",
       "  'second',\n",
       "  'hurricane',\n",
       "  'to',\n",
       "  'hit',\n",
       "  'the',\n",
       "  'gulf',\n",
       "  'coast',\n",
       "  'in',\n",
       "  'less',\n",
       "  'than',\n",
       "  'three',\n",
       "  'weeks',\n",
       "  'and',\n",
       "  'the',\n",
       "  'latest',\n",
       "  'blow',\n",
       "  'in',\n",
       "  'one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'busiest',\n",
       "  'hurricane',\n",
       "  'seasons',\n",
       "  'ever',\n",
       "  'recorded',\n",
       "  'so',\n",
       "  'frenetic',\n",
       "  'that',\n",
       "  'forecasters',\n",
       "  'have',\n",
       "  'nearly',\n",
       "  'run',\n",
       "  'through',\n",
       "  'the',\n",
       "  'alphabet',\n",
       "  'of',\n",
       "  'storm',\n",
       "  'names',\n",
       "  'with',\n",
       "  'months',\n",
       "  'still',\n",
       "  'to',\n",
       "  'go',\n",
       "  'at',\n",
       "  'the',\n",
       "  'start',\n",
       "  'of',\n",
       "  'the',\n",
       "  'week',\n",
       "  'sally',\n",
       "  'was',\n",
       "  'one',\n",
       "  'of',\n",
       "  'record',\n",
       "  'tying',\n",
       "  'five',\n",
       "  'storms',\n",
       "  'churning',\n",
       "  'simultaneously',\n",
       "  'in',\n",
       "  'the',\n",
       "  'atlantic',\n",
       "  'strung',\n",
       "  'out',\n",
       "  'like',\n",
       "  'charms',\n",
       "  'on',\n",
       "  'bracelet',\n",
       "  'like',\n",
       "  'the',\n",
       "  'wildfires',\n",
       "  'raging',\n",
       "  'on',\n",
       "  'the',\n",
       "  'west',\n",
       "  'coast',\n",
       "  'the',\n",
       "  'onslaught',\n",
       "  'of',\n",
       "  'hurricanes',\n",
       "  'has',\n",
       "  'focused',\n",
       "  'attention',\n",
       "  'on',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'which',\n",
       "  'scientists',\n",
       "  'say',\n",
       "  'is',\n",
       "  'causing',\n",
       "  'slower',\n",
       "  'rainier',\n",
       "  'more',\n",
       "  'powerful',\n",
       "  'and',\n",
       "  'more',\n",
       "  'destructive',\n",
       "  'storms',\n",
       "  'an',\n",
       "  'emergency',\n",
       "  'crew',\n",
       "  'rescued',\n",
       "  'two',\n",
       "  'people',\n",
       "  'on',\n",
       "  'dauphin',\n",
       "  'island',\n",
       "  'alabama',\n",
       "  'after',\n",
       "  'the',\n",
       "  'hurricane',\n",
       "  'ripped',\n",
       "  'the',\n",
       "  'roof',\n",
       "  'off',\n",
       "  'their',\n",
       "  'home',\n",
       "  'and',\n",
       "  'the',\n",
       "  'rest',\n",
       "  'of',\n",
       "  'the',\n",
       "  'house',\n",
       "  'began',\n",
       "  'to',\n",
       "  'crumble',\n",
       "  'as',\n",
       "  'things',\n",
       "  'started',\n",
       "  'to',\n",
       "  'peel',\n",
       "  'off',\n",
       "  'and',\n",
       "  'fall',\n",
       "  'apart',\n",
       "  'they',\n",
       "  'got',\n",
       "  'scared',\n",
       "  'and',\n",
       "  'called',\n",
       "  'for',\n",
       "  'assistance',\n",
       "  'dauphin',\n",
       "  'island',\n",
       "  'mayor',\n",
       "  'jeff',\n",
       "  'collier',\n",
       "  'said',\n",
       "  'by',\n",
       "  'phone',\n",
       "  'he',\n",
       "  'said',\n",
       "  'no',\n",
       "  'one',\n",
       "  'was',\n",
       "  'injured',\n",
       "  'in',\n",
       "  'orange',\n",
       "  'beach',\n",
       "  'alabama',\n",
       "  'winds',\n",
       "  'blew',\n",
       "  'out',\n",
       "  'the',\n",
       "  'walls',\n",
       "  'in',\n",
       "  'one',\n",
       "  'corner',\n",
       "  'of',\n",
       "  'condominium',\n",
       "  'building',\n",
       "  'exposing',\n",
       "  'the',\n",
       "  'interiors',\n",
       "  'of',\n",
       "  'condos',\n",
       "  'on',\n",
       "  'at',\n",
       "  'least',\n",
       "  'five',\n",
       "  'floors',\n",
       "  'video',\n",
       "  'posted',\n",
       "  'online',\n",
       "  'showed',\n",
       "  'other',\n",
       "  'images',\n",
       "  'showed',\n",
       "  'boats',\n",
       "  'shoved',\n",
       "  'onshore',\n",
       "  'by',\n",
       "  'storm',\n",
       "  'surge',\n",
       "  'at',\n",
       "  'least',\n",
       "  'people',\n",
       "  'in',\n",
       "  'orange',\n",
       "  'beach',\n",
       "  'were',\n",
       "  'rescued',\n",
       "  'from',\n",
       "  'flooded',\n",
       "  'homes',\n",
       "  'and',\n",
       "  'taken',\n",
       "  'to',\n",
       "  'shelters',\n",
       "  'mayor',\n",
       "  'tony',\n",
       "  'kennon',\n",
       "  'said',\n",
       "  'we',\n",
       "  'got',\n",
       "  'few',\n",
       "  'people',\n",
       "  'that',\n",
       "  'we',\n",
       "  'just',\n",
       "  'haven',\n",
       "  'been',\n",
       "  'able',\n",
       "  'to',\n",
       "  'get',\n",
       "  'to',\n",
       "  'because',\n",
       "  'the',\n",
       "  'water',\n",
       "  'is',\n",
       "  'so',\n",
       "  'high',\n",
       "  'kennon',\n",
       "  'said',\n",
       "  'but',\n",
       "  'they',\n",
       "  'are',\n",
       "  'safe',\n",
       "  'in',\n",
       "  'their',\n",
       "  'home',\n",
       "  'as',\n",
       "  'soon',\n",
       "  'as',\n",
       "  'the',\n",
       "  'water',\n",
       "  'recedes',\n",
       "  'we',\n",
       "  'will',\n",
       "  'rescue',\n",
       "  'them',\n",
       "  'street',\n",
       "  'lights',\n",
       "  'were',\n",
       "  'knocked',\n",
       "  'out',\n",
       "  'in',\n",
       "  'downtown',\n",
       "  'mobile',\n",
       "  'trees',\n",
       "  'were',\n",
       "  'bent',\n",
       "  'over',\n",
       "  'as',\n",
       "  'the',\n",
       "  'rain',\n",
       "  'blew',\n",
       "  'sideways',\n",
       "  'in',\n",
       "  'the',\n",
       "  'howling',\n",
       "  'wind',\n",
       "  'in',\n",
       "  'downtown',\n",
       "  'pensacola',\n",
       "  'water',\n",
       "  'rushed',\n",
       "  'down',\n",
       "  'some',\n",
       "  'streets',\n",
       "  'like',\n",
       "  'river',\n",
       "  'rapids',\n",
       "  'forming',\n",
       "  'whitecaps',\n",
       "  'as',\n",
       "  'it',\n",
       "  'slapped',\n",
       "  'against',\n",
       "  'buildings',\n",
       "  'and',\n",
       "  'rose',\n",
       "  'above',\n",
       "  'the',\n",
       "  'tires',\n",
       "  'on',\n",
       "  'cars',\n",
       "  'before',\n",
       "  'sunrise',\n",
       "  'water',\n",
       "  'was',\n",
       "  'up',\n",
       "  'to',\n",
       "  'the',\n",
       "  'doors',\n",
       "  'of',\n",
       "  'jordan',\n",
       "  'muse',\n",
       "  'car',\n",
       "  'outside',\n",
       "  'the',\n",
       "  'pensacola',\n",
       "  'hotel',\n",
       "  'where',\n",
       "  'her',\n",
       "  'family',\n",
       "  'took',\n",
       "  'shelter',\n",
       "  'after',\n",
       "  'fleeing',\n",
       "  'their',\n",
       "  'mobile',\n",
       "  'home',\n",
       "  'the',\n",
       "  'power',\n",
       "  'failed',\n",
       "  'early',\n",
       "  'in',\n",
       "  'the',\n",
       "  'morning',\n",
       "  'making',\n",
       "  'it',\n",
       "  'too',\n",
       "  'stuffy',\n",
       "  'to',\n",
       "  'sleep',\n",
       "  'her',\n",
       "  'year',\n",
       "  'old',\n",
       "  'son',\n",
       "  'played',\n",
       "  'with',\n",
       "  'toys',\n",
       "  'underneath',\n",
       "  'the',\n",
       "  'hotel',\n",
       "  'room',\n",
       "  'desk',\n",
       "  'as',\n",
       "  'muse',\n",
       "  'peered',\n",
       "  'out',\n",
       "  'the',\n",
       "  'window',\n",
       "  'watching',\n",
       "  'rain',\n",
       "  'fly',\n",
       "  'by',\n",
       "  'in',\n",
       "  'sheets',\n",
       "  'the',\n",
       "  'power',\n",
       "  'trucks',\n",
       "  'are',\n",
       "  'the',\n",
       "  'only',\n",
       "  'ones',\n",
       "  'above',\n",
       "  'water',\n",
       "  'and',\n",
       "  'they',\n",
       "  're',\n",
       "  'the',\n",
       "  'biggest',\n",
       "  'muse',\n",
       "  'said',\n",
       "  'can',\n",
       "  'believe',\n",
       "  'it',\n",
       "  'got',\n",
       "  'so',\n",
       "  'bad',\n",
       "  'that',\n",
       "  'why',\n",
       "  'we',\n",
       "  'came',\n",
       "  'here',\n",
       "  'michele',\n",
       "  'lamar',\n",
       "  'acuff',\n",
       "  'woke',\n",
       "  'to',\n",
       "  'the',\n",
       "  'thud',\n",
       "  'of',\n",
       "  'small',\n",
       "  'tree',\n",
       "  'falling',\n",
       "  'against',\n",
       "  'window',\n",
       "  'of',\n",
       "  'her',\n",
       "  'pensacola',\n",
       "  'home',\n",
       "  'waist',\n",
       "  'deep',\n",
       "  'water',\n",
       "  'gushed',\n",
       "  'down',\n",
       "  'her',\n",
       "  'street',\n",
       "  'above',\n",
       "  'the',\n",
       "  'loud',\n",
       "  'whistling',\n",
       "  'of',\n",
       "  'the',\n",
       "  'wind',\n",
       "  'she',\n",
       "  'heard',\n",
       "  'what',\n",
       "  'sounded',\n",
       "  'like',\n",
       "  'transformers',\n",
       "  'exploding',\n",
       "  'full',\n",
       "  'coverage',\n",
       "  'hurricanes',\n",
       "  'don',\n",
       "  'feel',\n",
       "  'safe',\n",
       "  'to',\n",
       "  'leave',\n",
       "  'lamar',\n",
       "  'acuff',\n",
       "  'said',\n",
       "  'from',\n",
       "  'the',\n",
       "  'porch',\n",
       "  'of',\n",
       "  'neighbor',\n",
       "  'house',\n",
       "  'just',\n",
       "  'staying',\n",
       "  'put',\n",
       "  'and',\n",
       "  'hoping',\n",
       "  'for',\n",
       "  'the',\n",
       "  'best',\n",
       "  'sally',\n",
       "  'blew',\n",
       "  'ashore',\n",
       "  'as',\n",
       "  'category',\n",
       "  'storm',\n",
       "  'but',\n",
       "  'weakened',\n",
       "  'to',\n",
       "  'still',\n",
       "  'dangerous',\n",
       "  'category',\n",
       "  'with',\n",
       "  'winds',\n",
       "  'of',\n",
       "  'mph',\n",
       "  'by',\n",
       "  'mid',\n",
       "  'morning',\n",
       "  'it',\n",
       "  'was',\n",
       "  'moving',\n",
       "  'to',\n",
       "  'the',\n",
       "  'northeast',\n",
       "  'at',\n",
       "  'mph',\n",
       "  'kph',\n",
       "  'forecasters',\n",
       "  'warned',\n",
       "  'that',\n",
       "  'heavy',\n",
       "  'rain',\n",
       "  'will',\n",
       "  'continue',\n",
       "  'into',\n",
       "  'thursday',\n",
       "  'as',\n",
       "  'the',\n",
       "  'storm',\n",
       "  'moves',\n",
       "  'inland',\n",
       "  'over',\n",
       "  'alabama',\n",
       "  'and',\n",
       "  'into',\n",
       "  'central',\n",
       "  'georgia',\n",
       "  'national',\n",
       "  'hurricane',\n",
       "  'center',\n",
       "  'forecaster',\n",
       "  'stacy',\n",
       "  'stewart',\n",
       "  'said',\n",
       "  'the',\n",
       "  'rain',\n",
       "  'will',\n",
       "  'be',\n",
       "  'catastrophic',\n",
       "  'and',\n",
       "  'life',\n",
       "  'threatening',\n",
       "  'over',\n",
       "  'portions',\n",
       "  'of',\n",
       "  'the',\n",
       "  'gulf',\n",
       "  'coast',\n",
       "  'forecasters',\n",
       "  'predicted',\n",
       "  'to',\n",
       "  'inches',\n",
       "  'centimeters',\n",
       "  'of',\n",
       "  'rain',\n",
       "  'with',\n",
       "  'up',\n",
       "  'to',\n",
       "  'inches',\n",
       "  'centimeters',\n",
       "  'in',\n",
       "  'some',\n",
       "  'spots',\n",
       "  'sally',\n",
       "  'has',\n",
       "  'characteristic',\n",
       "  'that',\n",
       "  'isn',\n",
       "  'often',\n",
       "  'seen',\n",
       "  'and',\n",
       "  'that',\n",
       "  'slow',\n",
       "  'forward',\n",
       "  'speed',\n",
       "  'and',\n",
       "  'that',\n",
       "  'going',\n",
       "  'to',\n",
       "  'exacerbate',\n",
       "  'the',\n",
       "  'flooding',\n",
       "  'said',\n",
       "  'ed',\n",
       "  'rappaport',\n",
       "  'deputy',\n",
       "  'director',\n",
       "  'of',\n",
       "  'the',\n",
       "  'hurricane',\n",
       "  'center',\n",
       "  'he',\n",
       "  'likened',\n",
       "  'the',\n",
       "  'storm',\n",
       "  'plodding',\n",
       "  'pace',\n",
       "  'to',\n",
       "  'that',\n",
       "  'of',\n",
       "  'hurricane',\n",
       "  'harvey',\n",
       "  'which',\n",
       "  'inundated',\n",
       "  'houston',\n",
       "  'in',\n",
       "  'sally',\n",
       "  'effects',\n",
       "  'were',\n",
       "  'felt',\n",
       "  'all',\n",
       "  'along',\n",
       "  'the',\n",
       "  'northern',\n",
       "  'gulf',\n",
       "  'coast',\n",
       "  'low',\n",
       "  'lying',\n",
       "  'properties',\n",
       "  'in',\n",
       "  'southeastern',\n",
       "  'louisiana',\n",
       "  'were',\n",
       "  'swamped',\n",
       "  'by',\n",
       "  'the',\n",
       "  'surge',\n",
       "  'water',\n",
       "  'covered',\n",
       "  'mississippi',\n",
       "  'beaches',\n",
       "  'and',\n",
       "  'parts',\n",
       "  'of',\n",
       "  'the',\n",
       "  'highway',\n",
       "  'that',\n",
       "  'runs',\n",
       "  'parallel',\n",
       "  'to',\n",
       "  'them',\n",
       "  'president',\n",
       "  'donald',\n",
       "  'trump',\n",
       "  'issued',\n",
       "  'emergency',\n",
       "  'declarations',\n",
       "  'for',\n",
       "  'parts',\n",
       "  'of',\n",
       "  'florida',\n",
       "  'alabama',\n",
       "  'mississippi',\n",
       "  'and',\n",
       "  'louisiana',\n",
       "  'white',\n",
       "  'house',\n",
       "  'press',\n",
       "  'secretary',\n",
       "  'kayleigh',\n",
       "  'mcenany',\n",
       "  'said',\n",
       "  'on',\n",
       "  'fox',\n",
       "  'news',\n",
       "  'channel',\n",
       "  'that',\n",
       "  'trump',\n",
       "  ...]]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_text_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hurricane',\n",
       "  'sally',\n",
       "  'unleashes',\n",
       "  'flooding',\n",
       "  'along',\n",
       "  'the',\n",
       "  'gulf',\n",
       "  'coast',\n",
       "  'by',\n",
       "  'jay',\n",
       "  'reeves',\n",
       "  'angie',\n",
       "  'wang',\n",
       "  'and',\n",
       "  'jeff',\n",
       "  'martin',\n",
       "  'minutes',\n",
       "  'ago',\n",
       "  'pensacola',\n",
       "  'fla',\n",
       "  'ap',\n",
       "  'hurricane',\n",
       "  'sally',\n",
       "  'lumbered',\n",
       "  'ashore',\n",
       "  'near',\n",
       "  'the',\n",
       "  'florida',\n",
       "  'alabama',\n",
       "  'line',\n",
       "  'wednesday',\n",
       "  'with',\n",
       "  'mph',\n",
       "  'winds',\n",
       "  'and',\n",
       "  'rain',\n",
       "  'measured',\n",
       "  'in',\n",
       "  'feet',\n",
       "  'not',\n",
       "  'inches',\n",
       "  'swamping',\n",
       "  'homes',\n",
       "  'and',\n",
       "  'trapping',\n",
       "  'people',\n",
       "  'in',\n",
       "  'high',\n",
       "  'water',\n",
       "  'as',\n",
       "  'it',\n",
       "  'crept',\n",
       "  'inland',\n",
       "  'for',\n",
       "  'what',\n",
       "  'could',\n",
       "  'be',\n",
       "  'long',\n",
       "  'slow',\n",
       "  'and',\n",
       "  'disastrous',\n",
       "  'drenching',\n",
       "  'across',\n",
       "  'the',\n",
       "  'deep',\n",
       "  'south',\n",
       "  'moving',\n",
       "  'at',\n",
       "  'an',\n",
       "  'agonizing',\n",
       "  'mph',\n",
       "  'or',\n",
       "  'about',\n",
       "  'as',\n",
       "  'fast',\n",
       "  'as',\n",
       "  'person',\n",
       "  'can',\n",
       "  'walk',\n",
       "  'the',\n",
       "  'storm',\n",
       "  'made',\n",
       "  'landfall',\n",
       "  'at',\n",
       "  'close',\n",
       "  'to',\n",
       "  'gulf',\n",
       "  'shores',\n",
       "  'alabama',\n",
       "  'battering',\n",
       "  'the',\n",
       "  'metropolitan',\n",
       "  'areas',\n",
       "  'of',\n",
       "  'mobile',\n",
       "  'alabama',\n",
       "  'and',\n",
       "  'pensacola',\n",
       "  'florida',\n",
       "  'which',\n",
       "  'have',\n",
       "  'combined',\n",
       "  'population',\n",
       "  'of',\n",
       "  'almost',\n",
       "  'million',\n",
       "  'emergency',\n",
       "  'crews',\n",
       "  'plucked',\n",
       "  'people',\n",
       "  'from',\n",
       "  'flooded',\n",
       "  'homes',\n",
       "  'in',\n",
       "  'escambia',\n",
       "  'county',\n",
       "  'which',\n",
       "  'includes',\n",
       "  'pensacola',\n",
       "  'more',\n",
       "  'than',\n",
       "  'were',\n",
       "  'rescued',\n",
       "  'including',\n",
       "  'family',\n",
       "  'of',\n",
       "  'four',\n",
       "  'found',\n",
       "  'in',\n",
       "  'tree',\n",
       "  'sheriff',\n",
       "  'david',\n",
       "  'morgan',\n",
       "  'said',\n",
       "  'he',\n",
       "  'estimated',\n",
       "  'thousands',\n",
       "  'more',\n",
       "  'will',\n",
       "  'need',\n",
       "  'to',\n",
       "  'flee',\n",
       "  'rising',\n",
       "  'waters',\n",
       "  'in',\n",
       "  'the',\n",
       "  'coming',\n",
       "  'days',\n",
       "  'county',\n",
       "  'officials',\n",
       "  'urged',\n",
       "  'residents',\n",
       "  'to',\n",
       "  'stick',\n",
       "  'to',\n",
       "  'text',\n",
       "  'messages',\n",
       "  'for',\n",
       "  'contacting',\n",
       "  'family',\n",
       "  'and',\n",
       "  'friends',\n",
       "  'to',\n",
       "  'keep',\n",
       "  'cellphone',\n",
       "  'service',\n",
       "  'open',\n",
       "  'for',\n",
       "  'calls',\n",
       "  'there',\n",
       "  'are',\n",
       "  'entire',\n",
       "  'communities',\n",
       "  'that',\n",
       "  'we',\n",
       "  're',\n",
       "  'going',\n",
       "  'to',\n",
       "  'have',\n",
       "  'to',\n",
       "  'evacuate',\n",
       "  'morgan',\n",
       "  'said',\n",
       "  'it',\n",
       "  'going',\n",
       "  'to',\n",
       "  'be',\n",
       "  'tremendous',\n",
       "  'operation',\n",
       "  'over',\n",
       "  'the',\n",
       "  'next',\n",
       "  'several',\n",
       "  'days',\n",
       "  'the',\n",
       "  'storm',\n",
       "  'collapsed',\n",
       "  'section',\n",
       "  'of',\n",
       "  'the',\n",
       "  'three',\n",
       "  'mile',\n",
       "  'bridge',\n",
       "  'across',\n",
       "  'pensacola',\n",
       "  'bay',\n",
       "  'the',\n",
       "  'sheriff',\n",
       "  'said',\n",
       "  'and',\n",
       "  'crews',\n",
       "  'struggled',\n",
       "  'to',\n",
       "  'stop',\n",
       "  'barge',\n",
       "  'that',\n",
       "  'had',\n",
       "  'broken',\n",
       "  'loose',\n",
       "  'from',\n",
       "  'drifting',\n",
       "  'into',\n",
       "  'nearby',\n",
       "  'bridge',\n",
       "  'that',\n",
       "  'is',\n",
       "  'part',\n",
       "  'of',\n",
       "  'interstate',\n",
       "  'officials',\n",
       "  'closed',\n",
       "  'which',\n",
       "  'runs',\n",
       "  'parallel',\n",
       "  'to',\n",
       "  'the',\n",
       "  'gulf',\n",
       "  'coast',\n",
       "  'in',\n",
       "  'hard',\n",
       "  'hit',\n",
       "  'areas',\n",
       "  'of',\n",
       "  'both',\n",
       "  'florida',\n",
       "  'and',\n",
       "  'alabama',\n",
       "  'more',\n",
       "  'than',\n",
       "  'feet',\n",
       "  'of',\n",
       "  'rain',\n",
       "  'centimeters',\n",
       "  'was',\n",
       "  'recorded',\n",
       "  'near',\n",
       "  'naval',\n",
       "  'air',\n",
       "  'station',\n",
       "  'pensacola',\n",
       "  'and',\n",
       "  'nearly',\n",
       "  'feet',\n",
       "  'meter',\n",
       "  'of',\n",
       "  'water',\n",
       "  'covered',\n",
       "  'streets',\n",
       "  'in',\n",
       "  'downtown',\n",
       "  'pensacola',\n",
       "  'the',\n",
       "  'national',\n",
       "  'weather',\n",
       "  'service',\n",
       "  'reported',\n",
       "  'more',\n",
       "  'stories',\n",
       "  'the',\n",
       "  'latest',\n",
       "  'sally',\n",
       "  'knocks',\n",
       "  'out',\n",
       "  'part',\n",
       "  'of',\n",
       "  'bridge',\n",
       "  'in',\n",
       "  'pensacola',\n",
       "  'huge',\n",
       "  'rainmaker',\n",
       "  'hurricane',\n",
       "  'sally',\n",
       "  'threatens',\n",
       "  'historic',\n",
       "  'floods',\n",
       "  'damage',\n",
       "  'reported',\n",
       "  'as',\n",
       "  'paulette',\n",
       "  'makes',\n",
       "  'rare',\n",
       "  'landfall',\n",
       "  'in',\n",
       "  'bermuda',\n",
       "  'it',\n",
       "  'not',\n",
       "  'common',\n",
       "  'that',\n",
       "  'you',\n",
       "  'start',\n",
       "  'measuring',\n",
       "  'rainfall',\n",
       "  'in',\n",
       "  'feet',\n",
       "  'said',\n",
       "  'forecaster',\n",
       "  'david',\n",
       "  'eversole',\n",
       "  'in',\n",
       "  'mobile',\n",
       "  'sally',\n",
       "  'moving',\n",
       "  'so',\n",
       "  'slowly',\n",
       "  'so',\n",
       "  'it',\n",
       "  'just',\n",
       "  'keeps',\n",
       "  'pounding',\n",
       "  'and',\n",
       "  'pounding',\n",
       "  'and',\n",
       "  'pounding',\n",
       "  'the',\n",
       "  'area',\n",
       "  'with',\n",
       "  'tropical',\n",
       "  'rain',\n",
       "  'and',\n",
       "  'just',\n",
       "  'powerful',\n",
       "  'winds',\n",
       "  'it',\n",
       "  'just',\n",
       "  'nightmare',\n",
       "  'the',\n",
       "  'storm',\n",
       "  'knocked',\n",
       "  'out',\n",
       "  'power',\n",
       "  'to',\n",
       "  'about',\n",
       "  'half',\n",
       "  'million',\n",
       "  'homes',\n",
       "  'and',\n",
       "  'businesses',\n",
       "  'it',\n",
       "  'was',\n",
       "  'the',\n",
       "  'second',\n",
       "  'hurricane',\n",
       "  'to',\n",
       "  'hit',\n",
       "  'the',\n",
       "  'gulf',\n",
       "  'coast',\n",
       "  'in',\n",
       "  'less',\n",
       "  'than',\n",
       "  'three',\n",
       "  'weeks',\n",
       "  'and',\n",
       "  'the',\n",
       "  'latest',\n",
       "  'blow',\n",
       "  'in',\n",
       "  'one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'busiest',\n",
       "  'hurricane',\n",
       "  'seasons',\n",
       "  'ever',\n",
       "  'recorded',\n",
       "  'so',\n",
       "  'frenetic',\n",
       "  'that',\n",
       "  'forecasters',\n",
       "  'have',\n",
       "  'nearly',\n",
       "  'run',\n",
       "  'through',\n",
       "  'the',\n",
       "  'alphabet',\n",
       "  'of',\n",
       "  'storm',\n",
       "  'names',\n",
       "  'with',\n",
       "  'months',\n",
       "  'still',\n",
       "  'to',\n",
       "  'go',\n",
       "  'at',\n",
       "  'the',\n",
       "  'start',\n",
       "  'of',\n",
       "  'the',\n",
       "  'week',\n",
       "  'sally',\n",
       "  'was',\n",
       "  'one',\n",
       "  'of',\n",
       "  'record',\n",
       "  'tying',\n",
       "  'five',\n",
       "  'storms',\n",
       "  'churning',\n",
       "  'simultaneously',\n",
       "  'in',\n",
       "  'the',\n",
       "  'atlantic',\n",
       "  'strung',\n",
       "  'out',\n",
       "  'like',\n",
       "  'charms',\n",
       "  'on',\n",
       "  'bracelet',\n",
       "  'like',\n",
       "  'the',\n",
       "  'wildfires',\n",
       "  'raging',\n",
       "  'on',\n",
       "  'the',\n",
       "  'west',\n",
       "  'coast',\n",
       "  'the',\n",
       "  'onslaught',\n",
       "  'of',\n",
       "  'hurricanes',\n",
       "  'has',\n",
       "  'focused',\n",
       "  'attention',\n",
       "  'on',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'which',\n",
       "  'scientists',\n",
       "  'say',\n",
       "  'is',\n",
       "  'causing',\n",
       "  'slower',\n",
       "  'rainier',\n",
       "  'more',\n",
       "  'powerful',\n",
       "  'and',\n",
       "  'more',\n",
       "  'destructive',\n",
       "  'storms',\n",
       "  'an',\n",
       "  'emergency',\n",
       "  'crew',\n",
       "  'rescued',\n",
       "  'two',\n",
       "  'people',\n",
       "  'on',\n",
       "  'dauphin',\n",
       "  'island',\n",
       "  'alabama',\n",
       "  'after',\n",
       "  'the',\n",
       "  'hurricane',\n",
       "  'ripped',\n",
       "  'the',\n",
       "  'roof',\n",
       "  'off',\n",
       "  'their',\n",
       "  'home',\n",
       "  'and',\n",
       "  'the',\n",
       "  'rest',\n",
       "  'of',\n",
       "  'the',\n",
       "  'house',\n",
       "  'began',\n",
       "  'to',\n",
       "  'crumble',\n",
       "  'as',\n",
       "  'things',\n",
       "  'started',\n",
       "  'to',\n",
       "  'peel',\n",
       "  'off',\n",
       "  'and',\n",
       "  'fall',\n",
       "  'apart',\n",
       "  'they',\n",
       "  'got',\n",
       "  'scared',\n",
       "  'and',\n",
       "  'called',\n",
       "  'for',\n",
       "  'assistance',\n",
       "  'dauphin',\n",
       "  'island',\n",
       "  'mayor',\n",
       "  'jeff',\n",
       "  'collier',\n",
       "  'said',\n",
       "  'by',\n",
       "  'phone',\n",
       "  'he',\n",
       "  'said',\n",
       "  'no',\n",
       "  'one',\n",
       "  'was',\n",
       "  'injured',\n",
       "  'in',\n",
       "  'orange',\n",
       "  'beach',\n",
       "  'alabama',\n",
       "  'winds',\n",
       "  'blew',\n",
       "  'out',\n",
       "  'the',\n",
       "  'walls',\n",
       "  'in',\n",
       "  'one',\n",
       "  'corner',\n",
       "  'of',\n",
       "  'condominium',\n",
       "  'building',\n",
       "  'exposing',\n",
       "  'the',\n",
       "  'interiors',\n",
       "  'of',\n",
       "  'condos',\n",
       "  'on',\n",
       "  'at',\n",
       "  'least',\n",
       "  'five',\n",
       "  'floors',\n",
       "  'video',\n",
       "  'posted',\n",
       "  'online',\n",
       "  'showed',\n",
       "  'other',\n",
       "  'images',\n",
       "  'showed',\n",
       "  'boats',\n",
       "  'shoved',\n",
       "  'onshore',\n",
       "  'by',\n",
       "  'storm',\n",
       "  'surge',\n",
       "  'at',\n",
       "  'least',\n",
       "  'people',\n",
       "  'in',\n",
       "  'orange',\n",
       "  'beach',\n",
       "  'were',\n",
       "  'rescued',\n",
       "  'from',\n",
       "  'flooded',\n",
       "  'homes',\n",
       "  'and',\n",
       "  'taken',\n",
       "  'to',\n",
       "  'shelters',\n",
       "  'mayor',\n",
       "  'tony',\n",
       "  'kennon',\n",
       "  'said',\n",
       "  'we',\n",
       "  'got',\n",
       "  'few',\n",
       "  'people',\n",
       "  'that',\n",
       "  'we',\n",
       "  'just',\n",
       "  'haven',\n",
       "  'been',\n",
       "  'able',\n",
       "  'to',\n",
       "  'get',\n",
       "  'to',\n",
       "  'because',\n",
       "  'the',\n",
       "  'water',\n",
       "  'is',\n",
       "  'so',\n",
       "  'high',\n",
       "  'kennon',\n",
       "  'said',\n",
       "  'but',\n",
       "  'they',\n",
       "  'are',\n",
       "  'safe',\n",
       "  'in',\n",
       "  'their',\n",
       "  'home',\n",
       "  'as',\n",
       "  'soon',\n",
       "  'as',\n",
       "  'the',\n",
       "  'water',\n",
       "  'recedes',\n",
       "  'we',\n",
       "  'will',\n",
       "  'rescue',\n",
       "  'them',\n",
       "  'street',\n",
       "  'lights',\n",
       "  'were',\n",
       "  'knocked',\n",
       "  'out',\n",
       "  'in',\n",
       "  'downtown',\n",
       "  'mobile',\n",
       "  'trees',\n",
       "  'were',\n",
       "  'bent',\n",
       "  'over',\n",
       "  'as',\n",
       "  'the',\n",
       "  'rain',\n",
       "  'blew',\n",
       "  'sideways',\n",
       "  'in',\n",
       "  'the',\n",
       "  'howling',\n",
       "  'wind',\n",
       "  'in',\n",
       "  'downtown',\n",
       "  'pensacola',\n",
       "  'water',\n",
       "  'rushed',\n",
       "  'down',\n",
       "  'some',\n",
       "  'streets',\n",
       "  'like',\n",
       "  'river',\n",
       "  'rapids',\n",
       "  'forming',\n",
       "  'whitecaps',\n",
       "  'as',\n",
       "  'it',\n",
       "  'slapped',\n",
       "  'against',\n",
       "  'buildings',\n",
       "  'and',\n",
       "  'rose',\n",
       "  'above',\n",
       "  'the',\n",
       "  'tires',\n",
       "  'on',\n",
       "  'cars',\n",
       "  'before',\n",
       "  'sunrise',\n",
       "  'water',\n",
       "  'was',\n",
       "  'up',\n",
       "  'to',\n",
       "  'the',\n",
       "  'doors',\n",
       "  'of',\n",
       "  'jordan',\n",
       "  'muse',\n",
       "  'car',\n",
       "  'outside',\n",
       "  'the',\n",
       "  'pensacola',\n",
       "  'hotel',\n",
       "  'where',\n",
       "  'her',\n",
       "  'family',\n",
       "  'took',\n",
       "  'shelter',\n",
       "  'after',\n",
       "  'fleeing',\n",
       "  'their',\n",
       "  'mobile',\n",
       "  'home',\n",
       "  'the',\n",
       "  'power',\n",
       "  'failed',\n",
       "  'early',\n",
       "  'in',\n",
       "  'the',\n",
       "  'morning',\n",
       "  'making',\n",
       "  'it',\n",
       "  'too',\n",
       "  'stuffy',\n",
       "  'to',\n",
       "  'sleep',\n",
       "  'her',\n",
       "  'year',\n",
       "  'old',\n",
       "  'son',\n",
       "  'played',\n",
       "  'with',\n",
       "  'toys',\n",
       "  'underneath',\n",
       "  'the',\n",
       "  'hotel',\n",
       "  'room',\n",
       "  'desk',\n",
       "  'as',\n",
       "  'muse',\n",
       "  'peered',\n",
       "  'out',\n",
       "  'the',\n",
       "  'window',\n",
       "  'watching',\n",
       "  'rain',\n",
       "  'fly',\n",
       "  'by',\n",
       "  'in',\n",
       "  'sheets',\n",
       "  'the',\n",
       "  'power',\n",
       "  'trucks',\n",
       "  'are',\n",
       "  'the',\n",
       "  'only',\n",
       "  'ones',\n",
       "  'above',\n",
       "  'water',\n",
       "  'and',\n",
       "  'they',\n",
       "  're',\n",
       "  'the',\n",
       "  'biggest',\n",
       "  'muse',\n",
       "  'said',\n",
       "  'can',\n",
       "  'believe',\n",
       "  'it',\n",
       "  'got',\n",
       "  'so',\n",
       "  'bad',\n",
       "  'that',\n",
       "  'why',\n",
       "  'we',\n",
       "  'came',\n",
       "  'here',\n",
       "  'michele',\n",
       "  'lamar',\n",
       "  'acuff',\n",
       "  'woke',\n",
       "  'to',\n",
       "  'the',\n",
       "  'thud',\n",
       "  'of',\n",
       "  'small',\n",
       "  'tree',\n",
       "  'falling',\n",
       "  'against',\n",
       "  'window',\n",
       "  'of',\n",
       "  'her',\n",
       "  'pensacola',\n",
       "  'home',\n",
       "  'waist',\n",
       "  'deep',\n",
       "  'water',\n",
       "  'gushed',\n",
       "  'down',\n",
       "  'her',\n",
       "  'street',\n",
       "  'above',\n",
       "  'the',\n",
       "  'loud',\n",
       "  'whistling',\n",
       "  'of',\n",
       "  'the',\n",
       "  'wind',\n",
       "  'she',\n",
       "  'heard',\n",
       "  'what',\n",
       "  'sounded',\n",
       "  'like',\n",
       "  'transformers',\n",
       "  'exploding',\n",
       "  'full',\n",
       "  'coverage',\n",
       "  'hurricanes',\n",
       "  'don',\n",
       "  'feel',\n",
       "  'safe',\n",
       "  'to',\n",
       "  'leave',\n",
       "  'lamar',\n",
       "  'acuff',\n",
       "  'said',\n",
       "  'from',\n",
       "  'the',\n",
       "  'porch',\n",
       "  'of',\n",
       "  'neighbor',\n",
       "  'house',\n",
       "  'just',\n",
       "  'staying',\n",
       "  'put',\n",
       "  'and',\n",
       "  'hoping',\n",
       "  'for',\n",
       "  'the',\n",
       "  'best',\n",
       "  'sally',\n",
       "  'blew',\n",
       "  'ashore',\n",
       "  'as',\n",
       "  'category',\n",
       "  'storm',\n",
       "  'but',\n",
       "  'weakened',\n",
       "  'to',\n",
       "  'still',\n",
       "  'dangerous',\n",
       "  'category',\n",
       "  'with',\n",
       "  'winds',\n",
       "  'of',\n",
       "  'mph',\n",
       "  'by',\n",
       "  'mid',\n",
       "  'morning',\n",
       "  'it',\n",
       "  'was',\n",
       "  'moving',\n",
       "  'to',\n",
       "  'the',\n",
       "  'northeast',\n",
       "  'at',\n",
       "  'mph',\n",
       "  'kph',\n",
       "  'forecasters',\n",
       "  'warned',\n",
       "  'that',\n",
       "  'heavy',\n",
       "  'rain',\n",
       "  'will',\n",
       "  'continue',\n",
       "  'into',\n",
       "  'thursday',\n",
       "  'as',\n",
       "  'the',\n",
       "  'storm',\n",
       "  'moves',\n",
       "  'inland',\n",
       "  'over',\n",
       "  'alabama',\n",
       "  'and',\n",
       "  'into',\n",
       "  'central',\n",
       "  'georgia',\n",
       "  'national',\n",
       "  'hurricane',\n",
       "  'center',\n",
       "  'forecaster',\n",
       "  'stacy',\n",
       "  'stewart',\n",
       "  'said',\n",
       "  'the',\n",
       "  'rain',\n",
       "  'will',\n",
       "  'be',\n",
       "  'catastrophic',\n",
       "  'and',\n",
       "  'life',\n",
       "  'threatening',\n",
       "  'over',\n",
       "  'portions',\n",
       "  'of',\n",
       "  'the',\n",
       "  'gulf',\n",
       "  'coast',\n",
       "  'forecasters',\n",
       "  'predicted',\n",
       "  'to',\n",
       "  'inches',\n",
       "  'centimeters',\n",
       "  'of',\n",
       "  'rain',\n",
       "  'with',\n",
       "  'up',\n",
       "  'to',\n",
       "  'inches',\n",
       "  'centimeters',\n",
       "  'in',\n",
       "  'some',\n",
       "  'spots',\n",
       "  'sally',\n",
       "  'has',\n",
       "  'characteristic',\n",
       "  'that',\n",
       "  'isn',\n",
       "  'often',\n",
       "  'seen',\n",
       "  'and',\n",
       "  'that',\n",
       "  'slow',\n",
       "  'forward',\n",
       "  'speed',\n",
       "  'and',\n",
       "  'that',\n",
       "  'going',\n",
       "  'to',\n",
       "  'exacerbate',\n",
       "  'the',\n",
       "  'flooding',\n",
       "  'said',\n",
       "  'ed',\n",
       "  'rappaport',\n",
       "  'deputy',\n",
       "  'director',\n",
       "  'of',\n",
       "  'the',\n",
       "  'hurricane',\n",
       "  'center',\n",
       "  'he',\n",
       "  'likened',\n",
       "  'the',\n",
       "  'storm',\n",
       "  'plodding',\n",
       "  'pace',\n",
       "  'to',\n",
       "  'that',\n",
       "  'of',\n",
       "  'hurricane',\n",
       "  'harvey',\n",
       "  'which',\n",
       "  'inundated',\n",
       "  'houston',\n",
       "  'in',\n",
       "  'sally',\n",
       "  'effects',\n",
       "  'were',\n",
       "  'felt',\n",
       "  'all',\n",
       "  'along',\n",
       "  'the',\n",
       "  'northern',\n",
       "  'gulf',\n",
       "  'coast',\n",
       "  'low',\n",
       "  'lying',\n",
       "  'properties',\n",
       "  'in',\n",
       "  'southeastern',\n",
       "  'louisiana',\n",
       "  'were',\n",
       "  'swamped',\n",
       "  'by',\n",
       "  'the',\n",
       "  'surge',\n",
       "  'water',\n",
       "  'covered',\n",
       "  'mississippi',\n",
       "  'beaches',\n",
       "  'and',\n",
       "  'parts',\n",
       "  'of',\n",
       "  'the',\n",
       "  'highway',\n",
       "  'that',\n",
       "  'runs',\n",
       "  'parallel',\n",
       "  'to',\n",
       "  'them',\n",
       "  'president',\n",
       "  'donald',\n",
       "  'trump',\n",
       "  'issued',\n",
       "  'emergency',\n",
       "  'declarations',\n",
       "  'for',\n",
       "  'parts',\n",
       "  'of',\n",
       "  'florida',\n",
       "  'alabama',\n",
       "  'mississippi',\n",
       "  'and',\n",
       "  'louisiana',\n",
       "  'white',\n",
       "  'house',\n",
       "  'press',\n",
       "  'secretary',\n",
       "  'kayleigh',\n",
       "  'mcenany',\n",
       "  'said',\n",
       "  'on',\n",
       "  'fox',\n",
       "  'news',\n",
       "  'channel',\n",
       "  'that',\n",
       "  'trump',\n",
       "  ...]]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_text_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.get_document_topics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
