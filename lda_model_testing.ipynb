{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA model testing for downloaded article database for 'debiaser' data product\n",
    "#### Sagar Setru, September 21th, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief description using CoNVO framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context\n",
    "\n",
    "Some people are eager to get news from outside of their echo chamber. However, they do not know where to go outside of their echo chambers, and may also have some activation energy when it comes to seeking information from other sources. In the meantime, most newsfeeds only push you content that you agree with. You end up in an echo chamber, but may not have ever wanted to be in one in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need\n",
    "\n",
    "A way to find news articles from different yet reliable media sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision\n",
    "\n",
    "Debiaser, a data product (maybe Chrome plug-in?) that will recommend news articles similar in topic to the one currently being read, but from several pre-curated and reliable news media organizations across the political spectrum, for example, following the \"media bias chart\" here https://www.adfontesmedia.com/ or the \"media bias ratings\" here: https://www.allsides.com/media-bias/media-bias-ratings. The app will determine the main topics of the text of a news article, and then show links to similar articles from other news organizations.\n",
    "\n",
    "The product will generate topics for a given document via latent Dirichlet allocation (LDA) and then search news websites for the topic words generated.\n",
    "\n",
    "Caveats: Many of these articles may be behind paywalls. News aggregators already basically do this. How different is this than just searching Google using the title of an article?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outcome\n",
    "\n",
    "People who are motivated to engage in content outside of their echo chambers have a tool that enables them to quickly find news similar to what they are currently reading, but from a variety of news organizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing LDA on larger document corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conda environment:\n",
      "insight\n"
     ]
    }
   ],
   "source": [
    "print('Conda environment:')\n",
    "print(os.environ['CONDA_DEFAULT_ENV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import text processing and NLP specific packages\n",
    "\n",
    "# for generating LDA models\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# for preprocessing documents\n",
    "from gensim.parsing.preprocessing import preprocess_documents\n",
    "\n",
    "# for counting frequency of words\n",
    "from collections import defaultdict\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stop_words_csv_to_list(full_file_name):\n",
    "    \"\"\"fxn that loads stop words list downloaded from git repo called 'news-stopwords'\"\"\"\n",
    "    \n",
    "    stop_words = pd.read_csv(full_file_name)\n",
    "\n",
    "    stop_words = stop_words['term']\n",
    "\n",
    "    stop_words = [word for word in stop_words]\n",
    "    \n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_corpus_dictionary_bow(texts,word_frequency_threshold):\n",
    "    \"\"\"fxn returns corpus, proc. dict, bag of words\"\"\"\n",
    "    \n",
    "    # Count word frequencies\n",
    "    frequency = defaultdict(int)\n",
    "    for text in texts:\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "\n",
    "    # Only keep words that appear more than set frequency, to produce the corpus\n",
    "    processed_corpus = [[token for token in text if frequency[token] > word_frequency_threshold] for text in texts]\n",
    "    \n",
    "    # generate a dictionary via gensim\n",
    "    processed_dictionary = Dictionary(processed_corpus)\n",
    "    \n",
    "    # generate bag of words of the corpus\n",
    "    bow_corpus = [processed_dictionary.doc2bow(text) for text in processed_corpus]\n",
    "    \n",
    "    return processed_corpus, processed_dictionary, bow_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose list of stop words\n",
    "\n",
    "# choose whether 1k, 10k, 100k, or nltk\n",
    "which_stop_words = '1k'\n",
    "# which_stop_words = '10k'\n",
    "# which_stop_words = '100k'\n",
    "# which_stop_words = 'nltk'\n",
    "\n",
    "stop_words_path = '/Users/sagarsetru/Documents/post PhD positions search/insightDataScience/project/debiaser/stop_words_db/news-stopwords-master/'\n",
    "\n",
    "\n",
    "if which_stop_words == '1k':\n",
    "    \n",
    "    # doing 1k words list\n",
    "    stop_words_file_name = 'sw1k.csv'\n",
    "    \n",
    "    # make full file name\n",
    "    stop_words_full_file_name = stop_words_path+stop_words_file_name\n",
    "    \n",
    "    # get list of stop words\n",
    "    stop_words = load_stop_words_csv_to_list(stop_words_full_file_name)\n",
    "    \n",
    "elif which_stop_words == '10k':\n",
    "    \n",
    "    # doing 10k words list\n",
    "    stop_words_file_name = 'sw10k.csv'\n",
    "\n",
    "    # make full file name\n",
    "    stop_words_full_file_name = stop_words_path+stop_words_file_name\n",
    "    \n",
    "    # get list of stop words\n",
    "    stop_words = load_stop_words_csv_to_list(stop_words_full_file_name)\n",
    "\n",
    "elif which_stop_words == '100k':\n",
    "    \n",
    "    # doing 100k\n",
    "    stop_words_file_name = 'sw100k.csv'  \n",
    "    \n",
    "    # get full file name\n",
    "    stop_words_full_file_name = stop_words_path+stop_words_file_name\n",
    "    \n",
    "    # get list of stop words\n",
    "    stop_words = load_stop_words_csv_to_list(stop_words_full_file_name)\n",
    "\n",
    "\n",
    "elif which_stop_words == 'nltk':\n",
    "    # import from nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "else:\n",
    "    print('Select proper variable name for \"which_stop_words\"')\n",
    "    \n",
    "# adding custom words\n",
    "stop_words.append('said')\n",
    "stop_words.append('youre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sagarsetru/anaconda3/envs/insight/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3145: DtypeWarning: Columns (12,13) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# load csv of processed data to pandas dataframe\n",
    "articles_df = pd.read_csv('./all_the_news/all_news_df_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>index.1</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>publication</th>\n",
       "      <th>category</th>\n",
       "      <th>digital</th>\n",
       "      <th>section</th>\n",
       "      <th>url</th>\n",
       "      <th>article_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Agent Cooper in Twin Peaks is the audience: on...</td>\n",
       "      <td>\\nTasha Robinson\\n</td>\n",
       "      <td>2017-05-31</td>\n",
       "      <td>And never more so than in Showtimeâ€™s new...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>AI, the humanity!</td>\n",
       "      <td>\\nSam Byford\\n</td>\n",
       "      <td>2017-05-30</td>\n",
       "      <td>AlphaGoâ€™s victory isnâ€™t a defeat for hum...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>The Viral Machine</td>\n",
       "      <td>\\nKaitlyn Tiffany\\n</td>\n",
       "      <td>2017-05-25</td>\n",
       "      <td>Super Deluxe built a weird internet empi...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>How Anker is beating Apple and Samsung at thei...</td>\n",
       "      <td>\\nNick Statt\\n</td>\n",
       "      <td>2017-05-22</td>\n",
       "      <td>Steven Yang quit his job at Google in th...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Tour Black Pantherâ€™s reimagined homeland with ...</td>\n",
       "      <td>\\nKwame Opam\\n</td>\n",
       "      <td>2017-05-15</td>\n",
       "      <td>Ahead of Black Pantherâ€™s 2018 theatrical...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  index.1  id                                              title  \\\n",
       "0      0        0   1  Agent Cooper in Twin Peaks is the audience: on...   \n",
       "1      1        1   2                                  AI, the humanity!   \n",
       "2      2        2   3                                  The Viral Machine   \n",
       "3      3        3   4  How Anker is beating Apple and Samsung at thei...   \n",
       "4      4        4   5  Tour Black Pantherâ€™s reimagined homeland with ...   \n",
       "\n",
       "                author        date  \\\n",
       "0   \\nTasha Robinson\\n  2017-05-31   \n",
       "1       \\nSam Byford\\n  2017-05-30   \n",
       "2  \\nKaitlyn Tiffany\\n  2017-05-25   \n",
       "3       \\nNick Statt\\n  2017-05-22   \n",
       "4       \\nKwame Opam\\n  2017-05-15   \n",
       "\n",
       "                                             content    year  month  \\\n",
       "0        And never more so than in Showtimeâ€™s new...  2017.0    5.0   \n",
       "1        AlphaGoâ€™s victory isnâ€™t a defeat for hum...  2017.0    5.0   \n",
       "2        Super Deluxe built a weird internet empi...  2017.0    5.0   \n",
       "3        Steven Yang quit his job at Google in th...  2017.0    5.0   \n",
       "4        Ahead of Black Pantherâ€™s 2018 theatrical...  2017.0    5.0   \n",
       "\n",
       "  publication  category  digital section  url  article_length  \n",
       "0       Verge  Longform      1.0     NaN  NaN            2121  \n",
       "1       Verge  Longform      1.0     NaN  NaN            1948  \n",
       "2       Verge  Longform      1.0     NaN  NaN            3011  \n",
       "3       Verge  Longform      1.0     NaN  NaN            3281  \n",
       "4       Verge  Longform      1.0     NaN  NaN             239  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview the data\n",
    "articles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# get just the articles content and titles\n",
    "articles_content = articles_df['content'].astype('str')\n",
    "articles_titles = articles_df['title'].astype('str')\n",
    "\n",
    "# check for nans; if there are any, make sure to not add nan\n",
    "print(articles_df['title'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "# merge titles and content\n",
    "article_full = []\n",
    "\n",
    "for content,title in zip(articles_content, articles_titles):\n",
    "    \n",
    "    # don't add word 'nan'\n",
    "    if title == 'nan':\n",
    "        \n",
    "        print(title)\n",
    "        \n",
    "        article_full.append(content)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        article_full.append(title+content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(common_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
