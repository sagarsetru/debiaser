{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF testing for downloaded article database for 'debiaser' data product\n",
    "#### Sagar Setru, September 21th, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief description using CoNVO framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context\n",
    "\n",
    "Some people are eager to get news from outside of their echo chamber. However, they do not know where to go outside of their echo chambers, and may also have some activation energy when it comes to seeking information from other sources. In the meantime, most newsfeeds only push you content that you agree with. You end up in an echo chamber, but may not have ever wanted to be in one in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need\n",
    "\n",
    "A way to find news articles from different yet reliable media sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision\n",
    "\n",
    "Debiaser, a data product (maybe Chrome plug-in?) that will recommend news articles similar in topic to the one currently being read, but from several pre-curated and reliable news media organizations across the political spectrum, for example, following the \"media bias chart\" here https://www.adfontesmedia.com/ or the \"media bias ratings\" here: https://www.allsides.com/media-bias/media-bias-ratings. The app will determine the main topics of the text of a news article, and then show links to similar articles from other news organizations.\n",
    "\n",
    "The product will generate topics and keywords for a given document via LDA and TF-IDF then search news websites for the topic words generated.\n",
    "\n",
    "Caveats: Many of these articles may be behind paywalls. News aggregators already basically do this. How different is this than just searching Google using the title of an article?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outcome\n",
    "\n",
    "People who are motivated to engage in content outside of their echo chambers have a tool that enables them to quickly find news similar to what they are currently reading, but from a variety of news organizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing TFIDF on larger document corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "import json\n",
    "\n",
    "# NLP Packages\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# to break articles up into sentences\n",
    "from nltk import tokenize\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "from debiaser_validation_function import return_suggested_articles2\n",
    "from debiaser_validation_function import make_bigrams\n",
    "from debiaser_validation_function import make_trigrams\n",
    "from debiaser_validation_function import make_quadgrams\n",
    "\n",
    "from text_processing_functions import process_all_articles\n",
    "from text_processing_functions import remove_stopwords\n",
    "from text_processing_functions import get_simple_corpus_dictionary_bow\n",
    "\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import string\n",
    "\n",
    "import pickle\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conda environment:\n",
      "debiaser\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print('Conda environment:')\n",
    "print(os.environ['CONDA_DEFAULT_ENV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# load dictionary used to train model on EC2\n",
    "id2word_file = '/Users/sagarsetru/Documents/post PhD positions search/insightDataScience/project/debiaser/all_the_news/id2word_ec2.pkl'\n",
    "with open(id2word_file, 'rb') as pickle_file:\n",
    "    processed_dictionary = pickle.load(pickle_file)\n",
    "    \n",
    "# load the processed bow corpus\n",
    "corpus_file = '/Users/sagarsetru/Documents/post PhD positions search/insightDataScience/project/debiaser/all_the_news/corpus.pkl'\n",
    "with open(corpus_file, 'rb') as pickle_file:\n",
    "    bow_corpus = pickle.load(pickle_file)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "CPU times: user 8.68 s, sys: 5.04 s, total: 13.7 s\n",
      "Wall time: 17.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tfidf = TfidfModel(bow_corpus, id2word=processed_dictionary)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "pkl_file_name = '/Users/sagarsetru/Documents/post PhD positions search/insightDataScience/project/debiaser/all_the_news/tfidf_matrix.pkl'\n",
    "pickle.dump( tfidf, open( pkl_file_name, 'wb'))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pkl_file_name, 'rb') as pickle_file:\n",
    "    tfidf = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://www.nytimes.com/2020/09/25/us/politics/rbg-retirement-obama.html'\n",
    "url = 'https://www.nytimes.com/2020/09/30/health/covid-cruise-ships.html'\n",
    "url = 'https://www.theguardian.com/us-news/2020/oct/06/vice-presidential-debates-white-house-covid-19'\n",
    "# url = 'https://www.npr.org/2020/10/06/920684113/michelle-obama-makes-final-pitch-vote-for-joe-biden-like-your-lives-depend-on-it'\n",
    "# url = 'https://www.foxnews.com/politics/pence-warns-voters-you-wont-be-safe-if-biden-wins'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFTER STOPWORDS\n",
      "[['covid', 'outbreak', 'overshadows', 'vice', 'debate', 'wed', 'oct', 'edt', 'modified', 'wed', 'oct', 'edtafter', 'exclamation', 'recklessness', 'handling', 'coronavirus', 'crisis', 'vice', 'mike', 'pence', 'defend', 'televised', 'debate', 'democratic', 'vice', 'nominee', 'kamala', 'harris', 'pandemic', 'task', 'pence', 'polls', 'indicating', 'majority', 'americans', 'faith', 'ability', 'confront', 'virus', 'blame', 'mishandling', 'counter', 'pence', 'explain', 'virus', 'tear', 'republican', 'donor', 'circles', 'hospitalizing', 'exposing', 'mounting', 'secret', 'personnel', 'covid', 'pence', 'deepest', 'infection', 'zone', 'karen', 'pence', 'virus', 'perhaps', 'daunting', 'pence', 'harris', 'sharp', 'cross', 'examinations', 'settings', 'powerful', 'attorney', 'william', 'barr', 'supreme', 'brett', 'kavanaugh', 'tough', 'mike', 'pence', 'karthik', 'ganapathy', 'progressive', 'strategist', 'mvmt', 'communications', 'outbreak', 'metaphor', 'handling', 'virus', 'writ', 'mike', 'pence', 'defend', 'senator', 'harris', 'thoughtful', 'illustrates', 'sort', 'leadership', 'historic', 'benchmark', 'harris', 'attorney', 'california', 'color', 'ticket', 'analysts', 'focused', 'pence', 'failings', 'rattle', 'apparently', 'democratic', 'nominee', 'joe', 'biden', 'enjoy', 'race', 'convalescing', 'testing', 'positive', 'coronavirus', 'democrats', 'anxious', 'protect', 'biden', 'debated', 'elephant', 'pence', 'harris', 'foreseeably', 'ascend', 'presidency', 'debate', 'vp', 'debates', 'hudak', 'deputy', 'effective', 'brookings', 'institution', 'americans', 'watching', 'vp', 'debate', 'renewed', 'awareness', 'individuals', 'easily', 'ran', 'succeeded', 'nod', 'extraordinary', 'precautions', 'demands', 'preview', 'clash', 'onstage', 'sides', 'traded', 'appropriate', 'measures', 'biden', 'harris', 'camp', 'demanded', 'candidates', 'shield', 'separating', 'pence', 'spokesperson', 'scoffed', 'senator', 'harris', 'fortress', 'herself', 'guidelines', 'centers', 'disease', 'prevention', 'cdc', 'recommend', 'candidates', 'separated', 'shield', 'pence', 'quarters', 'covid', 'cases', 'appear', 'placing', 'quarantine', 'letter', 'pence', 'doctor', 'jesse', 'schonau', 'ignored', 'advice', 'vice', 'mike', 'pence', 'contact', 'individuals', 'tested', 'positive', 'covid', 'letter', 'eliding', 'pence', 'contact', 'female', 'democratic', 'outcry', 'carelessness', 'virus', 'demanding', 'pence', 'confirmation', 'negative', 'coronavirus', 'skipped', 'pre', 'debate', 'coronavirus', 'debate', 'commission', 'plexiglass', 'shield', 'separate', 'candidates', 'wearing', 'mask', 'escorted', 'hall', 'adviser', 'barack', 'tommy', 'vietor', 'advice', 'harris', 'vietor', 'podcast', 'mike', 'pence', 'safe', 'mike', 'pence', 'covid', 'tomorrow', 'madness', 'madness', 'debate', 'debate', 'scheduled', 'utah', 'salt', 'lake', 'et', 'moderated', 'susan', 'page', 'bureau', 'usa', 'implemented', 'lottery', 'invite', 'fewer', 'guests', 'sit', 'hall', 'distance', 'wearing', 'masks', 'respective', 'debate', 'preparations', 'harris', 'pence', 'faced', 'ins', 'democratic', 'candidate', 'pete', 'buttigieg', 'debated', 'harris', 'pence', 'florida', 'attorney', 'pam', 'bondi', 'debated', 'pence', 'harris', 'pence', 'debate', 'outing', 'hillary', 'clinton', 'mate', 'tim', 'kaine', 'mostly', 'forgotten', 'fireworks', 'confrontation', 'harris', 'harris', 'prosecutor', 'reputation', 'senate', 'judiciary', 'methodical', 'ominous', 'questioning', 'hostile', 'witnesses', 'attorney', 'barr', 'whom', 'harris', 'pressed', 'pursuing', 'politically', 'motivated', 'investigations', 'suggested', 'harris', 'barr', 'fumbled', 'remember', 'coronavirus', 'pandemic', 'dominated', 'dialogue', 'closing', 'harris', 'box', 'pence', 'stance', 'climate', 'emergency', 'racial', 'criminal', 'ganapathy', 'senator', 'harris', 'trap', 'arguing', 'providing', 'trap', 'send', 'racist', 'dog', 'whistles']]\n",
      "TIME FOR NGRAMS\n",
      "0.002969000000007327\n",
      "AFTER NGRAMS\n"
     ]
    }
   ],
   "source": [
    "# dummy nlp variable\n",
    "nlp=[]\n",
    "\n",
    "# if lemmatizing into sentences\n",
    "do_sentences = 0\n",
    "\n",
    "print_article = 0\n",
    "\n",
    "do_ngrams = 1\n",
    "    \n",
    "# get html content of url\n",
    "page = requests.get(url)\n",
    "coverpage = page.content\n",
    "\n",
    "# create soup object\n",
    "soup = BeautifulSoup(coverpage, 'html.parser')\n",
    "\n",
    "# get title\n",
    "headline = soup.find('h1').get_text()\n",
    "\n",
    "# get text from all <p> tags\n",
    "p_tags = soup.find_all('p')\n",
    "\n",
    "# get text from each p tag and strip whitespace\n",
    "p_tags_text = [tag.get_text().strip() for tag in p_tags]\n",
    "\n",
    "# filter out sentences without periods\n",
    "p_tags_text = [sentence for sentence in p_tags_text if '.' in sentence]\n",
    "\n",
    "# convert all p_tags_text to single article text string\n",
    "p_tags_text_1string = ''\n",
    "\n",
    "for p_tag_text in p_tags_text:\n",
    "    p_tags_text_1string += p_tag_text\n",
    "\n",
    "# if print_ptags:\n",
    "#     print(p_tags)\n",
    "#     print(' ')\n",
    "#     print(p_tags_text_1string)\n",
    "\n",
    "combined_article = headline+'. '+p_tags_text_1string\n",
    "if print_article:\n",
    "    print(combined_article)\n",
    "\n",
    "\n",
    "if do_ngrams:\n",
    "    bigram_mod_file = '/Users/sagarsetru/Documents/post PhD positions search/insightDataScience/project/debiaser/all_the_news/bigram_mod.pkl'\n",
    "    with open(bigram_mod_file, 'rb') as pickle_file:\n",
    "        bigram_mod = pickle.load(pickle_file)\n",
    "\n",
    "    trigram_mod_file = '/Users/sagarsetru/Documents/post PhD positions search/insightDataScience/project/debiaser/all_the_news/trigram_mod.pkl'\n",
    "    with open(trigram_mod_file, 'rb') as pickle_file:\n",
    "        trigram_mod = pickle.load(pickle_file)\n",
    "\n",
    "    quadgram_mod_file = '/Users/sagarsetru/Documents/post PhD positions search/insightDataScience/project/debiaser/all_the_news/quadgram_mod.pkl'\n",
    "    with open(quadgram_mod_file, 'rb') as pickle_file:\n",
    "        quadgram_mod = pickle.load(pickle_file)\n",
    "\n",
    "    # use nltk stopwords\n",
    "    # stop_words = list(stopwords.words('english'))\n",
    "    \n",
    "# load stop words\n",
    "stop_words = pd.read_csv('/Users/sagarsetru/Documents/post PhD positions search/insightDataScience/project/debiaser/stop_words_db/news-stopwords-master/sw1k.csv')\n",
    "\n",
    "stop_words = stop_words['term']\n",
    "stop_words = [word for word in stop_words]\n",
    "\n",
    "\n",
    "# load all sides data\n",
    "all_sides = pd.read_csv('/Users/sagarsetru/Documents/post PhD positions search/insightDataScience/project/debiaser/all_sides_media_data/allsides_final_plus_others_with_domains.csv')\n",
    "\n",
    "all_sides_names = all_sides['name']\n",
    "all_sides_domains = all_sides['domain']\n",
    "\n",
    "# break up into sentences\n",
    "if do_sentences:\n",
    "    combined_article = tokenize.sent_tokenize(combined_article)\n",
    "else:\n",
    "    combined_article = [combined_article]\n",
    "\n",
    "if print_article:\n",
    "    print('TOKENIZED TO SENTENCES')\n",
    "    print(combined_article)\n",
    "\n",
    "article_processed = process_all_articles(combined_article,nlp)\n",
    "\n",
    "# remove stopwords\n",
    "article_processed = remove_stopwords(article_processed,stop_words)\n",
    "print('AFTER STOPWORDS')\n",
    "print(article_processed)\n",
    "\n",
    "\n",
    "if do_ngrams:\n",
    "\n",
    "    start = time.process_time()\n",
    "    article_processed = make_quadgrams(article_processed,bigram_mod,trigram_mod,quadgram_mod)\n",
    "    print('TIME FOR NGRAMS')\n",
    "    print(time.process_time() - start)\n",
    "    print('AFTER NGRAMS')\n",
    "\n",
    "# generate bag of words of the article\n",
    "bow_corpus_article = [processed_dictionary.doc2bow(text) for text in article_processed]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bow_corpus_article[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "CPU times: user 3.29 ms, sys: 14.2 ms, total: 17.4 ms\n",
      "Wall time: 111 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tfidf_vector = tfidf[bow_corpus_article[0]]\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getKey(item):\n",
    "    return item[1]\n",
    "\n",
    "tfidf_vector_sort = sorted(tfidf_vector,key=getKey,reverse=True)\n",
    "len(tfidf_vector_sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13993, 36232, 127767, 231756, 12068, 22705, 9138, 3804, 6095, 4073]\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "n_keywords = 10\n",
    "\n",
    "top_tfidf_values = [tfidf_vector_sort[i][0] for i in range(0,n_keywords)]\n",
    "print(top_tfidf_values)\n",
    "\n",
    "top_words_list = [processed_dictionary[i].replace(\"_\",\" \") for i in top_tfidf_values]\n",
    "\n",
    "top_words_string = ' '\n",
    "for word in top_words_list:\n",
    "    \n",
    "    if word not in top_words_string:\n",
    "        top_words_string += ' '+word\n",
    "\n",
    "# top_words = [processed_dictionary[top_tfidf_values[0][0]]]\n",
    "\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['harris', 'pence', 'mike pence', 'coronavirus', 'virus', 'shield', 'attorney', 'debate', 'senator', 'debated']\n",
      "['harris', 'pence', 'mike', 'pence', 'coronavirus', 'virus', 'shield', 'attorney', 'debate', 'senator', 'debated']\n"
     ]
    }
   ],
   "source": [
    "print(top_words_list)\n",
    "# print(top_words_string)\n",
    "\n",
    "top_words_list2 = []\n",
    "for word in top_words_list:\n",
    "    if \" \" in word:\n",
    "        word_split = word.split()\n",
    "        \n",
    "        for new_word in word_split:\n",
    "            top_words_list2.append(new_word)\n",
    "            \n",
    "    else:\n",
    "        top_words_list2.append(word)\n",
    "print(top_words_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word in top_words_list:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pence', 'biden', 'coronavirus', 'vice', 'lady', 'spotlighted', 'factset', 'convention', 'fighting', 'quotes']\n",
      "  pence biden coronavirus vice lady spotlighted factset convention fighting quotes\n"
     ]
    }
   ],
   "source": [
    "print(top_words_list)\n",
    "print(top_words_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mike', 'pence']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'mike pence'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
