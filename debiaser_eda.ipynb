{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA for 'debiaser' data product\n",
    "#### Sagar Setru, September 16th, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief description using CoNVO framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context\n",
    "\n",
    "Some people are eager to get news from outside of their echo chamber. However, they do not know where to go outside of their echo chambers, and may also have some activation energy when it comes to seeking information from other sources. In the meantime, most newsfeeds only push you content that you agree with. You end up in an echo chamber, but may not have ever wanted to be in one in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need\n",
    "\n",
    "A way to find news articles from different yet reliable media sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision\n",
    "\n",
    "Debiaser, a data product (maybe Chrome plug-in?) that will recommend news articles similar in topic to the one currently being read, but from several pre-curated and reliable news media organizations across the political spectrum, for example, following the \"media bias chart\" here https://www.adfontesmedia.com/ or the \"media bias ratings\" here: https://www.allsides.com/media-bias/media-bias-ratings. The app will determine the main topics of the text of a news article, and then show links to similar articles from other news organizations.\n",
    "\n",
    "Caveats: Many of these articles may be behind paywalls. News aggregators already basically do this. How different is this than just searching Google using the title of an article?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outcome\n",
    "\n",
    "People who are motivated to engage in content outside of their echo chambers have a tool that enables them to quickly find news similar to what they are currently reading, but from a variety of news organizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conda environment:\n",
      "insight\n"
     ]
    }
   ],
   "source": [
    "# make sure I'm in the right environment\n",
    "\n",
    "print('Conda environment:')\n",
    "print(os.environ['CONDA_DEFAULT_ENV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import base packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import text processing and NLP specific packages\n",
    "\n",
    "# for generating LDA models\n",
    "import gensim\n",
    "\n",
    "# to break articles up into sentences (currently not in use)\n",
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stop_words_csv_to_list(full_file_name):\n",
    "    \"\"\"fxn that loads stop words list downloaded from git repo called 'news-stopwords'\"\"\"\n",
    "    \n",
    "    stop_words = pd.read_csv(full_file_name)\n",
    "\n",
    "    stop_words = stop_words['term']\n",
    "\n",
    "    stop_words = [word for word in stop_words]\n",
    "    \n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of manually made text files of articles\n",
    "\n",
    "full_path = '/Users/sagarsetru/Documents/post PhD positions search/insightDataScience/project/debiaser/article_text_files/'\n",
    "\n",
    "full_file_names = [\n",
    "full_path+'ap_hurricane_sally_unleashes_20200916.txt',\n",
    "full_path+'cnn_big_ten_backtracks_20200916.txt',\n",
    "full_path+'nyt_on_the_fire_line_20200915.txt',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose list of stop words\n",
    "\n",
    "# choose whether 1k, 10k, 100k, or nltk\n",
    "which_stop_words = '1k'\n",
    "# which_stop_words = '10k'\n",
    "# which_stop_words = '100k'\n",
    "# which_stop_words = 'nltk'\n",
    "\n",
    "stop_words_path = '/Users/sagarsetru/Documents/post PhD positions search/insightDataScience/project/debiaser/stop_words_db/news-stopwords-master/'\n",
    "\n",
    "\n",
    "if which_stop_words == '1k':\n",
    "    \n",
    "    # doing 1k words list\n",
    "    stop_words_file_name = 'sw1k.csv'\n",
    "    \n",
    "    # make full file name\n",
    "    stop_words_full_file_name = stop_words_path+stop_words_file_name\n",
    "    \n",
    "    # get list of stop words\n",
    "    stop_words = load_stop_words_csv_to_list(stop_words_full_file_name)\n",
    "    \n",
    "elif which_stop_words == '10k':\n",
    "    \n",
    "    # doing 10k words list\n",
    "    stop_words_file_name = 'sw10k.csv'\n",
    "\n",
    "    # make full file name\n",
    "    stop_words_full_file_name = stop_words_path+stop_words_file_name\n",
    "    \n",
    "    # get list of stop words\n",
    "    stop_words = load_stop_words_csv_to_list(stop_words_full_file_name)\n",
    "\n",
    "elif which_stop_words == '100k':\n",
    "    \n",
    "    # doing 100k\n",
    "    stop_words_file_name = 'sw100k.csv'  \n",
    "    \n",
    "    # get full file name\n",
    "    stop_words_full_file_name = stop_words_path+stop_words_file_name\n",
    "    \n",
    "    # get list of stop words\n",
    "    stop_words = load_stop_words_csv_to_list(stop_words_full_file_name)\n",
    "\n",
    "\n",
    "elif which_stop_words == 'nltk':\n",
    "    # import from nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "else:\n",
    "    print('Select proper variable name for \"which_stop_words\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through files\n",
    "\n",
    "# get full file names\n",
    "for ind, full_file_name in enumerate(full_file_names):\n",
    "    \n",
    "    # get the article text as one string, remove new lines\n",
    "    with open(full_file_name, 'r') as file:\n",
    "        article_text = file.read().replace('\\n', ' ')\n",
    "\n",
    "    # break article into sentences\n",
    "    article_sentences = tokenize.sent_tokenize(article_text)\n",
    "    \n",
    "    if ind == 1:\n",
    "        break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(6 unique tokens: ['coaches,', 'college', 'football', 'sports', 'student-athletes']...)\n"
     ]
    }
   ],
   "source": [
    "# Create a set of frequent meaningless words to remove\n",
    "\n",
    "word_frequency_threshold = 2\n",
    "\n",
    "# following https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#sphx-glr-auto-examples-core-run-core-concepts-py\n",
    "\n",
    "# Lowercase each document, split it by white space and filter out stopwords\n",
    "texts = [[word for word in document.lower().split() if word not in stop_words]\n",
    "         for document in [article_text]]\n",
    "\n",
    "# Count word frequencies\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# Only keep words that appear more than once\n",
    "processed_corpus = [[token for token in text if frequency[token] > word_frequency_threshold] for text in texts]\n",
    "\n",
    "processed_dictionary = Dictionary(processed_corpus)\n",
    "print(processed_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ten',\n",
       " 'football',\n",
       " 'ten',\n",
       " 'college',\n",
       " 'football',\n",
       " 'ten',\n",
       " 'football',\n",
       " 'ten',\n",
       " 'coaches,',\n",
       " 'ten',\n",
       " 'football',\n",
       " 'coaches,',\n",
       " 'coaches,',\n",
       " 'student-athletes',\n",
       " 'ten',\n",
       " 'student-athletes',\n",
       " 'college',\n",
       " 'football',\n",
       " 'ten',\n",
       " 'sports']"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_corpus[0][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [processed_dictionary.doc2bow(text) for text in processed_corpus]\n",
    "lda_2 = gensim.models.LdaModel(corpus=bow_corpus,num_topics=5,id2word=processed_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ten', 'football', 'sports', 'student', 'athletes', 'coaches', 'college']\n",
      "ten\n",
      "football\n",
      "sports\n",
      "student\n",
      "athletes\n",
      "coaches\n",
      "college\n",
      " \n",
      "['football', 'ten', 'sports', 'coaches', 'college', 'student', 'athletes']\n",
      "football\n",
      "ten\n",
      "sports\n",
      "coaches\n",
      "college\n",
      "student\n",
      "athletes\n",
      " \n",
      "['ten', 'sports', 'coaches', 'football', 'college', 'student', 'athletes']\n",
      "ten\n",
      "sports\n",
      "coaches\n",
      "football\n",
      "college\n",
      "student\n",
      "athletes\n",
      " \n",
      "['ten', 'football', 'sports', 'student', 'athletes', 'coaches', 'college']\n",
      "ten\n",
      "football\n",
      "sports\n",
      "student\n",
      "athletes\n",
      "coaches\n",
      "college\n",
      " \n",
      "['ten', 'football', 'sports', 'college', 'student', 'athletes', 'coaches']\n",
      "ten\n",
      "football\n",
      "sports\n",
      "college\n",
      "student\n",
      "athletes\n",
      "coaches\n",
      " \n",
      "['ten', 'football', 'sports', 'student', 'athletes', 'coaches', 'college', 'football', 'ten', 'sports', 'coaches', 'college', 'student', 'athletes', 'ten', 'sports', 'coaches', 'football', 'college', 'student', 'athletes', 'ten', 'football', 'sports', 'student', 'athletes', 'coaches', 'college', 'ten', 'football', 'sports', 'college', 'student', 'athletes', 'coaches']\n"
     ]
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "from gensim.parsing.preprocessing import strip_numeric\n",
    "\n",
    "# # lda.top_topics(corpus=bow_corpus,dictionary=processed_dictionary)\n",
    "# for i in range(0, lda.num_topics-1):\n",
    "#     current_topic = lda.print_topic(i)\n",
    "#     print(current_topic)\n",
    "    \n",
    "lda_topics = lda_2.show_topics()\n",
    "\n",
    "topics = []\n",
    "filters = [lambda x: x.lower(), strip_punctuation, strip_numeric]\n",
    "\n",
    "for topic in lda_topics:\n",
    "#     print(topic)\n",
    "    topics.append(preprocess_string(topic[1], filters))\n",
    "\n",
    "all_topic_words = []\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "    for topic_word in topic:\n",
    "        print(topic_word)\n",
    "        all_topic_words.append(topic_word)\n",
    "    \n",
    "    print(' ')\n",
    "\n",
    "print(all_topic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
