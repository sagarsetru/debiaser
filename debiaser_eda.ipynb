{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA for 'debiaser' data product\n",
    "#### Sagar Setru, September 16th, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief description using CoNVO framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context\n",
    "\n",
    "Some people are eager to get news from outside of their echo chamber. However, they do not know where to go outside of their echo chambers, and may also have some activation energy when it comes to seeking information from other sources. In the meantime, most newsfeeds only push you content that you agree with. You end up in an echo chamber, but may not have ever wanted to be in one in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need\n",
    "\n",
    "A way to find news articles from different yet reliable media sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision\n",
    "\n",
    "Debiaser, a data product (maybe Chrome plug-in?) that will recommend news articles similar in topic to the one currently being read, but from several pre-curated and reliable news media organizations across the political spectrum, for example, following the \"media bias chart\" here https://www.adfontesmedia.com/ or the \"media bias ratings\" here: https://www.allsides.com/media-bias/media-bias-ratings. The app will determine the main topics of the text of a news article, and then show links to similar articles from other news organizations.\n",
    "\n",
    "Caveats: Many of these articles may be behind paywalls. News aggregators already basically do this. How different is this than just searching Google using the title of an article?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outcome\n",
    "\n",
    "People who are motivated to engage in content outside of their echo chambers have a tool that enables them to quickly find news similar to what they are currently reading, but from a variety of news organizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conda environment:\n",
      "insight\n"
     ]
    }
   ],
   "source": [
    "# make sure I'm in the right environment\n",
    "\n",
    "print('Conda environment:')\n",
    "print(os.environ['CONDA_DEFAULT_ENV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import base packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import text processing and NLP specific packages\n",
    "\n",
    "# for generating LDA models\n",
    "import gensim\n",
    "\n",
    "# for preprocessing documents\n",
    "from gensim.parsing.preprocessing import preprocess_documents\n",
    "\n",
    "# to break articles up into sentences (currently not in use)\n",
    "from nltk import tokenize\n",
    "\n",
    "# for doing lemmatizing\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "\n",
    "# for counting frequency of words\n",
    "from collections import defaultdict\n",
    "\n",
    "# for processing lda topic output\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "from gensim.parsing.preprocessing import strip_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stop_words_csv_to_list(full_file_name):\n",
    "    \"\"\"fxn that loads stop words list downloaded from git repo called 'news-stopwords'\"\"\"\n",
    "    \n",
    "    stop_words = pd.read_csv(full_file_name)\n",
    "\n",
    "    stop_words = stop_words['term']\n",
    "\n",
    "    stop_words = [word for word in stop_words]\n",
    "    \n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of manually made text files of articles\n",
    "\n",
    "full_path = '/Users/sagarsetru/Documents/post PhD positions search/insightDataScience/project/debiaser/article_text_files/'\n",
    "\n",
    "full_file_names = [\n",
    "full_path+'ap_hurricane_sally_unleashes_20200916.txt',\n",
    "full_path+'cnn_big_ten_backtracks_20200916.txt',\n",
    "full_path+'nyt_on_the_fire_line_20200915.txt',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose list of stop words\n",
    "\n",
    "# choose whether 1k, 10k, 100k, or nltk\n",
    "which_stop_words = '1k'\n",
    "# which_stop_words = '10k'\n",
    "# which_stop_words = '100k'\n",
    "# which_stop_words = 'nltk'\n",
    "\n",
    "stop_words_path = '/Users/sagarsetru/Documents/post PhD positions search/insightDataScience/project/debiaser/stop_words_db/news-stopwords-master/'\n",
    "\n",
    "\n",
    "if which_stop_words == '1k':\n",
    "    \n",
    "    # doing 1k words list\n",
    "    stop_words_file_name = 'sw1k.csv'\n",
    "    \n",
    "    # make full file name\n",
    "    stop_words_full_file_name = stop_words_path+stop_words_file_name\n",
    "    \n",
    "    # get list of stop words\n",
    "    stop_words = load_stop_words_csv_to_list(stop_words_full_file_name)\n",
    "    \n",
    "elif which_stop_words == '10k':\n",
    "    \n",
    "    # doing 10k words list\n",
    "    stop_words_file_name = 'sw10k.csv'\n",
    "\n",
    "    # make full file name\n",
    "    stop_words_full_file_name = stop_words_path+stop_words_file_name\n",
    "    \n",
    "    # get list of stop words\n",
    "    stop_words = load_stop_words_csv_to_list(stop_words_full_file_name)\n",
    "\n",
    "elif which_stop_words == '100k':\n",
    "    \n",
    "    # doing 100k\n",
    "    stop_words_file_name = 'sw100k.csv'  \n",
    "    \n",
    "    # get full file name\n",
    "    stop_words_full_file_name = stop_words_path+stop_words_file_name\n",
    "    \n",
    "    # get list of stop words\n",
    "    stop_words = load_stop_words_csv_to_list(stop_words_full_file_name)\n",
    "\n",
    "\n",
    "elif which_stop_words == 'nltk':\n",
    "    # import from nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "else:\n",
    "    print('Select proper variable name for \"which_stop_words\"')\n",
    "    \n",
    "# adding custom words\n",
    "stop_words.append('said')\n",
    "stop_words.append(\"you're\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 1\n",
      "['hurricane', 'storm', 'sally', 'rain', 'said.', 'pensacola,', 'alabama,', 'pensacola', 'gulf', 'forecasters', 'winds', 'blew', 'centimeters)', 'emergency']\n",
      "{'hurricane': [0.047211945, 0.051296424, 0.04130953, 0.041064326, 0.040835414], 'storm': [0.036870763, 0.030749425, 0.037262868, 0.025596414, 0.038195804], 'sally': [0.032726645, 0.027231643, 0.024566054, 0.017805716, 0.020892657], 'rain': [0.030358573, 0.03443476, 0.033684663, 0.024933834, 0.022981912], 'said.': [0.030050473, 0.019308154, 0.020453814, 0.019839799, 0.017185377], 'pensacola,': [0.029875752, 0.026556166, 0.026439466, 0.029992012], 'alabama,': [0.022863112, 0.02659731, 0.021632824, 0.021176517, 0.018378071], 'pensacola': [0.022259897, 0.01867975], 'gulf': [0.021231234, 0.033371836, 0.02719501, 0.026691295, 0.026246408], 'forecasters': [0.020532189, 0.019476593, 0.018836439, 0.016948646], 'winds': [0.021298429, 0.019346211], 'blew': [0.017759714], 'centimeters)': [0.018146867], 'emergency': [0.016744329]}\n",
      " \n",
      "Number of documents: 1\n",
      "['football', 'ten', 'sports', 'coaches,', 'college', 'ncaa', 'student-athletes', 'coronavirus', 'require', 'ohio', 'parents,', 'wednesday,', 'players,', 'doctor', 'article:']\n",
      "{'football': [0.074112415, 0.06862126, 0.10503476, 0.062629215, 0.06260031], 'ten': [0.06892169, 0.07017225, 0.16723673, 0.06260631, 0.06263075], 'sports': [0.06536016, 0.06187296, 0.08554787, 0.062535085, 0.062547654], 'coaches,': [0.06312132, 0.061578892, 0.06509983, 0.06253981], 'college': [0.06289076, 0.06443824, 0.06491057, 0.062520094, 0.062477432], 'ncaa': [0.062423375, 0.062155485, 0.062479824, 0.062480174], 'student-athletes': [0.061379373, 0.06228555, 0.06518093, 0.062482756, 0.062499907], 'coronavirus': [0.060897045, 0.061683644, 0.06248084], 'require': [0.06085321, 0.061691955, 0.062510945], 'ohio': [0.060467567], 'parents,': [0.061858557, 0.06248455, 0.062483285], 'wednesday,': [0.044822324], 'players,': [0.044821277, 0.062481176], 'doctor': [0.044762958, 0.06247572], 'article:': [0.044761576, 0.062488616]}\n",
      " \n",
      "Number of documents: 1\n",
      "['firefighters', 'said.', '—', 'firefighting', 'fighting', 'captain', 'said,', 'fires', 'climate', 'lightning', 'heavy', 'conditions', 'lnu', 'mr.']\n",
      "{'firefighters': [0.02304634, 0.029365048, 0.039329335, 0.03616285, 0.03355466], 'said.': [0.022518776, 0.031583123, 0.038876474, 0.039682113, 0.026454458], '—': [0.020202765, 0.02287625, 0.026716838, 0.031207934, 0.020203114], 'firefighting': [0.019083092, 0.018780896, 0.024354022, 0.017320983, 0.01772654], 'fighting': [0.018039048, 0.020768382, 0.017145464, 0.018256547], 'captain': [0.017167315, 0.015461605, 0.018359112, 0.018703897], 'said,': [0.016165646, 0.016930025, 0.01928383, 0.01576759, 0.015542698], 'fires': [0.015002051, 0.019489532, 0.020029435, 0.022817057, 0.017929673], 'climate': [0.014675563, 0.017328257, 0.01646927, 0.018178264, 0.016858961], 'lightning': [0.014327382, 0.015655307, 0.015968481], 'heavy': [0.015529831], 'conditions': [0.016844511], 'lnu': [0.016131626], 'mr.': [0.016983535]}\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Create a floor of the frequency of words to remove\n",
    "word_frequency_threshold = 1\n",
    "\n",
    "# choose the number of LDA topics\n",
    "num_lda_topics = 5\n",
    "\n",
    "# loop through files\n",
    "for ind, full_file_name in enumerate(full_file_names):\n",
    "    \n",
    "    # get the article text as one string, remove new lines\n",
    "    with open(full_file_name, 'r') as file:\n",
    "        article_text = file.read().replace('\\n', ' ')\n",
    "        \n",
    "    # replace weird apostrophes\n",
    "    article_text = article_text.replace(\"`\",\"'\")\n",
    "    article_text = article_text.replace(\"’\",\"'\")\n",
    "    article_text = article_text.replace(\"'\",\"'\")\n",
    "    \n",
    "    # get rid of punctuation\n",
    "    article_text.translate(article_text.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # break article into sentences\n",
    "#     article_sentences = tokenize.sent_tokenize(article_text)\n",
    "\n",
    "    # following https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#sphx-glr-auto-examples-core-run-core-concepts-py\n",
    "    # Lowercase each document, split it by white space and filter out stopwords\n",
    "    texts = [[word for word in document.lower().split() if word not in stop_words] \n",
    "             for document in [article_text]]\n",
    "\n",
    "    # Count word frequencies\n",
    "    frequency = defaultdict(int)\n",
    "    for text in texts:\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "\n",
    "    # Only keep words that appear more than set frequency, to produce the corpus\n",
    "    processed_corpus = [[token for token in text if frequency[token] > word_frequency_threshold] for text in texts]\n",
    "\n",
    "    print('Number of documents: %d' % len(processed_corpus))\n",
    "    \n",
    "    # generate a dictionary via gensim\n",
    "    processed_dictionary = Dictionary(processed_corpus)\n",
    "    \n",
    "    # generate bag of words of the corpus\n",
    "    bow_corpus = [processed_dictionary.doc2bow(text) for text in processed_corpus]\n",
    "    \n",
    "    # generate the LDA model\n",
    "    lda = gensim.models.LdaModel(corpus = bow_corpus,\n",
    "                                 num_topics = num_lda_topics,\n",
    "                                 id2word = processed_dictionary,\n",
    "                                  passes = 1)\n",
    "\n",
    "    # get the topics from the lda model\n",
    "    lda_topics = lda.show_topics(formatted=False)\n",
    "    \n",
    "    # dictionary for topics and the probabilities associated with them\n",
    "    topics_probs_dict = {}\n",
    "    \n",
    "    # list of unique topic names\n",
    "    topics = []\n",
    "    \n",
    "    # loop through each list of generated topics\n",
    "    for topic in lda_topics:\n",
    "        \n",
    "        # get the list of topics\n",
    "        topic_words = topic[1]\n",
    "        \n",
    "        # loop through topic words and probabilities\n",
    "        for topic_word, prob in topic_words:\n",
    "            \n",
    "            # if the word isn't already in the list of topics, add it to list of topics\n",
    "            if topic_word not in topics: \n",
    "                topics.append(topic_word)\n",
    "                \n",
    "            # if the word is not a key in the dictionary of topics to probabilities, add it to dictionary\n",
    "            if topic_word not in topics_probs_dict.keys():\n",
    "                \n",
    "                topics_probs_dict[topic_word] = [prob]\n",
    "            \n",
    "            # if the word is a key in the dictionary of topics to probabilities, append probability\n",
    "            else:\n",
    "                topics_probs_dict[topic_word].append(prob)\n",
    "                \n",
    "    print(topics)\n",
    "    print(topics_probs_dict)\n",
    "    print(' ')\n",
    "        \n",
    "\n",
    "    # get the topics from the lda model\n",
    "#     lda_topics = lda.show_topics()\n",
    "\n",
    "    # initialize empty list for topics\n",
    "#     topics = []\n",
    "    \n",
    "#     # generate filter for lda topic output as lambda function\n",
    "#     filters = [lambda x: x.lower(), strip_punctuation, strip_numeric]\n",
    "\n",
    "#     # loop through generated topics\n",
    "#     for topic in lda_topics:\n",
    "#         word_to_add = preprocess_string(topic[1], filters)\n",
    "#         if word_to_add not in stop_words:\n",
    "#             topics.append(word_to_add)\n",
    "\n",
    "#     all_unique_topics = []\n",
    "#     for topic_list in topics:\n",
    "#         for topic in topic_list:\n",
    "#             if topic not in all_unique_topics:\n",
    "#                 if topic not in stop_words:\n",
    "#                     all_unique_topics.append(topic)\n",
    "    \n",
    "    \n",
    "#     print(all_unique_topics)\n",
    "    \n",
    "#     print(topics)\n",
    "    \n",
    "#     if ind == 1:\n",
    "#         break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics_probs_dict = {}\n",
    "# topics = []\n",
    "# for topic in lda_topics:\n",
    "#     word_topics = topic[1]\n",
    "#     for word_topic, prob in word_topics:\n",
    "#         if word_topic not in topics:\n",
    "#             topics.append(word_topic)\n",
    "#         if word_topic not in topics_probs_dict.keys():\n",
    "#             topics_probs_dict[word_topic] = [prob]\n",
    "#         else:\n",
    "#             topics_probs_dict[word_topic].append(prob)\n",
    "        \n",
    "# dictionary for topics and the probabilities associated with them\n",
    "topics_probs_dict = {}\n",
    "\n",
    "# list of unique topic names\n",
    "topics = []\n",
    "\n",
    "# loop through each list of generated topics\n",
    "for topic in lda_topics:\n",
    "\n",
    "    # get the list of topics\n",
    "    topic_words = topic[1]\n",
    "\n",
    "    # loop through topic words and probabilities\n",
    "    for topic_word, prob in topic_words:\n",
    "        if topic_word not in topics:\n",
    "            topics.append(topic_word)\n",
    "        if topic_word not in topics_probs_dict.keys():\n",
    "            topics_probs_dict[topic_word] = [prob]\n",
    "        else:\n",
    "            topics_probs_dict[topic_word].append(prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hurricane', 'storm', 'gulf', 'alabama,', 'said.', 'rain', 'winds', 'forecasters', 'pensacola,', 'sally', 'pensacola', 'centimeters)']\n"
     ]
    }
   ],
   "source": [
    "print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hurricane': [0.028900892, 0.023605594, 0.056523804, 0.041764483, 0.03936894], 'storm': [0.026775975, 0.024644408, 0.039426956, 0.029284615, 0.032886967], 'gulf': [0.022866178, 0.018373583, 0.02937628, 0.021296008, 0.029017441], 'alabama,': [0.022147043, 0.019578151, 0.022074558, 0.023198724, 0.0257787], 'said.': [0.019591311, 0.018274995, 0.024987014, 0.021416679, 0.022029705], 'rain': [0.019554488, 0.017821308, 0.03733048, 0.021670463, 0.030659337], 'winds': [0.019485176, 0.023764139], 'forecasters': [0.019385332, 0.018169055, 0.019014213, 0.021644952], 'pensacola,': [0.019187111, 0.020984672, 0.029835008, 0.028988319, 0.022981185], 'sally': [0.018017035, 0.019778242, 0.02662366, 0.02455861, 0.034632687], 'pensacola': [0.016891448, 0.019275548, 0.019613955], 'centimeters)': [0.018891351]}\n"
     ]
    }
   ],
   "source": [
    "print(topics_probs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(6 unique tokens: ['coaches,', 'college', 'football', 'sports', 'student-athletes']...)\n"
     ]
    }
   ],
   "source": [
    "# Create a set of frequent meaningless words to remove\n",
    "word_frequency_threshold = 2\n",
    "\n",
    "# following https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#sphx-glr-auto-examples-core-run-core-concepts-py\n",
    "\n",
    "# Lowercase each document, split it by white space and filter out stopwords\n",
    "texts = [[word for word in document.lower().split() if word not in stop_words]\n",
    "         for document in [article_text]]\n",
    "\n",
    "# Count word frequencies\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# Only keep words that appear more than once\n",
    "processed_corpus = [[token for token in text if frequency[token] > word_frequency_threshold] for text in texts]\n",
    "\n",
    "processed_dictionary = Dictionary(processed_corpus)\n",
    "print(processed_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [processed_dictionary.doc2bow(text) for text in processed_corpus]\n",
    "lda_2 = gensim.models.LdaModel(corpus=bow_corpus,num_topics=5,id2word=processed_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ten', 'football', 'sports', 'student', 'athletes', 'coaches', 'college']\n",
      "ten\n",
      "football\n",
      "sports\n",
      "student\n",
      "athletes\n",
      "coaches\n",
      "college\n",
      " \n",
      "['ten', 'football', 'college', 'sports', 'coaches', 'student', 'athletes']\n",
      "ten\n",
      "football\n",
      "college\n",
      "sports\n",
      "coaches\n",
      "student\n",
      "athletes\n",
      " \n",
      "['ten', 'football', 'college', 'coaches', 'sports', 'student', 'athletes']\n",
      "ten\n",
      "football\n",
      "college\n",
      "coaches\n",
      "sports\n",
      "student\n",
      "athletes\n",
      " \n",
      "['ten', 'football', 'college', 'coaches', 'student', 'athletes', 'sports']\n",
      "ten\n",
      "football\n",
      "college\n",
      "coaches\n",
      "student\n",
      "athletes\n",
      "sports\n",
      " \n",
      "['ten', 'football', 'coaches', 'college', 'student', 'athletes', 'sports']\n",
      "ten\n",
      "football\n",
      "coaches\n",
      "college\n",
      "student\n",
      "athletes\n",
      "sports\n",
      " \n",
      "['ten', 'football', 'sports', 'student', 'athletes', 'coaches', 'college', 'ten', 'football', 'college', 'sports', 'coaches', 'student', 'athletes', 'ten', 'football', 'college', 'coaches', 'sports', 'student', 'athletes', 'ten', 'football', 'college', 'coaches', 'student', 'athletes', 'sports', 'ten', 'football', 'coaches', 'college', 'student', 'athletes', 'sports']\n"
     ]
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "from gensim.parsing.preprocessing import strip_numeric\n",
    "\n",
    "# # lda.top_topics(corpus=bow_corpus,dictionary=processed_dictionary)\n",
    "# for i in range(0, lda.num_topics-1):\n",
    "#     current_topic = lda.print_topic(i)\n",
    "#     print(current_topic)\n",
    "    \n",
    "lda_topics = lda_2.show_topics()\n",
    "\n",
    "topics = []\n",
    "filters = [lambda x: x.lower(), strip_punctuation, strip_numeric]\n",
    "\n",
    "for topic in lda_topics:\n",
    "#     print(topic)\n",
    "    topics.append(preprocess_string(topic[1], filters))\n",
    "\n",
    "all_topic_words = []\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "    for topic_word in topic:\n",
    "        print(topic_word)\n",
    "        all_topic_words.append(topic_word)\n",
    "    \n",
    "    print(' ')\n",
    "\n",
    "print(all_topic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ten', 'football', 'sports', 'student', 'athletes', 'coaches', 'college'], ['ten', 'football', 'college', 'sports', 'coaches', 'student', 'athletes'], ['ten', 'football', 'college', 'coaches', 'sports', 'student', 'athletes'], ['ten', 'football', 'college', 'coaches', 'student', 'athletes', 'sports'], ['ten', 'football', 'coaches', 'college', 'student', 'athletes', 'sports']]\n"
     ]
    }
   ],
   "source": [
    "print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ten', 'football', 'sports', 'student', 'athletes', 'coaches', 'college']\n"
     ]
    }
   ],
   "source": [
    "all_unique_topics = []\n",
    "for topic_list in topics:\n",
    "    for topic in topic_list:\n",
    "        if topic not in all_unique_topics:\n",
    "            all_unique_topics.append(topic)\n",
    "\n",
    "print(all_unique_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coaches', 'college', 'football', 'sports', 'student', 'ten', 'athletes']"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ten', 'football', 'sports', 'student', 'athletes', 'coaches', 'college']\n"
     ]
    }
   ],
   "source": [
    "len(topics[0])\n",
    "print(topics[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
