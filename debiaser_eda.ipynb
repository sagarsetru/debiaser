{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA for 'debiaser' data product\n",
    "#### Sagar Setru, September 16th, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief description using CoNVO framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context\n",
    "\n",
    "Some people are eager to get news from outside of their echo chamber. However, they do not know where to go outside of their echo chambers, and may also have some activation energy when it comes to seeking information from other sources. In the meantime, most newsfeeds only push you content that you agree with. You end up in an echo chamber, but may not have ever wanted to be in one in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need\n",
    "\n",
    "A way to find news articles from different yet reliable media sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision\n",
    "\n",
    "Debiaser, a data product (maybe Chrome plug-in?) that will recommend news articles similar in topic to the one currently being read, but from several pre-curated and reliable news media organizations across the political spectrum, for example, following the \"media bias chart\" here https://www.adfontesmedia.com/ or the \"media bias ratings\" here: https://www.allsides.com/media-bias/media-bias-ratings. The app will determine the main topics of the text of a news article, and then show links to similar articles from other news organizations.\n",
    "\n",
    "Caveats: Many of these articles may be behind paywalls. News aggregators already basically do this. How different is this than just searching Google using the title of an article?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outcome\n",
    "\n",
    "People who are motivated to engage in content outside of their echo chambers have a tool that enables them to quickly find news similar to what they are currently reading, but from a variety of news organizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conda environment:\n",
      "insight\n"
     ]
    }
   ],
   "source": [
    "# make sure I'm in the right environment\n",
    "\n",
    "print('Conda environment:')\n",
    "print(os.environ['CONDA_DEFAULT_ENV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import base packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import text processing and NLP specific packages\n",
    "\n",
    "# for generating LDA models\n",
    "import gensim\n",
    "\n",
    "# for preprocessing documents\n",
    "from gensim.parsing.preprocessing import preprocess_documents\n",
    "\n",
    "# to break articles up into sentences (currently not in use)\n",
    "from nltk import tokenize\n",
    "\n",
    "# for counting frequency of words\n",
    "from collections import defaultdict\n",
    "\n",
    "# for processing lda topic output\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "from gensim.parsing.preprocessing import strip_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stop_words_csv_to_list(full_file_name):\n",
    "    \"\"\"fxn that loads stop words list downloaded from git repo called 'news-stopwords'\"\"\"\n",
    "    \n",
    "    stop_words = pd.read_csv(full_file_name)\n",
    "\n",
    "    stop_words = stop_words['term']\n",
    "\n",
    "    stop_words = [word for word in stop_words]\n",
    "    \n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of manually made text files of articles\n",
    "\n",
    "full_path = '/Users/sagarsetru/Documents/post PhD positions search/insightDataScience/project/debiaser/article_text_files/'\n",
    "\n",
    "full_file_names = [\n",
    "full_path+'ap_hurricane_sally_unleashes_20200916.txt',\n",
    "full_path+'cnn_big_ten_backtracks_20200916.txt',\n",
    "full_path+'nyt_on_the_fire_line_20200915.txt',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose list of stop words\n",
    "\n",
    "# choose whether 1k, 10k, 100k, or nltk\n",
    "which_stop_words = '1k'\n",
    "# which_stop_words = '10k'\n",
    "# which_stop_words = '100k'\n",
    "# which_stop_words = 'nltk'\n",
    "\n",
    "stop_words_path = '/Users/sagarsetru/Documents/post PhD positions search/insightDataScience/project/debiaser/stop_words_db/news-stopwords-master/'\n",
    "\n",
    "\n",
    "if which_stop_words == '1k':\n",
    "    \n",
    "    # doing 1k words list\n",
    "    stop_words_file_name = 'sw1k.csv'\n",
    "    \n",
    "    # make full file name\n",
    "    stop_words_full_file_name = stop_words_path+stop_words_file_name\n",
    "    \n",
    "    # get list of stop words\n",
    "    stop_words = load_stop_words_csv_to_list(stop_words_full_file_name)\n",
    "    \n",
    "elif which_stop_words == '10k':\n",
    "    \n",
    "    # doing 10k words list\n",
    "    stop_words_file_name = 'sw10k.csv'\n",
    "\n",
    "    # make full file name\n",
    "    stop_words_full_file_name = stop_words_path+stop_words_file_name\n",
    "    \n",
    "    # get list of stop words\n",
    "    stop_words = load_stop_words_csv_to_list(stop_words_full_file_name)\n",
    "\n",
    "elif which_stop_words == '100k':\n",
    "    \n",
    "    # doing 100k\n",
    "    stop_words_file_name = 'sw100k.csv'  \n",
    "    \n",
    "    # get full file name\n",
    "    stop_words_full_file_name = stop_words_path+stop_words_file_name\n",
    "    \n",
    "    # get list of stop words\n",
    "    stop_words = load_stop_words_csv_to_list(stop_words_full_file_name)\n",
    "\n",
    "\n",
    "elif which_stop_words == 'nltk':\n",
    "    # import from nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "else:\n",
    "    print('Select proper variable name for \"which_stop_words\"')\n",
    "    \n",
    "# adding custom words\n",
    "stop_words.append('said')\n",
    "stop_words.append(\"you're\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "decoding to str: need a bytes-like object, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-392-98bd4b0f5be0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# loop through generated topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlda_topics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mword_to_add\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mword_to_add\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mtopics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_to_add\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/insight/lib/python3.8/site-packages/gensim/parsing/preprocessing.py\u001b[0m in \u001b[0;36mpreprocess_string\u001b[0;34m(s, filters)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m     \"\"\"\n\u001b[0;32m--> 373\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/insight/lib/python3.8/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36many2unicode\u001b[0;34m(text, encoding, errors)\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: decoding to str: need a bytes-like object, list found"
     ]
    }
   ],
   "source": [
    "# Create a floor of the frequency of words to remove\n",
    "word_frequency_threshold = 1\n",
    "\n",
    "# choose the number of LDA topics\n",
    "num_lda_topics = 5\n",
    "\n",
    "# loop through files\n",
    "for ind, full_file_name in enumerate(full_file_names):\n",
    "    \n",
    "    # get the article text as one string, remove new lines\n",
    "    with open(full_file_name, 'r') as file:\n",
    "        article_text = file.read().replace('\\n', ' ')\n",
    "        \n",
    "    # replace weird apostrophes\n",
    "    article_text = article_text.replace(\"`\",\"'\")\n",
    "    article_text = article_text.replace(\"’\",\"'\")\n",
    "    article_text = article_text.replace(\"'\",\"'\")\n",
    "    \n",
    "    # get rid of punctuation\n",
    "    article_text.translate(article_text.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # break article into sentences\n",
    "#     article_sentences = tokenize.sent_tokenize(article_text)\n",
    "\n",
    "    # following https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#sphx-glr-auto-examples-core-run-core-concepts-py\n",
    "    # Lowercase each document, split it by white space and filter out stopwords\n",
    "    texts = [[word for word in document.lower().split() if word not in stop_words] \n",
    "             for document in [article_text]]\n",
    "\n",
    "    # Count word frequencies\n",
    "    frequency = defaultdict(int)\n",
    "    for text in texts:\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "\n",
    "    # Only keep words that appear more than set frequency, to produce the corpus\n",
    "    processed_corpus = [[token for token in text if frequency[token] > word_frequency_threshold] for text in texts]\n",
    "\n",
    "    print('Number of documents: %d' % len(processed_corpus))\n",
    "    \n",
    "    # generate a dictionary via gensim\n",
    "    processed_dictionary = Dictionary(processed_corpus)\n",
    "    \n",
    "    # generate bag of words of the corpus\n",
    "    bow_corpus = [processed_dictionary.doc2bow(text) for text in processed_corpus]\n",
    "    \n",
    "    # generate the LDA model\n",
    "    lda = gensim.models.LdaModel(corpus = bow_corpus,\n",
    "                                 num_topics = num_lda_topics,\n",
    "                                 id2word = processed_dictionary,\n",
    "                                  passes = 1)\n",
    "\n",
    "    # get the topics from the lda model\n",
    "    lda_topics = lda.show_topics(formatted=False)\n",
    "\n",
    "    # initialize empty list for topics\n",
    "    topics = []\n",
    "    \n",
    "#     # generate filter for lda topic output as lambda function\n",
    "#     filters = [lambda x: x.lower(), strip_punctuation, strip_numeric]\n",
    "\n",
    "#     # loop through generated topics\n",
    "#     for topic in lda_topics:\n",
    "#         word_to_add = preprocess_string(topic[1], filters)\n",
    "#         if word_to_add not in stop_words:\n",
    "#             topics.append(word_to_add)\n",
    "\n",
    "#     all_unique_topics = []\n",
    "#     for topic_list in topics:\n",
    "#         for topic in topic_list:\n",
    "#             if topic not in all_unique_topics:\n",
    "#                 if topic not in stop_words:\n",
    "#                     all_unique_topics.append(topic)\n",
    "    \n",
    "    \n",
    "    print(all_unique_topics)\n",
    "    \n",
    "#     print(topics)\n",
    "    \n",
    "#     if ind == 1:\n",
    "#         break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_probs_dict = {}\n",
    "topics = []\n",
    "for topic in lda_topics:\n",
    "    word_topics = topic[1]\n",
    "    for word_topic, prob in word_topics:\n",
    "        if word_topic not in topics:\n",
    "            topics.append(word_topic)\n",
    "        if word_topic not in topics_probs_dict.keys():\n",
    "            topics_probs_dict[word_topic] = [prob]\n",
    "        else:\n",
    "            topics_probs_dict[word_topic].append(prob)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hurricane', 'storm', 'gulf', 'alabama,', 'said.', 'rain', 'winds', 'forecasters', 'pensacola,', 'sally', 'pensacola', 'centimeters)']\n"
     ]
    }
   ],
   "source": [
    "print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hurricane': [0.028900892, 0.023605594, 0.056523804, 0.041764483, 0.03936894], 'storm': [0.026775975, 0.024644408, 0.039426956, 0.029284615, 0.032886967], 'gulf': [0.022866178, 0.018373583, 0.02937628, 0.021296008, 0.029017441], 'alabama,': [0.022147043, 0.019578151, 0.022074558, 0.023198724, 0.0257787], 'said.': [0.019591311, 0.018274995, 0.024987014, 0.021416679, 0.022029705], 'rain': [0.019554488, 0.017821308, 0.03733048, 0.021670463, 0.030659337], 'winds': [0.019485176, 0.023764139], 'forecasters': [0.019385332, 0.018169055, 0.019014213, 0.021644952], 'pensacola,': [0.019187111, 0.020984672, 0.029835008, 0.028988319, 0.022981185], 'sally': [0.018017035, 0.019778242, 0.02662366, 0.02455861, 0.034632687], 'pensacola': [0.016891448, 0.019275548, 0.019613955], 'centimeters)': [0.018891351]}\n"
     ]
    }
   ],
   "source": [
    "print(topics_probs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(6 unique tokens: ['coaches,', 'college', 'football', 'sports', 'student-athletes']...)\n"
     ]
    }
   ],
   "source": [
    "# Create a set of frequent meaningless words to remove\n",
    "word_frequency_threshold = 2\n",
    "\n",
    "# following https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#sphx-glr-auto-examples-core-run-core-concepts-py\n",
    "\n",
    "# Lowercase each document, split it by white space and filter out stopwords\n",
    "texts = [[word for word in document.lower().split() if word not in stop_words]\n",
    "         for document in [article_text]]\n",
    "\n",
    "# Count word frequencies\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# Only keep words that appear more than once\n",
    "processed_corpus = [[token for token in text if frequency[token] > word_frequency_threshold] for text in texts]\n",
    "\n",
    "processed_dictionary = Dictionary(processed_corpus)\n",
    "print(processed_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [processed_dictionary.doc2bow(text) for text in processed_corpus]\n",
    "lda_2 = gensim.models.LdaModel(corpus=bow_corpus,num_topics=5,id2word=processed_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ten', 'football', 'sports', 'student', 'athletes', 'coaches', 'college']\n",
      "ten\n",
      "football\n",
      "sports\n",
      "student\n",
      "athletes\n",
      "coaches\n",
      "college\n",
      " \n",
      "['ten', 'football', 'college', 'sports', 'coaches', 'student', 'athletes']\n",
      "ten\n",
      "football\n",
      "college\n",
      "sports\n",
      "coaches\n",
      "student\n",
      "athletes\n",
      " \n",
      "['ten', 'football', 'college', 'coaches', 'sports', 'student', 'athletes']\n",
      "ten\n",
      "football\n",
      "college\n",
      "coaches\n",
      "sports\n",
      "student\n",
      "athletes\n",
      " \n",
      "['ten', 'football', 'college', 'coaches', 'student', 'athletes', 'sports']\n",
      "ten\n",
      "football\n",
      "college\n",
      "coaches\n",
      "student\n",
      "athletes\n",
      "sports\n",
      " \n",
      "['ten', 'football', 'coaches', 'college', 'student', 'athletes', 'sports']\n",
      "ten\n",
      "football\n",
      "coaches\n",
      "college\n",
      "student\n",
      "athletes\n",
      "sports\n",
      " \n",
      "['ten', 'football', 'sports', 'student', 'athletes', 'coaches', 'college', 'ten', 'football', 'college', 'sports', 'coaches', 'student', 'athletes', 'ten', 'football', 'college', 'coaches', 'sports', 'student', 'athletes', 'ten', 'football', 'college', 'coaches', 'student', 'athletes', 'sports', 'ten', 'football', 'coaches', 'college', 'student', 'athletes', 'sports']\n"
     ]
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "from gensim.parsing.preprocessing import strip_numeric\n",
    "\n",
    "# # lda.top_topics(corpus=bow_corpus,dictionary=processed_dictionary)\n",
    "# for i in range(0, lda.num_topics-1):\n",
    "#     current_topic = lda.print_topic(i)\n",
    "#     print(current_topic)\n",
    "    \n",
    "lda_topics = lda_2.show_topics()\n",
    "\n",
    "topics = []\n",
    "filters = [lambda x: x.lower(), strip_punctuation, strip_numeric]\n",
    "\n",
    "for topic in lda_topics:\n",
    "#     print(topic)\n",
    "    topics.append(preprocess_string(topic[1], filters))\n",
    "\n",
    "all_topic_words = []\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "    for topic_word in topic:\n",
    "        print(topic_word)\n",
    "        all_topic_words.append(topic_word)\n",
    "    \n",
    "    print(' ')\n",
    "\n",
    "print(all_topic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ten', 'football', 'sports', 'student', 'athletes', 'coaches', 'college'], ['ten', 'football', 'college', 'sports', 'coaches', 'student', 'athletes'], ['ten', 'football', 'college', 'coaches', 'sports', 'student', 'athletes'], ['ten', 'football', 'college', 'coaches', 'student', 'athletes', 'sports'], ['ten', 'football', 'coaches', 'college', 'student', 'athletes', 'sports']]\n"
     ]
    }
   ],
   "source": [
    "print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ten', 'football', 'sports', 'student', 'athletes', 'coaches', 'college']\n"
     ]
    }
   ],
   "source": [
    "all_unique_topics = []\n",
    "for topic_list in topics:\n",
    "    for topic in topic_list:\n",
    "        if topic not in all_unique_topics:\n",
    "            all_unique_topics.append(topic)\n",
    "\n",
    "print(all_unique_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coaches', 'college', 'football', 'sports', 'student', 'ten', 'athletes']"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ten', 'football', 'sports', 'student', 'athletes', 'coaches', 'college']\n"
     ]
    }
   ],
   "source": [
    "len(topics[0])\n",
    "print(topics[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
